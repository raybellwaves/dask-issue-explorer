[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1130066243",
        "html_url": "https://github.com/dask/dask/issues/9051#issuecomment-1130066243",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9051",
        "id": 1130066243,
        "node_id": "IC_kwDOAbcwm85DW3FD",
        "user": {
            "login": "alienscience",
            "id": 16070,
            "node_id": "MDQ6VXNlcjE2MDcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/16070?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alienscience",
            "html_url": "https://github.com/alienscience",
            "followers_url": "https://api.github.com/users/alienscience/followers",
            "following_url": "https://api.github.com/users/alienscience/following{/other_user}",
            "gists_url": "https://api.github.com/users/alienscience/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alienscience/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alienscience/subscriptions",
            "organizations_url": "https://api.github.com/users/alienscience/orgs",
            "repos_url": "https://api.github.com/users/alienscience/repos",
            "events_url": "https://api.github.com/users/alienscience/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alienscience/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-18T14:08:15Z",
        "updated_at": "2022-05-18T14:08:15Z",
        "author_association": "NONE",
        "body": "If `aggregate_files=True` is removed, would it be possible to use `split_row_groups=N` where `N` is a number to get similar functionality?\r\n\r\nIf not, then then I guess that a `df.repartition()` would have to be used. I have seen parquet datasets that cause problems in Dask for any non-trivial operation. I think that these datasets start off as too many partitions and, since we started using `aggregate_files=True`, processing the datasets has been less problematic. My guess is that datasets such as these would be painful to process if there is no option to `aggregate_files` or similar.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1130066243/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1130136604",
        "html_url": "https://github.com/dask/dask/issues/9051#issuecomment-1130136604",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9051",
        "id": 1130136604,
        "node_id": "IC_kwDOAbcwm85DXIQc",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-18T15:08:00Z",
        "updated_at": "2022-05-18T15:08:00Z",
        "author_association": "MEMBER",
        "body": ">since we started using aggregate_files=True, processing the datasets has been less problematic.\r\n\r\nThank you fo sharing this @alienscience !  Do your datasets typically contain many single row-group files such that using `read_parquet(..., split_row_groups=N, aggregate_files=True)` means that you are aggregating `N` small files into each Dask partition? If so, we may be able to *change* the specific meaning of `aggregate_files` to preserve that behavior (e.g. `aggregate_files=N`). The logic that I am most interested in removing supports the current relationship between `aggregate_files` and `chunksize`/`split_row_groups`.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1130136604/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1130149354",
        "html_url": "https://github.com/dask/dask/issues/9051#issuecomment-1130149354",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9051",
        "id": 1130149354,
        "node_id": "IC_kwDOAbcwm85DXLXq",
        "user": {
            "login": "alienscience",
            "id": 16070,
            "node_id": "MDQ6VXNlcjE2MDcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/16070?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alienscience",
            "html_url": "https://github.com/alienscience",
            "followers_url": "https://api.github.com/users/alienscience/followers",
            "following_url": "https://api.github.com/users/alienscience/following{/other_user}",
            "gists_url": "https://api.github.com/users/alienscience/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alienscience/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alienscience/subscriptions",
            "organizations_url": "https://api.github.com/users/alienscience/orgs",
            "repos_url": "https://api.github.com/users/alienscience/repos",
            "events_url": "https://api.github.com/users/alienscience/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alienscience/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-18T15:15:04Z",
        "updated_at": "2022-05-18T15:15:04Z",
        "author_association": "NONE",
        "body": "> Do your datasets typically contain many single row-group files such that using `read_parquet(..., split_row_groups=N, aggregate_files=True)` means that you are aggregating `N` small files into each Dask partition?\r\n\r\nYes, and it would be good to have a way to combine multiple small files into a single Dask partition.\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1130149354/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]