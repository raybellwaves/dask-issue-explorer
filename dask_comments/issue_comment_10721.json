[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1862705177",
        "html_url": "https://github.com/dask/dask/issues/10721#issuecomment-1862705177",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10721",
        "id": 1862705177,
        "node_id": "IC_kwDOAbcwm85vBqAZ",
        "user": {
            "login": "fjetter",
            "id": 8629629,
            "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fjetter",
            "html_url": "https://github.com/fjetter",
            "followers_url": "https://api.github.com/users/fjetter/followers",
            "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
            "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
            "organizations_url": "https://api.github.com/users/fjetter/orgs",
            "repos_url": "https://api.github.com/users/fjetter/repos",
            "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fjetter/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-12-19T12:52:43Z",
        "updated_at": "2023-12-19T12:52:43Z",
        "author_association": "MEMBER",
        "body": "I also encountered this in https://github.com/dask/dask/pull/10722",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1862705177/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1862930050",
        "html_url": "https://github.com/dask/dask/issues/10721#issuecomment-1862930050",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10721",
        "id": 1862930050,
        "node_id": "IC_kwDOAbcwm85vCg6C",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-12-19T15:00:32Z",
        "updated_at": "2023-12-19T15:00:32Z",
        "author_association": "MEMBER",
        "body": ">@rjzamora do you have time to poke at this?\r\n\r\nYes, I also find this concerning. So, I definitely want to figure out what is going wrong.\r\n\r\nSide Note: When we start implementing https://github.com/dask/dask/issues/10602, I have some ideas for how we can get rid of all of this ugly \"adaptive aggregation\" code without sacrificing our ability to split files.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1862930050/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1863572162",
        "html_url": "https://github.com/dask/dask/issues/10721#issuecomment-1863572162",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10721",
        "id": 1863572162,
        "node_id": "IC_kwDOAbcwm85vE9rC",
        "user": {
            "login": "jrbourbeau",
            "id": 11656932,
            "node_id": "MDQ6VXNlcjExNjU2OTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jrbourbeau",
            "html_url": "https://github.com/jrbourbeau",
            "followers_url": "https://api.github.com/users/jrbourbeau/followers",
            "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}",
            "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions",
            "organizations_url": "https://api.github.com/users/jrbourbeau/orgs",
            "repos_url": "https://api.github.com/users/jrbourbeau/repos",
            "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jrbourbeau/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-12-19T22:41:28Z",
        "updated_at": "2023-12-19T22:41:28Z",
        "author_association": "MEMBER",
        "body": "I ran into the same `test_split_adaptive_aggregate_files` test failing in https://github.com/dask/dask/pull/10729 but with a different traceback\r\n\r\n```python\r\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\r\n```\r\n\r\nwhen attempting to read in a partition with the `fastparquet` engine (full traceback below)\r\n\r\n<details>\r\n<summary>Full traceback:</summary>\r\n\r\n```\r\n_______ test_split_adaptive_aggregate_files[b-fastparquet-fastparquet] ________\r\n[gw0] win32 -- Python 3.10.13 C:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\python.exe\r\n\r\ncolumn = column_index_length: null\r\ncolumn_index_offset: null\r\ncrypto_metadata: null\r\nencrypted_column_metadata: null\r\nfile_offset:...otal_compressed_size: 37\r\n  total_uncompressed_size: null\r\n  type: 1\r\noffset_index_length: null\r\noffset_index_offset: null\r\n\r\nschema_helper = <Parquet Schema with 3 entries>\r\ninfile = <fsspec.implementations.local.LocalFileOpener object at 0x000001AF9C49BA30>\r\nuse_cat = False, selfmade = True, assign = array([0]), catdef = None\r\nrow_filter = False\r\n\r\n    def read_col(column, schema_helper, infile, use_cat=False,\r\n                 selfmade=False, assign=None, catdef=None,\r\n                 row_filter=None):\r\n        \"\"\"Using the given metadata, read one column in one row-group.\r\n    \r\n        Parameters\r\n        ----------\r\n        column: thrift structure\r\n            Details on the column\r\n        schema_helper: schema.SchemaHelper\r\n            Based on the schema for this parquet data\r\n        infile: open file or string\r\n            If a string, will open; if an open object, will use as-is\r\n        use_cat: bool (False)\r\n            If this column is encoded throughout with dict encoding, give back\r\n            a pandas categorical column; otherwise, decode to values\r\n        row_filter: bool array or None\r\n            if given, selects which of the values read are to be written\r\n            into the output. Effectively implies NULLs, even for a required\r\n            column.\r\n        \"\"\"\r\n        cmd = column.meta_data\r\n        try:\r\n>           se = schema_helper.schema_element(cmd.path_in_schema)\r\n\r\nC:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:450: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <Parquet Schema with 3 entries>, name = ['d']\r\n\r\n    def schema_element(self, name):\r\n        \"\"\"Get the schema element with the given name or path\"\"\"\r\n        root = self.root\r\n        if isinstance(name, str):\r\n            name = name.split('.')\r\n        for part in name:\r\n>           root = root[\"children\"][part]\r\nE           KeyError: 'd'\r\n\r\nC:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\schema.py:119: KeyError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\ntmpdir = local('C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-runneradmin\\\\pytest-0\\\\popen-gw0\\\\test_split_adaptive_aggregate_4')\r\nwrite_engine = 'fastparquet', read_engine = 'fastparquet', aggregate_files = 'b'\r\n\r\n    @write_read_engines()\r\n    @pytest.mark.parametrize(\"aggregate_files\", [\"a\", \"b\"])\r\n    def test_split_adaptive_aggregate_files(\r\n        tmpdir, write_engine, read_engine, aggregate_files\r\n    ):\r\n        blocksize = \"1MiB\"\r\n        partition_on = [\"a\", \"b\"]\r\n        df_size = 100\r\n        df1 = pd.DataFrame(\r\n            {\r\n                \"a\": np.random.choice([\"apple\", \"banana\", \"carrot\"], size=df_size),\r\n                \"b\": np.random.choice([\"small\", \"large\"], size=df_size),\r\n                \"c\": np.random.random(size=df_size),\r\n                \"d\": np.random.randint(1, 100, size=df_size),\r\n            }\r\n        )\r\n        ddf1 = dd.from_pandas(df1, npartitions=9)\r\n    \r\n        ddf1.to_parquet(\r\n            str(tmpdir),\r\n            engine=write_engine,\r\n            partition_on=partition_on,\r\n            write_index=False,\r\n        )\r\n        with pytest.warns(FutureWarning, match=\"Behavior may change\"):\r\n            ddf2 = dd.read_parquet(\r\n                str(tmpdir),\r\n                engine=read_engine,\r\n                blocksize=blocksize,\r\n                split_row_groups=\"adaptive\",\r\n                aggregate_files=aggregate_files,\r\n            )\r\n    \r\n        # Check that files where aggregated as expected\r\n        if aggregate_files == \"a\":\r\n            assert ddf2.npartitions == 3\r\n        elif aggregate_files == \"b\":\r\n            assert ddf2.npartitions == 6\r\n    \r\n        # Check that the final data is correct\r\n>       df2 = ddf2.compute().sort_values([\"c\", \"d\"])\r\n\r\ndask\\dataframe\\io\\tests\\test_parquet.py:3132: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ndask\\base.py:342: in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\ndask\\base.py:628: in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\ndask\\dataframe\\io\\parquet\\core.py:96: in __call__\r\n    return read_parquet_part(\r\ndask\\dataframe\\io\\parquet\\core.py:668: in read_parquet_part\r\n    df = engine.read_partition(\r\ndask\\dataframe\\io\\parquet\\fastparquet.py:1075: in read_partition\r\n    return cls.pf_to_pandas(\r\ndask\\dataframe\\io\\parquet\\fastparquet.py:1172: in pf_to_pandas\r\n    pf.read_row_group_file(\r\nC:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\api.py:386: in read_row_group_file\r\n    core.read_row_group(\r\nC:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:642: in read_row_group\r\n    read_row_group_arrays(file, rg, columns, categories, schema_helper,\r\nC:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:612: in read_row_group_arrays\r\n    read_col(column, schema_helper, file, use_cat=name+'-catdef' in out,\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ncolumn = column_index_length: null\r\ncolumn_index_offset: null\r\ncrypto_metadata: null\r\nencrypted_column_metadata: null\r\nfile_offset:...otal_compressed_size: 37\r\n  total_uncompressed_size: null\r\n  type: 1\r\noffset_index_length: null\r\noffset_index_offset: null\r\n\r\nschema_helper = <Parquet Schema with 3 entries>\r\ninfile = <fsspec.implementations.local.LocalFileOpener object at 0x000001AF9C49BA30>\r\nuse_cat = False, selfmade = True, assign = array([0]), catdef = None\r\nrow_filter = False\r\n\r\n    def read_col(column, schema_helper, infile, use_cat=False,\r\n                 selfmade=False, assign=None, catdef=None,\r\n                 row_filter=None):\r\n        \"\"\"Using the given metadata, read one column in one row-group.\r\n    \r\n        Parameters\r\n        ----------\r\n        column: thrift structure\r\n            Details on the column\r\n        schema_helper: schema.SchemaHelper\r\n            Based on the schema for this parquet data\r\n        infile: open file or string\r\n            If a string, will open; if an open object, will use as-is\r\n        use_cat: bool (False)\r\n            If this column is encoded throughout with dict encoding, give back\r\n            a pandas categorical column; otherwise, decode to values\r\n        row_filter: bool array or None\r\n            if given, selects which of the values read are to be written\r\n            into the output. Effectively implies NULLs, even for a required\r\n            column.\r\n        \"\"\"\r\n        cmd = column.meta_data\r\n        try:\r\n            se = schema_helper.schema_element(cmd.path_in_schema)\r\n        except KeyError:\r\n            # column not present in this row group\r\n>           assign[:] = None\r\nE           TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\r\n\r\nC:\\Users\\runneradmin\\miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:453: TypeError\r\n```\r\n\r\n</details>",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1863572162/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1864582557",
        "html_url": "https://github.com/dask/dask/issues/10721#issuecomment-1864582557",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10721",
        "id": 1864582557,
        "node_id": "IC_kwDOAbcwm85vI0Wd",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-12-20T14:36:35Z",
        "updated_at": "2023-12-20T14:36:35Z",
        "author_association": "MEMBER",
        "body": ">when attempting to read in a partition with the fastparquet engine (full traceback below)\r\n\r\nThanks @jrbourbeau ! I have been struggling to figure out what is going wrong beyond \"The fastparquet engine is messing up somewhere with hive-partitioned data\" - This is helpful.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1864582557/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]