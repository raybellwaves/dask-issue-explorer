[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/730615475",
        "html_url": "https://github.com/dask/dask/issues/6865#issuecomment-730615475",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6865",
        "id": 730615475,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczMDYxNTQ3NQ==",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-19T20:22:25Z",
        "updated_at": "2020-11-19T23:29:09Z",
        "author_association": "MEMBER",
        "body": "Thanks @jangorecki !  The existence of a pandas `read_feather` API sounds like a good motivation for dask.dataframe support to me :)\r\n\r\nCan you say anything about the feather format or typical use case that might be important to know here?  For example, would it be sufficient to simply create a distinct DataFrame partition for each feather-formatted  file in a directory or list?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/730615475/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/730970207",
        "html_url": "https://github.com/dask/dask/issues/6865#issuecomment-730970207",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6865",
        "id": 730970207,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczMDk3MDIwNw==",
        "user": {
            "login": "jangorecki",
            "id": 3627377,
            "node_id": "MDQ6VXNlcjM2MjczNzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3627377?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jangorecki",
            "html_url": "https://github.com/jangorecki",
            "followers_url": "https://api.github.com/users/jangorecki/followers",
            "following_url": "https://api.github.com/users/jangorecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/jangorecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jangorecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jangorecki/subscriptions",
            "organizations_url": "https://api.github.com/users/jangorecki/orgs",
            "repos_url": "https://api.github.com/users/jangorecki/repos",
            "events_url": "https://api.github.com/users/jangorecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jangorecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-20T07:42:43Z",
        "updated_at": "2020-11-20T07:55:13Z",
        "author_association": "NONE",
        "body": "I have feather v2 format file, just single file storing whole dataset.\r\nI would like to load it into dask in-memory data frame so it can be used as efficient as possible.\r\nIdeally I would like to have an option to only mmap data, in case when dataset I am going to work with is too big for available memory. Then performance will be lower but I will able to compute out-of-memory tasks.\r\n\r\nI am now using csv format for loading data into memory and parquet for out-of-memory computation and would like to replace that with arrow file.\r\nThank you.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/730970207/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/731346150",
        "html_url": "https://github.com/dask/dask/issues/6865#issuecomment-731346150",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6865",
        "id": 731346150,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczMTM0NjE1MA==",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-20T18:47:31Z",
        "updated_at": "2020-11-20T18:47:31Z",
        "author_association": "MEMBER",
        "body": "It may be tricky to produce a multi-partition dask DataFrame from a single feather file.  Also, I'm not sure how mmap would help you handle larger-than-memory data given (1) the dask-graph programming model and (2) the available pyarrow API.  My understanding is that a feather file can be written as multiple `RecordBatch` chunks, but there is currently no mechanism available to perform partial IO (i.e. only read a subset of the `RecordBatch`s at a time).  This makes it difficult (if not impossible) to construct a task graph to coordinate the reading of distinct row/byte-ranges from the same file in parallel.  Instead, a single task will probably need to read an entire file (for the column selection of interest). \r\n\r\nMy knowledge about feather and the arrow IPC format is limited.  So, please let me know if I am missing anything important, or if I am mistaken about the available pyarrow/pandas API.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/731346150/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/807106414",
        "html_url": "https://github.com/dask/dask/issues/6865#issuecomment-807106414",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6865",
        "id": 807106414,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgwNzEwNjQxNA==",
        "user": {
            "login": "kylebarron",
            "id": 15164633,
            "node_id": "MDQ6VXNlcjE1MTY0NjMz",
            "avatar_url": "https://avatars.githubusercontent.com/u/15164633?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kylebarron",
            "html_url": "https://github.com/kylebarron",
            "followers_url": "https://api.github.com/users/kylebarron/followers",
            "following_url": "https://api.github.com/users/kylebarron/following{/other_user}",
            "gists_url": "https://api.github.com/users/kylebarron/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kylebarron/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kylebarron/subscriptions",
            "organizations_url": "https://api.github.com/users/kylebarron/orgs",
            "repos_url": "https://api.github.com/users/kylebarron/repos",
            "events_url": "https://api.github.com/users/kylebarron/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kylebarron/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-03-25T17:04:49Z",
        "updated_at": "2021-03-25T17:05:25Z",
        "author_association": "CONTRIBUTOR",
        "body": "I don't know the Pyarrow API super well, but it seems simple to do random IO in a batched feather file:\r\n\r\n```py\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.feather as feather\r\n\r\narr = np.random.random((10_000_000, 5))\r\ndf = pd.DataFrame(arr, columns=['a', 'b', 'c', 'd', 'e'])\r\ndf.to_feather('test.feather')\r\n\r\nreader = pa.RecordBatchFileReader('test.feather')\r\nreader.num_record_batches\r\n# 153\r\n\r\n%time batch = reader.get_batch(120)\r\n# CPU times: user 2.63 ms, sys: 4.69 ms, total: 7.32 ms\r\n# Wall time: 1.51 ms\r\n\r\nbatch_df = batch.to_pandas()\r\n# \ta\tb\tc\td\te\r\n# 0\t0.189827\t0.404387\t0.188649\t0.267698\t0.613487\r\n# 1\t0.205368\t0.283681\t0.699758\t0.877356\t0.876224\r\n# 2\t0.612154\t0.545432\t0.490574\t0.562790\t0.073749\r\n# 3\t0.245697\t0.655735\t0.843986\t0.619278\t0.216031\r\n# 4\t0.528330\t0.836063\t0.868445\t0.296022\t0.535843\r\n# ...\t...\t...\t...\t...\t...\r\n# 65531\t0.020002\t0.267757\t0.413674\t0.872035\t0.135909\r\n# 65532\t0.020331\t0.163924\t0.086865\t0.258297\t0.670558\r\n# 65533\t0.078295\t0.701330\t0.755074\t0.880473\t0.534841\r\n# 65534\t0.807105\t0.015029\t0.032281\t0.926612\t0.722516\r\n# 65535\t0.796258\t0.293905\t0.142886\t0.680169\t0.113665\r\n# \r\n# 65536 rows \u00d7 5 columns\r\n\r\n%time full_table = feather.read_table('test.feather')\r\n# CPU times: user 263 ms, sys: 919 ms, total: 1.18 s\r\n# Wall time: 954 ms\r\n```\r\n\r\nSo it seems pretty clear from the timings that `reader.get_batch()` is only reading a single batch from the file, and not the entire file.\r\n\r\nThis might work only with Feather v2, because that coincides with the Arrow IPC format. https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatchFileReader.html",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/807106414/reactions",
            "total_count": 3,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]