{
    "url": "https://api.github.com/repos/dask/dask/issues/6833",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/6833/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/6833/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/6833/events",
    "html_url": "https://github.com/dask/dask/issues/6833",
    "id": 742083076,
    "node_id": "MDU6SXNzdWU3NDIwODMwNzY=",
    "number": 6833,
    "title": "Massive memory (100GB) used by dask-scheduler",
    "user": {
        "login": "pseudotensor",
        "id": 2249614,
        "node_id": "MDQ6VXNlcjIyNDk2MTQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2249614?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pseudotensor",
        "html_url": "https://github.com/pseudotensor",
        "followers_url": "https://api.github.com/users/pseudotensor/followers",
        "following_url": "https://api.github.com/users/pseudotensor/following{/other_user}",
        "gists_url": "https://api.github.com/users/pseudotensor/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/pseudotensor/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/pseudotensor/subscriptions",
        "organizations_url": "https://api.github.com/users/pseudotensor/orgs",
        "repos_url": "https://api.github.com/users/pseudotensor/repos",
        "events_url": "https://api.github.com/users/pseudotensor/events{/privacy}",
        "received_events_url": "https://api.github.com/users/pseudotensor/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 386719400,
            "node_id": "MDU6TGFiZWwzODY3MTk0MDA=",
            "url": "https://api.github.com/repos/dask/dask/labels/scheduler",
            "name": "scheduler",
            "color": "D10945",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 31,
    "created_at": "2020-11-13T02:51:46Z",
    "updated_at": "2021-10-14T05:24:50Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "Using rapids 0.14 with dask 2.17.0 python 3.6, conda, Ubuntu 16.04\r\n\r\nI'm running xgboost using dask on GPUs.  I do:\r\n\r\n1) convert in-memory numpy frame -> dask distributed frame using from_array()\r\n2) chunk the frames sufficiently for every worker (here 3 nodes, 2 GPUs/node each) has data as required so xgboost does not hang\r\n3) Run dataset like 5M rows x 10 columns of airlines data\r\n\r\nNotes:\r\n1) Every time 1-3 is done it is in an isolate fork that dies at end of the fit.  So whatever instances of client etc. are destroyed.  Nothing remains on GPU, nothing remains in a process since it's gone.  So I don't believe I need a client.close() call.\r\n2) Even though these forks are gone, within the code I'm always using client as a context manager within a `with` statement.  So again shouldn't need a client.close() call or something like that.\r\n\r\nI see my application use reasonable amount of memory based upon that dataset.  I see workers using not much memory at all, like 2-3%.\r\n\r\nHowever, the dask-scheduler is using 70% of my 128GB system!  I don't understand how/why since the scheduler shouldn't hold data as far as I understand.  Perhaps the above sequence of sending dask frame to xgboost is a problem, but it would be odd that task graph is forced to hold data.\r\n\r\nEven if a single graph held data, which is already a problem, there's no way 90GB are needed to hold the data involved, so it's like there is repeatedly old data being stored.\r\n\r\n![image](https://user-images.githubusercontent.com/2249614/99022254-35adf300-2517-11eb-837a-2e750940c7dc.png)\r\n\r\nI don't have code to share to repro since it's not easy to extract, but I'm hoping still for ideas.  I will work on a repro, but any fixes/ideas would be good.\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/6833/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/6833/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}