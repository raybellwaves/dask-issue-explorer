[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/901537956",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-901537956",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 901537956,
        "node_id": "IC_kwDOAbcwm841vGCk",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-19T01:26:04Z",
        "updated_at": "2021-08-19T01:26:04Z",
        "author_association": "MEMBER",
        "body": "Thanks for writing this up Rick. I'm still digesting it, but do you think this \"allow parts/statistics to be collected in parallel\" would help with dask-geopandas, where they have to (re)-open each partition to read some metadata out of the partition's schema? (https://github.com/geopandas/dask-geopandas/blob/99fa1a865156649d66c9ae4fa6fe119b6be6206c/dask_geopandas/io/parquet.py#L22-L37)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/901537956/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/901943138",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-901943138",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 901943138,
        "node_id": "IC_kwDOAbcwm841wo9i",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-19T14:03:35Z",
        "updated_at": "2021-08-19T14:03:35Z",
        "author_association": "MEMBER",
        "body": ">do you think this \"allow parts/statistics to be collected in parallel\" would help with dask-geopandas, where they have to (re)-open each partition to read some metadata out of the partition's schema?\r\n\r\nThanks for sharing this application @TomAugspurger ! This is similar to how NVTabular needs to re-read the metadata to calculate the length of a dataset more efficiently (although our solution solution is quite a bit uglier than yours).\r\n\r\nGenerally speaking, the short-term proposal here does not solve your metadata problem by itself.  However, the refactor does provide an opportunity to expose an API for collecting custom metadata/statistics during the original pass.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/901943138/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/901975040",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-901975040",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 901975040,
        "node_id": "IC_kwDOAbcwm841wwwA",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-19T14:43:19Z",
        "updated_at": "2021-08-19T14:43:19Z",
        "author_association": "MEMBER",
        "body": "Like @TomAugspurger , I read through this, and everything made sense as I was reading it, but now I'm having difficulty packing it into a small enough space in my brain to reason about it effectively :)\r\n\r\nI'll add two notes here for context:\r\n\r\n1.  Whenever we call `compute` in dask collections code we break the ability to operate asynchronously.  This isn't a big deal for users (most of the time) but it can be annoying for testing.  \r\n\r\n    One solution to this would be to add an async `_read_parquet` method that `read_parquet` then called directly.  This would bring in the dask/distributed async system into dask/dask though, which we probably don't want to do.\r\n\r\n    My guess is that we just accept the loss of async for dask.dataframe and move on, but I wanted to bring this up so that we're at least intentional about that choice.\r\n\r\n2.  For simplicity's and consistency's sakes, I'll propose an option that we *always* read statistics from row groups and ignore the `_metadata` file.  Would this improve the state of the world?  Degrade it?  ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/901975040/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/902022717",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-902022717",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 902022717,
        "node_id": "IC_kwDOAbcwm841w8Y9",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-19T15:44:22Z",
        "updated_at": "2021-08-19T15:44:22Z",
        "author_association": "MEMBER",
        "body": "> My guess is that we just accept the loss of async for dask.dataframe and move on, but I wanted to bring this up so that we're at least intentional about that choice.\r\n\r\nYes - We probably want to accept that (like in `set_index`) we really **need** to use `compute` in `read_parquet` whenever the metadata needs to be collected manually.  I agree that we should be transparent about this decision.  \r\n\r\n> For simplicity's and consistency's sakes, I'll propose an option that we always read statistics from row groups and ignore the _metadata file. Would this improve the state of the world? Degrade it?\r\n\r\nI think it would degrade the state of the world a bit, but I am still feeling open minded.  My intuition is that we should preserve the \"_metadata-exists\" code path for users that want the ability to operate asynchronously.  If the _metadata file exists (and `ignore_metadata_file=False`), then it really feels like we should avoid the unnecessary `compute` call.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/902022717/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/902875301",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-902875301",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 902875301,
        "node_id": "IC_kwDOAbcwm8410Mil",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-20T18:23:09Z",
        "updated_at": "2021-08-20T18:23:09Z",
        "author_association": "MEMBER",
        "body": "Just a heads up that I submitted #8072 with a draft of some of the \"pyarrow-dataset\" changes I proposed.  If anyone has a system that was previously struggling with up-front metadata-processing, I'd be curious to know if the performance improves at all with that PR (cc @jrbourbeau).\r\n\r\nNote that you can play with both `ignore_metadata_file` and `files_per_metadata_tasks` to compare the different algorithms.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/902875301/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/919948930",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-919948930",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 919948930,
        "node_id": "IC_kwDOAbcwm8421U6C",
        "user": {
            "login": "jorisvandenbossche",
            "id": 1020496,
            "node_id": "MDQ6VXNlcjEwMjA0OTY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1020496?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jorisvandenbossche",
            "html_url": "https://github.com/jorisvandenbossche",
            "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
            "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
            "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
            "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
            "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
            "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-15T11:49:56Z",
        "updated_at": "2021-09-15T11:49:56Z",
        "author_association": "MEMBER",
        "body": "Thanks @rjzamora for this thorough wite-up and the proposal. Generally it certainly sounds as a good improvement of the current state. Will take a look at your open PR for this as well.\r\n\r\nPutting this a bit in a broader perspective, I am wondering to what extent this _really_ solves the problems, though. A large part of the problem comes from the single, large binary blob of the `_metadata` file that needs to be downloaded and parsed up-front (if you want to make use of it to avoid file listing on S3). While the proposed changes certainly move part of the effort (eg inferring divisions from the statistics) to a later stage and potentially parallelize that stage (i.e. `_make_partition_plan`), you still have the initial step of having to download and parse that big `_metadata` file that you don't avoid in this approach (now, to be fair, I don't really know how expensive those different stages are, and thus how much (or little) benefit it gives to move part to the `_make_partition_plan`. That might be interesting to profile).  \r\nIn the end you also say this at the top of the first post:\r\n\r\n> The origin of this pain seems to be the historical decision to rely on a shared _metadata file.\r\n\r\nMy feeling is that although we can further look into improving the current handling of the `_metadata` file, we are going to bump into inherit limits with this approach anyway when further scaling up. So to put it a bit bluntly, we should maybe also consider to \"give up\" on the `_metadata` approach _for huge datasets_, and look into more tightly integrating alternative approaches into dask.dataframe (eg Apache Iceberg, DeltaLake, .. i.e. approaches that were designed to overcome some of the inherent limitations of `_metadata`)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/919948930/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/927130374",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-927130374",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 927130374,
        "node_id": "IC_kwDOAbcwm843QuMG",
        "user": {
            "login": "MrPowers",
            "id": 2722395,
            "node_id": "MDQ6VXNlcjI3MjIzOTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2722395?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MrPowers",
            "html_url": "https://github.com/MrPowers",
            "followers_url": "https://api.github.com/users/MrPowers/followers",
            "following_url": "https://api.github.com/users/MrPowers/following{/other_user}",
            "gists_url": "https://api.github.com/users/MrPowers/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MrPowers/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MrPowers/subscriptions",
            "organizations_url": "https://api.github.com/users/MrPowers/orgs",
            "repos_url": "https://api.github.com/users/MrPowers/repos",
            "events_url": "https://api.github.com/users/MrPowers/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MrPowers/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-25T14:38:08Z",
        "updated_at": "2021-09-25T14:38:08Z",
        "author_association": "CONTRIBUTOR",
        "body": "@rjzamora - awesome write up!  \r\n\r\nDelta Lake writes Parquet files + metadata files.  Dask Parquet writers output Parquet files + metadata by default, similar to Delta Lake.\r\n\r\nAs @jorisvandenbossche mentioned, metadata can have the same scaling challenges as regular big data.  Delta Lake has solved the \"metadata scaling problem\".  They output one metadata file in the JSON file format per transaction and periodically compact the JSON files into Parquet.\r\n\r\nHere's an example Delta Lake transaction log metadata file for a write transaction:\r\n\r\n```json\r\n{\r\n  \"commitInfo\":{\r\n    \"timestamp\":1565119301357,\r\n    \"operation\":\"WRITE\",\r\n    \"operationParameters\":{\r\n      \"mode\":\"Overwrite\",\r\n      \"partitionBy\":\"[]\"\r\n    }\r\n  }\r\n}{\r\n  \"protocol\":{\r\n    \"minReaderVersion\":1,\r\n    \"minWriterVersion\":2\r\n  }\r\n}{\r\n  \"metaData\":{\r\n    \"id\":\"a3ca108e-3ba1-49dc-99a0-c9d29c8f1aec\",\r\n    \"format\":{\r\n      \"provider\":\"parquet\",\r\n      \"options\":{\r\n\r\n      }\r\n    },\r\n    \"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"first_name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"last_name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\r\n    \"partitionColumns\":[\r\n\r\n    ],\r\n    \"configuration\":{\r\n\r\n    },\r\n    \"createdTime\":1565119298882\r\n  }\r\n}{\r\n  \"add\":{\r\n    \"path\":\"part-00000-78f9c583-ea60-4962-af99-895f453dce23-c000.snappy.parquet\",\r\n    \"partitionValues\":{\r\n\r\n    },\r\n    \"size\":939,\r\n    \"modificationTime\":1565119299000,\r\n    \"dataChange\":true\r\n  }\r\n}\r\n```\r\n\r\nFrom this transaction log entry, we know that `part-00000-78f9c583-ea60-4962-af99-895f453dce23-c000.snappy.parquet` was added to the Delta Lake at 1565119301357.  It has first_name,last_name,country columns and is 939 bytes.\r\n\r\nThis transaction log format allows for time travel, schema evolution, backwards compatible Parquet file compaction (via `dataChange`), vacuuming, etc.  Any long term solution for this metadata problem should also allow for these features in my opinion.\r\n\r\nI am going to do some digging to figure out what data is stored in the Dask metadata file.  It'll be good to do a reconciliation of the different data Delta Lake & Dask are capturing in the metadata.\r\n\r\nI am going to write a blog post now to give concrete code examples that'll help with our metadata brainstorming.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/927130374/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/927992678",
        "html_url": "https://github.com/dask/dask/issues/8058#issuecomment-927992678",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8058",
        "id": 927992678,
        "node_id": "IC_kwDOAbcwm843UAtm",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-27T15:36:44Z",
        "updated_at": "2021-09-27T15:36:44Z",
        "author_association": "MEMBER",
        "body": "Thanks for engaging in this discussion @jorisvandenbossche and @MrPowers ! Sorry - I didn\u2019t realize there were comments I missed here.\u2028\u2028\r\n\r\n>Thanks\u00a0@rjzamora\u00a0for this thorough wite-up and the proposal. Generally it certainly sounds as a good improvement of the current state. Will take a look at your open PR for this as well.\r\n\r\n\u2028\u2028It would be super helpful to get your eyes on that PR if you have time :)\r\n\r\n>I am wondering to what extent this\u00a0really\u00a0solves the problems, though. A large part of the problem comes from the single, large binary blob of the\u00a0_metadata\u00a0file that needs to be downloaded and parsed up-front (if you want to make use of it to avoid file listing on S3). While the proposed changes certainly move part of the effort (eg inferring divisions from the statistics) to a later stage and potentially parallelize that stage (i.e.\u00a0_make_partition_plan), you still have the initial step of having to download and parse that big\u00a0_metadata\u00a0file that you don't avoid in this approach (now, to be fair, I don't really know how expensive those different stages are, and thus how much (or little) benefit it gives to move part to the\u00a0_make_partition_plan. That might be interesting to profile).\r\n\r\nYou have this all correct, but I believe my focus is on slightly different \u201cuse cases\u201d than you seem to be assuming here.  **I do not intend to improve the case where we have a proper _metadata file to  use for partition/division planning**. I am targeting the case where there is no _metadata file.  This may happen when (1) the dataset was written by some engine other than Dask, (2) The dataset was too large to efficiently write a single _metadata file, or (3) The _metadata file needs to be ignored by the Dask client for whatever reason; e.g. it is too large to process on a single process or the data is invalid.\r\n\r\nTherefore, the target of this plan (and of #8072 and #8092) is to redesign `read_metadata` to enable better performance for these non-_metadata cases (which I expect to become more and more common at scale).  Overall, the proposal is to merge a pretty straightforward refactor to reduce the (currently dramatic) performance hit that we now see when the _metadata file is missing or ignored.\r\n\r\n>we should maybe also consider to \"give up\" on the\u00a0_metadata\u00a0approach\u00a0for huge datasets, and look into more tightly integrating alternative approaches into dask.dataframe (eg Apache Iceberg, DeltaLake, .. i.e. approaches that were designed to overcome some of the inherent limitations of\u00a0_metadata)\r\n\r\n\u2028\u2028I am in complete agreement that Dask must throw away the assumption that the _metadata file will always be present.  That is one motivation for this proposal.  Also, it is a good point that alternative approaches like Apache Iceberg & DeltaLake may be the best answer for true \u201cbig data\u201d use cases.  I am **very** supportive of adding Dask support for other metadata-logging approaches, but I also think that it is important that Dask have reliable support for **raw** parquet datasets at relatively large scale.\r\n\r\n\u2028\u2028@MrPowers - It is great that you looking into Delta Lake! I am very excited to have your expertise involved here.  My gut tells me that many Dask users will benefit from something like the Delta Lake approach to metadata scaling. With that said, my current preference would be to expose any new system-specific IO approach as distinct `read_*`/`write_*` APIs in Dask. We could certainly leverage the same Engine/parquet logic, but I\u2019d like to avoid adding a new default global-metadata approach for raw `read`/`write_parquet`.  I\u2019m sure my preference could change here, and you may already have a separate API in mind, but I just wanted to share my thoughts.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/927992678/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]