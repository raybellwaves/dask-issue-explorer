[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1337519507",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1337519507",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1337519507,
        "node_id": "IC_kwDOAbcwm85PuO2T",
        "user": {
            "login": "ncclementi",
            "id": 7526622,
            "node_id": "MDQ6VXNlcjc1MjY2MjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7526622?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ncclementi",
            "html_url": "https://github.com/ncclementi",
            "followers_url": "https://api.github.com/users/ncclementi/followers",
            "following_url": "https://api.github.com/users/ncclementi/following{/other_user}",
            "gists_url": "https://api.github.com/users/ncclementi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ncclementi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ncclementi/subscriptions",
            "organizations_url": "https://api.github.com/users/ncclementi/orgs",
            "repos_url": "https://api.github.com/users/ncclementi/repos",
            "events_url": "https://api.github.com/users/ncclementi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ncclementi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-05T14:53:26Z",
        "updated_at": "2022-12-05T14:53:26Z",
        "author_association": "MEMBER",
        "body": "@charlesbluca and @rjzamora  for visibility, and maybe you can recommend the next steps.  ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1337519507/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1338240177",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1338240177",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1338240177,
        "node_id": "IC_kwDOAbcwm85Pw-yx",
        "user": {
            "login": "jakirkham",
            "id": 3019665,
            "node_id": "MDQ6VXNlcjMwMTk2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jakirkham",
            "html_url": "https://github.com/jakirkham",
            "followers_url": "https://api.github.com/users/jakirkham/followers",
            "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
            "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
            "organizations_url": "https://api.github.com/users/jakirkham/orgs",
            "repos_url": "https://api.github.com/users/jakirkham/repos",
            "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jakirkham/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-05T22:07:56Z",
        "updated_at": "2022-12-05T22:07:56Z",
        "author_association": "MEMBER",
        "body": "cc @pentschev",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1338240177/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1341101671",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1341101671",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1341101671,
        "node_id": "IC_kwDOAbcwm85P75Zn",
        "user": {
            "login": "lrlunin",
            "id": 1849145,
            "node_id": "MDQ6VXNlcjE4NDkxNDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1849145?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lrlunin",
            "html_url": "https://github.com/lrlunin",
            "followers_url": "https://api.github.com/users/lrlunin/followers",
            "following_url": "https://api.github.com/users/lrlunin/following{/other_user}",
            "gists_url": "https://api.github.com/users/lrlunin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lrlunin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lrlunin/subscriptions",
            "organizations_url": "https://api.github.com/users/lrlunin/orgs",
            "repos_url": "https://api.github.com/users/lrlunin/repos",
            "events_url": "https://api.github.com/users/lrlunin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lrlunin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-07T15:06:23Z",
        "updated_at": "2022-12-07T15:06:23Z",
        "author_association": "NONE",
        "body": "Hello everyone. Thanks for paying attention to the discovered bug.\r\n\r\nI used the given advice https://github.com/dask/dask-image/issues/275#issuecomment-1333225569 with `da.map_blocks`. This works without array types related problems. However, I got troubles with running out of memory. As far as I understand managing RAM and prevent the out of memory issues is one of dask key features. In a sense that despite the size of data being processed dask will manage load of the data without any chance that RAM will be overflowed.\r\n\r\nI guess that the running of computations on GPU should be implemented in a similar way, isn't it?\r\n\r\nI tried to learn more about CUDA architecture and how parallelism on GPU really works. It looks rather complex but I watched several videos. As far as I understand each call of a python-cuda function (for example `cucim.skimage.feature.corner_peaks` or any other function from the package) will be firstly initialize CUDA context with some time overhead. Then the function will be applied to each separate picture (let's say I have an array of 2000 pictures with 400px x 400px resolution) in a \"single-core\" mode because CUDA is not aware of all other 1999 pictures, right? So there will be no significant performance gain with GPU because numerous CUDA cores will not be involved in computation. So I need to call the function for each of 2000 picture, what  actually dask was used for. Do I understand correctly (more or less) what is happening here? I would really appreciate if @pentschev will comment this post. Thanks in advance! \r\n\r\nDoes it even possible then to vectorize a complex GPU function built from many other GPU function and manage GPU memory as well with dask?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1341101671/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1341444104",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1341444104",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1341444104,
        "node_id": "IC_kwDOAbcwm85P9NAI",
        "user": {
            "login": "jakirkham",
            "id": 3019665,
            "node_id": "MDQ6VXNlcjMwMTk2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jakirkham",
            "html_url": "https://github.com/jakirkham",
            "followers_url": "https://api.github.com/users/jakirkham/followers",
            "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
            "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
            "organizations_url": "https://api.github.com/users/jakirkham/orgs",
            "repos_url": "https://api.github.com/users/jakirkham/repos",
            "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jakirkham/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-07T19:03:54Z",
        "updated_at": "2022-12-07T19:03:54Z",
        "author_association": "MEMBER",
        "body": "@lrlunin, glad to hear you are exploring these things. Should we make a new issue about `corner_peaks` to discuss that specifically? Maybe this could be [a cuCIM issue]( https://github.com/rapidsai/cucim/issues/new/choose )?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1341444104/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1341848643",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1341848643",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1341848643,
        "node_id": "IC_kwDOAbcwm85P-vxD",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-08T01:34:13Z",
        "updated_at": "2022-12-08T01:34:36Z",
        "author_association": "MEMBER",
        "body": "> However, I got troubles with running out of memory. As far as I understand managing RAM and prevent the out of memory issues is one of dask key features.\r\n\r\nYou may already be doing this, but are you using the most recent versions of dask & distributed (version 2022.11.0 or above)? There have been some recent changes that dramatically improves memory use during computation (more details in this blogpost written by Gabe Joseph [\"Reducing memory usage in Dask workloads by 80%\"](https://blog.dask.org/2022/11/15/queuing)).\r\n\r\nYou should comment on [the discussion thread here](https://github.com/dask/distributed/discussions/7128), where Gabe is asking for community feedback on workloads that do (or don't!) work. It might be possible to adjust the config setting values to better suit your workload, but you'll get better advice if you post on that thread. Try it first with the default settings for dask & distributed 2022.11.0 though, if you haven't already done that.\r\n\r\nI hope this doesn't feel too much like we keep sending you off to different places, but it is the best way to get each aspect of your problem (gufunc cupy compatibility, memory management, & CUDA parallelism questions) in front of the right audience for advice. I appreciate you doing this.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1341848643/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1343068456",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1343068456",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1343068456,
        "node_id": "IC_kwDOAbcwm85QDZko",
        "user": {
            "login": "lrlunin",
            "id": 1849145,
            "node_id": "MDQ6VXNlcjE4NDkxNDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1849145?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lrlunin",
            "html_url": "https://github.com/lrlunin",
            "followers_url": "https://api.github.com/users/lrlunin/followers",
            "following_url": "https://api.github.com/users/lrlunin/following{/other_user}",
            "gists_url": "https://api.github.com/users/lrlunin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lrlunin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lrlunin/subscriptions",
            "organizations_url": "https://api.github.com/users/lrlunin/orgs",
            "repos_url": "https://api.github.com/users/lrlunin/repos",
            "events_url": "https://api.github.com/users/lrlunin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lrlunin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-08T17:36:46Z",
        "updated_at": "2022-12-08T17:36:46Z",
        "author_association": "NONE",
        "body": "Well, I now installed a newer environment:\r\n\r\n```\r\ndask 2022.11.1\r\ndask-cuda 22.12.0\r\ndask-image 2022.9.0 (still the same)\r\ncucim 22.10\r\n```\r\nI now have this function which contains one CUDA function inside:\r\n```python\r\ndef gpu_convolve_peak_map(img):\r\n    # img.shape here is (1, 400, 400). therefore I need to get img[0]\r\n    result = cuscipy.fftconvolve(img[0], cp.ones([2,2]), mode='same') \r\n    # result is a (400, 400) array\r\n    return result\r\n```\r\nif I load a single image and then use apply the function:\r\n```python\r\nimages_cp = dask_image.imread.imread(\"/home/lrlunin/50/220421_run_d0_f0_50_f00001.tiff\", arraytype=\"cupy\")\r\nf = da.map_blocks(gpu_convolve_peak_map, images_cp, dtype=cp.float32)\r\nf.compute()\r\n```\r\nit works without any errors!\r\n\r\nWhen I now try to load 2 images (or more) I obtain an error:\r\n```python\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[170], line 1\r\n----> 1 f.compute()\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/base.py:315, in DaskMethodsMixin.compute(self, **kwargs)\r\n    291 def compute(self, **kwargs):\r\n    292     \"\"\"Compute this dask collection\r\n    293 \r\n    294     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    313     dask.base.compute\r\n    314     \"\"\"\r\n--> 315     (result,) = compute(self, traverse=False, **kwargs)\r\n    316     return result\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/base.py:601, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n    600 results = schedule(dsk, keys, **kwargs)\r\n--> 601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/base.py:601, in <listcomp>(.0)\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n    600 results = schedule(dsk, keys, **kwargs)\r\n--> 601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:1282, in finalize(results)\r\n   1280 while isinstance(results2, (tuple, list)):\r\n   1281     if len(results2) > 1:\r\n-> 1282         return concatenate3(results)\r\n   1283     else:\r\n   1284         results2 = results2[0]\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:5269, in concatenate3(arrays)\r\n   5267 if concatenate_lookup.dispatch(type(advanced)) is not np.concatenate:\r\n   5268     x = unpack_singleton(arrays)\r\n-> 5269     return _concatenate2(arrays, axes=list(range(x.ndim)))\r\n   5271 ndim = ndimlist(arrays)\r\n   5272 if not ndim:\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:404, in _concatenate2(arrays, axes)\r\n    402     return arrays\r\n    403 if len(axes) > 1:\r\n--> 404     arrays = [_concatenate2(a, axes=axes[1:]) for a in arrays]\r\n    405 concatenate = concatenate_lookup.dispatch(\r\n    406     type(max(arrays, key=lambda x: getattr(x, \"__array_priority__\", 0)))\r\n    407 )\r\n    408 if isinstance(arrays[0], dict):\r\n    409     # Handle concatenation of `dict`s, used as a replacement for structured\r\n    410     # arrays when that's not supported by the array library (e.g., CuPy).\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:404, in <listcomp>(.0)\r\n    402     return arrays\r\n    403 if len(axes) > 1:\r\n--> 404     arrays = [_concatenate2(a, axes=axes[1:]) for a in arrays]\r\n    405 concatenate = concatenate_lookup.dispatch(\r\n    406     type(max(arrays, key=lambda x: getattr(x, \"__array_priority__\", 0)))\r\n    407 )\r\n    408 if isinstance(arrays[0], dict):\r\n    409     # Handle concatenation of `dict`s, used as a replacement for structured\r\n    410     # arrays when that's not supported by the array library (e.g., CuPy).\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:418, in _concatenate2(arrays, axes)\r\n    416     return ret\r\n    417 else:\r\n--> 418     return concatenate(arrays, axis=axes[0])\r\n\r\nFile <__array_function__ internals>:180, in concatenate(*args, **kwargs)\r\n\r\nFile cupy/_core/core.pyx:1473, in cupy._core.core._ndarray_base.__array__()\r\n\r\nTypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.\r\n```\r\nwhile `f.shape` has correct dimensions `(2, 400, 400)`.\r\n\r\n\r\nI guessed that my function now reduces dimensions of input `img` from (1, 400, 400) to output `result` (400, 400) and it is the issue. So as @GenevieveBuckley advised me to use `drop_axis`. So did I:\r\n```python\r\nf = da.map_blocks(gpu_convolve_peak_map, images_cp, drop_axis=0, dtype=cp.float32)\r\n```\r\nnow it causes no errors, BUT `f.shape` is now `(400, 400)` instead of expected `(2, 400, 400)` for two images. If I use `drop_axis = 1` the output of `f.shape` is `(2, 400)`.\r\n\r\nI guess it is not an expected behavior. I can possibly wrap `result` in the `gpu_convolve_peak_map` function in a way it will have `(1, 400, 400)` dimension. But in the same time I think it is not a good idea due to memory reallocation etc.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1343068456/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1343083745",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1343083745",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1343083745,
        "node_id": "IC_kwDOAbcwm85QDdTh",
        "user": {
            "login": "lrlunin",
            "id": 1849145,
            "node_id": "MDQ6VXNlcjE4NDkxNDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1849145?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lrlunin",
            "html_url": "https://github.com/lrlunin",
            "followers_url": "https://api.github.com/users/lrlunin/followers",
            "following_url": "https://api.github.com/users/lrlunin/following{/other_user}",
            "gists_url": "https://api.github.com/users/lrlunin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lrlunin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lrlunin/subscriptions",
            "organizations_url": "https://api.github.com/users/lrlunin/orgs",
            "repos_url": "https://api.github.com/users/lrlunin/repos",
            "events_url": "https://api.github.com/users/lrlunin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lrlunin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-08T17:51:08Z",
        "updated_at": "2022-12-08T17:57:49Z",
        "author_association": "NONE",
        "body": "I was way too curious to check whether my approach would work. So I modified the function as follows:\r\n```python\r\n def gpu_convolve_peak_map(img):\r\n    result = cuscipy.fftconvolve(img[0], cp.ones([2,2]), mode='same')\r\n    result_exp = cp.expand_dims(result, 0)\r\n    return result_exp\r\n```\r\nBut I still believe that it is a bad practice.\r\n\r\nNow I am able to apply this function for 36 images with `map_blocks` without any drop_axis! It works so far!\r\n\r\n\r\nEach image has size of 640.158 bytes. If I take now 2000 images (approx 1.3GB) with my GTX 1070Ti (8GB) I'll get the error:\r\n ```python\r\nOutOfMemoryError                          Traceback (most recent call last)\r\nCell In[238], line 1\r\n----> 1 f.compute()\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/base.py:315, in DaskMethodsMixin.compute(self, **kwargs)\r\n    291 def compute(self, **kwargs):\r\n    292     \"\"\"Compute this dask collection\r\n    293 \r\n    294     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    313     dask.base.compute\r\n    314     \"\"\"\r\n--> 315     (result,) = compute(self, traverse=False, **kwargs)\r\n    316     return result\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/base.py:601, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n    600 results = schedule(dsk, keys, **kwargs)\r\n--> 601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/base.py:601, in <listcomp>(.0)\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n    600 results = schedule(dsk, keys, **kwargs)\r\n--> 601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:1282, in finalize(results)\r\n   1280 while isinstance(results2, (tuple, list)):\r\n   1281     if len(results2) > 1:\r\n-> 1282         return concatenate3(results)\r\n   1283     else:\r\n   1284         results2 = results2[0]\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:5263, in concatenate3(arrays)\r\n   5261 try:\r\n   5262     x = unpack_singleton(arrays)\r\n-> 5263     return _concatenate2(arrays, axes=tuple(range(x.ndim)))\r\n   5264 except TypeError:\r\n   5265     pass\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:404, in _concatenate2(arrays, axes)\r\n    402     return arrays\r\n    403 if len(axes) > 1:\r\n--> 404     arrays = [_concatenate2(a, axes=axes[1:]) for a in arrays]\r\n    405 concatenate = concatenate_lookup.dispatch(\r\n    406     type(max(arrays, key=lambda x: getattr(x, \"__array_priority__\", 0)))\r\n    407 )\r\n    408 if isinstance(arrays[0], dict):\r\n    409     # Handle concatenation of `dict`s, used as a replacement for structured\r\n    410     # arrays when that's not supported by the array library (e.g., CuPy).\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:404, in <listcomp>(.0)\r\n    402     return arrays\r\n    403 if len(axes) > 1:\r\n--> 404     arrays = [_concatenate2(a, axes=axes[1:]) for a in arrays]\r\n    405 concatenate = concatenate_lookup.dispatch(\r\n    406     type(max(arrays, key=lambda x: getattr(x, \"__array_priority__\", 0)))\r\n    407 )\r\n    408 if isinstance(arrays[0], dict):\r\n    409     # Handle concatenation of `dict`s, used as a replacement for structured\r\n    410     # arrays when that's not supported by the array library (e.g., CuPy).\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/dask/array/core.py:418, in _concatenate2(arrays, axes)\r\n    416     return ret\r\n    417 else:\r\n--> 418     return concatenate(arrays, axis=axes[0])\r\n\r\nFile ~/miniconda3/envs/rapids-22.10-new-dask/lib/python3.8/site-packages/cupy/_manipulation/join.py:60, in concatenate(tup, axis, out, dtype, casting)\r\n     58     tup = [m.ravel() for m in tup]\r\n     59     axis = 0\r\n---> 60 return _core.concatenate_method(tup, axis, out, dtype, casting)\r\n\r\nFile cupy/_core/_routines_manipulation.pyx:586, in cupy._core._routines_manipulation.concatenate_method()\r\n\r\nFile cupy/_core/_routines_manipulation.pyx:662, in cupy._core._routines_manipulation.concatenate_method()\r\n\r\nFile cupy/_core/core.pyx:136, in cupy._core.core.ndarray.__new__()\r\n\r\nFile cupy/_core/core.pyx:224, in cupy._core.core._ndarray_base._init()\r\n\r\nFile cupy/cuda/memory.pyx:742, in cupy.cuda.memory.alloc()\r\n\r\nFile cupy/cuda/memory.pyx:1419, in cupy.cuda.memory.MemoryPool.malloc()\r\n\r\nFile cupy/cuda/memory.pyx:1440, in cupy.cuda.memory.MemoryPool.malloc()\r\n\r\nFile cupy/cuda/memory.pyx:1120, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc()\r\n\r\nFile cupy/cuda/memory.pyx:1141, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc()\r\n\r\nFile cupy/cuda/memory.pyx:1379, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc()\r\n\r\nOutOfMemoryError: Out of memory allocating 1,280,000 bytes (allocated so far: 5,363,304,448 bytes).\r\n```\r\n@jakirkham I think the real issue that dask tries to load all 2000 images into GPU despite the fact its memory pool is not big enough. So this `OutOfMemory` issue is not a bug in `cucim` or any other CUDA dependent function but in dask \"distribution pipeline\".\r\n\r\nUPD: approximately 600 images can be loaded without running out of memory.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1343083745/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1373705422",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1373705422",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1373705422,
        "node_id": "IC_kwDOAbcwm85R4RTO",
        "user": {
            "login": "lrlunin",
            "id": 1849145,
            "node_id": "MDQ6VXNlcjE4NDkxNDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1849145?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lrlunin",
            "html_url": "https://github.com/lrlunin",
            "followers_url": "https://api.github.com/users/lrlunin/followers",
            "following_url": "https://api.github.com/users/lrlunin/following{/other_user}",
            "gists_url": "https://api.github.com/users/lrlunin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lrlunin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lrlunin/subscriptions",
            "organizations_url": "https://api.github.com/users/lrlunin/orgs",
            "repos_url": "https://api.github.com/users/lrlunin/repos",
            "events_url": "https://api.github.com/users/lrlunin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lrlunin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-06T14:22:41Z",
        "updated_at": "2023-01-06T14:22:41Z",
        "author_association": "NONE",
        "body": "Could I help you performing other tests or maybe look up some kind of related source code?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1373705422/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1375664092",
        "html_url": "https://github.com/dask/dask/issues/9714#issuecomment-1375664092",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9714",
        "id": 1375664092,
        "node_id": "IC_kwDOAbcwm85R_vfc",
        "user": {
            "login": "pentschev",
            "id": 4398246,
            "node_id": "MDQ6VXNlcjQzOTgyNDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4398246?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pentschev",
            "html_url": "https://github.com/pentschev",
            "followers_url": "https://api.github.com/users/pentschev/followers",
            "following_url": "https://api.github.com/users/pentschev/following{/other_user}",
            "gists_url": "https://api.github.com/users/pentschev/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pentschev/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pentschev/subscriptions",
            "organizations_url": "https://api.github.com/users/pentschev/orgs",
            "repos_url": "https://api.github.com/users/pentschev/repos",
            "events_url": "https://api.github.com/users/pentschev/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pentschev/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-09T13:59:51Z",
        "updated_at": "2023-01-09T13:59:51Z",
        "author_association": "MEMBER",
        "body": "Sorry for the delay, I was only now able to come to this.\r\n\r\n> We could make a feature request issue in the [cupy repository](https://github.com/cupy/cupy/issues), for supporting the signature keyword argument in cupy.vectorize. I don't know where that request would fit in to their other priorities.\r\n\r\nThis assessment looks correct, `cupy.vectorize` doesn't implement `signature=`, and without that Dask won't be able to complete that operation.\r\n\r\n> If that was implemented, Dask could then consider replacing the three numpy-specific lines in [dask/array/gufunc.py](https://github.com/dask/dask/blob/main/dask/array/gufunc.py) with some sort of dispatching solution, so that np.vectorize is used by numpy backed dask arrays, and cupy.vectorize is used by cupy backed dask arrays.\r\n\r\nThis would _probably_ work without any changes if `signature=` was implemented, `__array_function__` would dispatch that to CuPy given the input type.\r\n\r\n> I think the real issue that dask tries to load all 2000 images into GPU despite the fact its memory pool is not big enough. So this OutOfMemory issue is not a bug in cucim or any other CUDA dependent function but in dask \"distribution pipeline\".\r\n\r\nAs for running out of GPU memory, one thing you could try for your single-GPU setup is [Dask-CUDA](https://docs.rapids.ai/api/dask-cuda/nightly/quickstart.html) with [CUDA managed memory via RMM](https://docs.rapids.ai/api/dask-cuda/nightly/api.html#cmdoption-dask-cuda-worker-rmm-managed-memory). This will allow the GPU to automatically spill do the system memory when it's going low.\r\n\r\nUsing `--rmm-managed-memory` has limitations for more complex setups (namely UCX support and spilling to disk if the system memory is also running low), so in such cases one would need to use [Dask-CUDA for device spilling](https://docs.rapids.ai/api/dask-cuda/nightly/spilling.html) together with an [RMM pool](https://docs.rapids.ai/api/dask-cuda/nightly/examples/best-practices.html#gpu-memory-management).",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1375664092/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]