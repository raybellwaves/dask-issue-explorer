{
    "url": "https://api.github.com/repos/dask/dask/issues/6587",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/6587/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/6587/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/6587/events",
    "html_url": "https://github.com/dask/dask/issues/6587",
    "id": 691066545,
    "node_id": "MDU6SXNzdWU2OTEwNjY1NDU=",
    "number": 6587,
    "title": "Unable to write parquet with \"empty\" categorical column with pyarrow engine",
    "user": {
        "login": "odovad",
        "id": 3889145,
        "node_id": "MDQ6VXNlcjM4ODkxNDU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3889145?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/odovad",
        "html_url": "https://github.com/odovad",
        "followers_url": "https://api.github.com/users/odovad/followers",
        "following_url": "https://api.github.com/users/odovad/following{/other_user}",
        "gists_url": "https://api.github.com/users/odovad/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/odovad/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/odovad/subscriptions",
        "organizations_url": "https://api.github.com/users/odovad/orgs",
        "repos_url": "https://api.github.com/users/odovad/repos",
        "events_url": "https://api.github.com/users/odovad/events{/privacy}",
        "received_events_url": "https://api.github.com/users/odovad/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 23,
    "created_at": "2020-09-02T14:24:04Z",
    "updated_at": "2022-04-08T15:24:29Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I encountered an issue when working on a concat involving multiples files.\r\nIt seems that the issue happens also when applied on only one file with a partition containing a categorical column with no values.\r\n\r\nMaybe this issue is linked : [Issue 6243](https://github.com/dask/dask/issues/6243)\r\nIf I am not mistaken, it worked with Pyarrow 0.17.1\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\n\r\nnames = [ 'name']\r\naddress = names\r\n\r\ndf1 = dd.from_pandas(pd.DataFrame({'name':names}), npartitions=1)\r\ndf2 = dd.from_pandas(pd.DataFrame({'address':address}), npartitions=1)\r\ndf1['name'] = df1['name'].astype('category')\r\n\r\ndf = dd.concat([df1, df2])\r\ndel df['address']\r\n\r\ndf.to_parquet('does_not_work')\r\n```\r\n\r\nThe error that is raised : \r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in _append_row_groups(metadata, md)\r\n     33     try:\r\n---> 34         metadata.append_row_groups(md)\r\n     35     except RuntimeError as err:\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.FileMetaData.append_row_groups()\r\n\r\nRuntimeError: AppendRowGroups requires equal schemas.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-46-4faae0e6b3fa> in <module>\r\n     12 del df['address']\r\n     13 \r\n---> 14 df.to_parquet('does_not_work')\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/core.py in to_parquet(self, path, *args, **kwargs)\r\n   3948         from .io import to_parquet\r\n   3949 \r\n-> 3950         return to_parquet(self, path, *args, **kwargs)\r\n   3951 \r\n   3952     @derived_from(pd.DataFrame)\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/core.py in to_parquet(df, path, engine, compression, write_index, append, ignore_divisions, partition_on, storage_options, write_metadata_file, compute, compute_kwargs, schema, **kwargs)\r\n    506         if compute_kwargs is None:\r\n    507             compute_kwargs = dict()\r\n--> 508         out = out.compute(**compute_kwargs)\r\n    509     return out\r\n    510 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/base.py in compute(self, **kwargs)\r\n    165         dask.base.compute\r\n    166         \"\"\"\r\n--> 167         (result,) = compute(self, traverse=False, **kwargs)\r\n    168         return result\r\n    169 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/base.py in compute(*args, **kwargs)\r\n    445         postcomputes.append(x.__dask_postcompute__())\r\n    446 \r\n--> 447     results = schedule(dsk, keys, **kwargs)\r\n    448     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    449 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/threaded.py in get(dsk, result, cache, num_workers, pool, **kwargs)\r\n     74                 pools[thread][num_workers] = pool\r\n     75 \r\n---> 76     results = get_async(\r\n     77         pool.apply_async,\r\n     78         len(pool._pool),\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/local.py in get_async(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\r\n    484                         _execute_task(task, data)  # Re-execute locally\r\n    485                     else:\r\n--> 486                         raise_exception(exc, tb)\r\n    487                 res, worker_id = loads(res_info)\r\n    488                 state[\"cache\"][key] = res\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/local.py in reraise(exc, tb)\r\n    314     if exc.__traceback__ is not tb:\r\n    315         raise exc.with_traceback(tb)\r\n--> 316     raise exc\r\n    317 \r\n    318 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/local.py in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\r\n    220     try:\r\n    221         task, data = loads(task_info)\r\n--> 222         result = _execute_task(task, data)\r\n    223         id = get_id()\r\n    224         result = dumps((result, id))\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/core.py in _execute_task(arg, cache, dsk)\r\n    119         # temporaries by their reference count and can execute certain\r\n    120         # operations in-place.\r\n--> 121         return func(*(_execute_task(a, cache) for a in args))\r\n    122     elif not ishashable(arg):\r\n    123         return arg\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/utils.py in apply(func, args, kwargs)\r\n     27 def apply(func, args, kwargs=None):\r\n     28     if kwargs:\r\n---> 29         return func(*args, **kwargs)\r\n     30     else:\r\n     31         return func(*args)\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in write_metadata(parts, fmd, fs, path, append, **kwargs)\r\n   1034                 i_start = 1\r\n   1035             for i in range(i_start, len(parts)):\r\n-> 1036                 _append_row_groups(_meta, parts[i][0][\"meta\"])\r\n   1037             with fs.open(metadata_path, \"wb\") as fil:\r\n   1038                 _meta.write_metadata_file(fil)\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in _append_row_groups(metadata, md)\r\n     35     except RuntimeError as err:\r\n     36         if \"requires equal schemas\" in str(err):\r\n---> 37             raise RuntimeError(\r\n     38                 \"Schemas are inconsistent, try using \"\r\n     39                 '`to_parquet(..., schema=\"infer\")`, or pass an explicit '\r\n\r\nRuntimeError: Schemas are inconsistent, try using `to_parquet(..., schema=\"infer\")`, or pass an explicit pyarrow schema.\r\n``` \r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.24.0\r\n- Pyarrow version: 1.0.1\r\n- Python version: 3.8\r\n- Operating System: Dask-Dev image\r\n- Install method (conda, pip, source) : Dask-Dev image",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/6587/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/6587/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}