[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/658207686",
        "html_url": "https://github.com/dask/dask/issues/6410#issuecomment-658207686",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6410",
        "id": 658207686,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODIwNzY4Ng==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-14T14:19:08Z",
        "updated_at": "2020-07-14T14:19:08Z",
        "author_association": "MEMBER",
        "body": "I'm curious, do you think that the groupby apply method work work in your\nuse case?\n\nOn Tue, Jul 14, 2020 at 2:16 AM Luca Venturini <notifications@github.com>\nwrote:\n\n> Example on the Dask documentation:\n>\n> >>> import itertools as it\n> >>> collect_list = dd.Aggregation(\n> ...     name=\"collect_list\",\n> ...     chunk=lambda s: s.apply(list),\n> ...     agg=lambda s0: s0.apply(lambda chunks: list(it.chain.from_iterable(chunks))),\n> ... )\n> >>> df.groupby('g').agg(collect_list)\n>\n> However, if the dataframe contains categories:\n>\n> >>> import dask.dataframe as dd; import pandas as pd>>> import itertools as it>>> df = pd.DataFrame().assign(index=[0, 0, 1, 1], values=[\"a\", \"a\", \"b\", \"b\"], ex=[1, 2, 3, 4]).astype({\"values\": \"category\"})>>> ddf = dd.from_pandas(df, chunksize=2)>>> collect_list = dd.Aggregation(\n>                      name=\"collect_list\", chunk=lambda s: s.apply(list),\n>                      agg=lambda s0: s0.apply(lambda chunks: list(it.chain.from_iterable(chunks))))>>> ddf.groupby([\"index\", \"values\"]).agg(collect_list)TypeError\n> [...]ValueError: Metadata inference failed in `_agg_finalize`.\n> You have supplied a custom function and Dask is unable todetermine the type of output that that function returns.\n> To resolve this please provide a meta= keyword.The docstring of the Dask function you ran should have more information.\n> Original error is below:------------------------TypeError(\"'float' object is not iterable\")\n> [...]\n>\n> This can be solved by having a different aggregation function:\n>\n> >>> def convert_to_list(c):\n>     if (c != c).all():   # c != c is a trick to find NaN values\n>         return [np.nan]\n>     f = [_ for _ in c if _ == _]  # Again, only select non-NaN values\n>     f = [_ if isinstance(_, list) else [_] for _ in f]\n>     return list(it.chain.from_iterable(f))\n>\n> >>> collect_list = dd.Aggregation(\n>      name=\"collect_list\", chunk=lambda s: s.apply(list),\n>      agg=lambda s0: s0.apply(lambda chunks: internal(chunks)))\n> >>> ddf.groupby([\"index\", \"values\"]).agg(collect_list).compute()\n>                   ex\n> index values\n> 0     a       [1, 2]\n>       b        [nan]\n> 1     a        [nan]\n>       b       [3, 4]\n>\n> *As a more general note*, this kind of operation (passing the whole list\n> of values per-group to the \"aggregation\" function) is a passage that is\n> sometimes necessary in data analysis, and it would be good in my humble\n> opinion that good examples on how to force Dask to pass the whole list of\n> values to the final aggregation stage would be both easier and more\n> prominent on the documentation.\n>\n> As a user-case, I found this edge-case while trying to calculate the MAD\n> on a column of a grouped dask.DataFrame (using the\n> scipy.stats.median_absolute_deviation function). Having to implement a\n> map-reduce version of this standard function is far less optimal than\n> simply passing the list of values to the end-stage (as it happens in pandas\n> in an equivalent scenario).\n>\n> *Environment*:\n>\n>    - Dask version: 2.17.2\n>    - Python version: 3.7.6 | Conda\n>    - Operating System: WSL Ubuntu\n>    - Install method (conda, pip, source): Conda\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/6410>, or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACKZTGZGDEAQKRAK3POLIDR3QO5PANCNFSM4OZKHJPQ>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/658207686/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/658231415",
        "html_url": "https://github.com/dask/dask/issues/6410#issuecomment-658231415",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6410",
        "id": 658231415,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODIzMTQxNQ==",
        "user": {
            "login": "lucventurini",
            "id": 8897821,
            "node_id": "MDQ6VXNlcjg4OTc4MjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8897821?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucventurini",
            "html_url": "https://github.com/lucventurini",
            "followers_url": "https://api.github.com/users/lucventurini/followers",
            "following_url": "https://api.github.com/users/lucventurini/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucventurini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucventurini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucventurini/subscriptions",
            "organizations_url": "https://api.github.com/users/lucventurini/orgs",
            "repos_url": "https://api.github.com/users/lucventurini/repos",
            "events_url": "https://api.github.com/users/lucventurini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucventurini/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-14T15:00:54Z",
        "updated_at": "2020-07-14T15:10:41Z",
        "author_association": "NONE",
        "body": "@mrocklin \r\n\r\nIt does, although thank you for prodding me - it helped me find a bug!\r\n\r\nSee the following:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\nimport numpy as np\r\nimport numpy.random\r\nimport scipy.stats\r\nimport itertools as it\r\n\r\n\r\ndef _mad1(chunks):\r\n    c = chunks.apply(list)\r\n    return c\r\n\r\n\r\ndef  _mad2(grouped):\r\n    def internal(c):\r\n        if (c != c).all():\r\n            return [np.nan]\r\n        f = [_ for _ in c if _ == _]\r\n        f = [_ if isinstance(_, list) else [_] for _ in f]\r\n        return list(it.chain.from_iterable(f))\r\n    chunks = grouped.apply(internal)\r\n    return chunks\r\n\r\n\r\ndef _mad3(grouped):\r\n    chunks = grouped.apply(lambda s: np.nan if len(s) == 0 else scipy.stats.median_absolute_deviation(s, nan_policy=\"omit\"))\r\n    return chunks\r\n\r\n\r\nmad = dd.Aggregation(\"mad\", chunk=_mad1, agg=_mad2, finalize=_mad3)\r\n\r\n\r\nindex = list(it.chain.from_iterable([[_] * 100 for _ in range(10)]))\r\ncats = [\"A\"] * 500 + [\"B\"] * 500\r\n# Create ten groups of dummy variables, normally distributed,\r\n# with stdev increasing from 0 to 100 in 10-step increment\r\nr = np.concatenate([np.random.normal(0, x, 100) for x in range(0, 100, 10)]) \r\ndf = pd.DataFrame().assign(i=index, cat=cats, ran=r).astype({\"cat\": \"category\"})\r\n\r\nprint(\"Pandas\")\r\nprint(df.groupby([\"i\", \"cat\"]).agg({\"ran\": scipy.stats.median_absolute_deviation}).dropna())\r\n\r\nprint(\"Dask\")\r\nddf = dd.from_pandas(df, chunksize=100)\r\nprint(ddf.groupby([\"i\", \"cat\"]).agg({\"ran\": mad}).dropna().compute())\r\n```\r\n\r\nThe results of the pandas and dask versions are the same.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/658231415/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]