{
    "url": "https://api.github.com/repos/dask/dask/issues/7933",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/7933/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/7933/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/7933/events",
    "html_url": "https://github.com/dask/dask/issues/7933",
    "id": 952324449,
    "node_id": "MDU6SXNzdWU5NTIzMjQ0NDk=",
    "number": 7933,
    "title": "High Level Expressions",
    "user": {
        "login": "mrocklin",
        "id": 306380,
        "node_id": "MDQ6VXNlcjMwNjM4MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mrocklin",
        "html_url": "https://github.com/mrocklin",
        "followers_url": "https://api.github.com/users/mrocklin/followers",
        "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
        "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
        "organizations_url": "https://api.github.com/users/mrocklin/orgs",
        "repos_url": "https://api.github.com/users/mrocklin/repos",
        "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mrocklin/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2156573524,
            "node_id": "MDU6TGFiZWwyMTU2NTczNTI0",
            "url": "https://api.github.com/repos/dask/dask/labels/highlevelgraph",
            "name": "highlevelgraph",
            "color": "8c24d6",
            "default": false,
            "description": "Issues relating to HighLevelGraphs."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 65,
    "created_at": "2021-07-25T17:56:18Z",
    "updated_at": "2022-09-22T19:21:46Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "### Summary\r\n\r\nWe should make space for high level query optimization.  There are a couple of ways to do this.  This issue includes motivation, a description of two approaches, and some thoughts on trade-offs.\r\n\r\n### Motivation\r\n\r\nThere are a variety of situations where we might want to rewrite a user's code.\r\n\r\n#### Dataframes\r\n\r\n1. Column projection, as in `dd.read_parquet(...)[\"x\"]` -> `dd.read_parquet(..., columns=[\"x\"])`\r\n2. Predicate pushdown (same as above)\r\n3. High level expression fusion (what we do today with blockwise)\r\n4. Pushing length calls down through elementwise calls\r\n5. Pushing filters earlier in a computation\r\n6. ...\r\n\r\n#### Arrays\r\n\r\n1.  Automatic rechunking at the beginning of a computation based on the end of the computation\r\n2. Slicing\r\n\r\n### History\r\n\r\nToday there is no real place where we capture a user's intent or their lines of code.  We immediately create a task graph for the requested operation, dump it into a mapping, and create a new dask.dataframe.DataFrame or dask.array.Array instance.  That instance has no knowledge of what created it, or what created the other input dataframes on which it depends.\r\n\r\nThis was a good choice early on.  It made it easy for us to quickly implement lots of complex operations without thinking about a class hierarchy for them.  This choice followed on from the choices of Blaze, where we started with high level expressions, but got a bit stuck because they constrained our thinking (and no one really cares about high level query optimization for a system that they don't use.\r\n\r\nHowever today we have maybe reached a point where our \"keep everything low-level and simple\" strategy has hit a limit, and now we're curious about how best to bolt on a high level expression system.  Doing this smoothly given the current system is hard.  I see two ways out.\r\n\r\n### High Level Graph layers\r\n\r\nWe *do* have a record of what operations came before us in the high level graph layers.  Currently the API of layers is very generic.  They must be a mapping that adheres to the Dask graph spec, and they must be serializable in a certain way.  There are some consistent subclasses, like blockwise, which enable high level optimizations which have proven useful.  \r\n\r\nThere isn't really much structure here though, and as a result it's hard to do interesting optimizations.  For example it would be nice if we could change a layer at the very bottom of the graph, and then replay all of the operations on that input over again to see how they would change.  High level layers today don't have enough shared structure that we know how to do this.\r\n\r\nI like High Level Graph Layers because it gives us a space to hijack and add in all sorts of complex machinery, but without affecting the user-facing Dataframe class.  We would have to add a lot more structure here though, and we'll always be working around the collection class, which is a drawback.\r\n\r\n### Collection subclasses\r\n\r\nI'm going to focus on an alternative that is a bit more radical.  We could also have every user call generate a DataFrame subclass.  There would still be a DataFrame instance that took in a generic graph/divisions/meta, but that would be mostly for backwards compatibility.  Instead most dataframe operations would produce subclasses that had a well-defined common structure, as well as more custom attributes for their specific operation.  Let's look at a couple of examples.\r\n\r\n```python\r\n# API calls just create instances.  All logic happens there.\r\ndef read_parquet(file, columns, filters):\r\n    return ReadParquet(file, columns, filters)\r\n\r\nclass ReadParquet(DataFrame):\r\n    args = [\"file\", \"column\", \"filters\"]  # List of arguments to use when reconstructing\r\n    inputs = []  # List of arguments that are DataFrame objects\r\n\r\n    def __init__(self, file, columns, filters):\r\n        self.file  = file\r\n        self.columns = columns\r\n        self.filters = filters\r\n\r\n        self.divisions, self.meta = # do a bit of work on metadata\r\n        \r\n    def _generate_dask_layer(self) -> dict:\r\n        ...\r\n\r\nclass ColumnProjection(DataFrame):\r\n    args = [\"dataframe\", \"columns\"]\r\n    inputs = [\"dataframe\"]\r\n\r\n    def __init__(self, dataframe, columns):\r\n        self.dataframe = dataframe\r\n        self.columns = columns\r\n        self._meta = self.dataframe._meta[columns]\r\n\r\n    def _generate_dask_layer(self) -> dict:\r\n        ...\r\n\r\nclass Add(DataFrame):\r\n    args = [\"left\", \"right\"]\r\n\r\n    def __init__(self, left, right):\r\n        self.left = left\r\n        self.right = right\r\n        self.inputs = []\r\n        if is_dask_collection(left):\r\n            self.inputs.append(\"left\")\r\n        if is_dask_collection(right):\r\n            self.inputs.append(\"right\")\r\n\r\n        self._meta = ...\r\n        self._divisions = ...\r\n\r\n    def _generate_dask_layer(self) -> dict:\r\n        ...\r\n\r\n\r\n```\r\n\r\nAs folks familiar with SymPy will recall, having attributes like args/inputs around makes it possible to re-generate a DataFrame automatically.  So if we do something like the following:\r\n\r\n```python\r\ndf = dd.read_parquet(...)\r\nz = df.x + df.y\r\n```\r\n\r\nThen this turns into an expression tree like the following:\r\n\r\n```python\r\nAdd(\r\n    ColumnProjection(\r\n        ReadParquet(..., columns=None),\r\n        \"x\",\r\n    ),\r\n    ColumnProjection(\r\n        ReadParquet(..., columns=None),\r\n        \"y\",\r\n    ),\r\n)\r\n```\r\n\r\nWe can then traverse this tree (which is easy because we have a list of all attributes that are dask collections in the `inputs` attribute) and apply optimizaitons (which is easy because we can easily reconstruct layers because we have the `args` attribute).  \r\n\r\nFor example the `ColumnProjection` class may have an optimization method like the following:\r\n\r\n```python\r\nclass ColumnProjection(DataFrame):\r\n    ... # continued from above\r\n\r\n    def _optimize(self) -> DataFrame:\r\n        if isinstance(self.dataframe, ReadParquet):\r\n            args = {arg: getattr(self.dataframe, arg) for arg in self.dataframe.args}\r\n            args[\"columns\"] = self.columns\r\n            return ReadParquet(**args)._optimize()\r\n\r\n        # here is another optimization, just to show variety\r\n        if isinstance(self.datafarme, ColumnProjection):  # like df[[\"x\", \"y\"][\"x\"]\r\n            return ColumnProjection(self.dataframe.dataframe, self.columns)._optimize()\r\n\r\n        # no known optimizations, optimize all inputs and then reconstruct (this would live in a superclass)\r\n        args = []\r\n        for arg in self.args:\r\n            if arg in self.inputs:\r\n                arg = getattr(self, arg)._optimize()\r\n            else:\r\n                arg = getattr(self, arg)\r\n            args.append(arg)\r\n        \r\n        return type(self)(*args)\r\n```\r\n\r\nThis is just one way of doing a traversal, using a method on a class.  We can do fancier things.  Mostly what I wanted to show here was that because we have args/inputs and class types it's fairly easy to encode optimizations and rewrite things.\r\n\r\nWhat's notable here is that we aren't generating the graph, or even any semblance of the graph ahead of time.  At any point where we run code that requires something like `_meta` or `_divisions` from an input we stop all opportunities to change the graph under that stage.  This is OK for our internal code.  We can defer graph generation I think.  \r\n\r\nI think that the main advantage to this approach is that we can easily reconstruct expressions given newly modified inputs.  I think that this is fundamentally what is lacking with our current HLG layer approach.  \r\n\r\nHowever, this would also be a significant deviation from how Dask works today.  It's likely that this would affect all downstream projects that subclass Dask arrays/dataframes today (RAPIDS, Pint, yt, ...).  I think that that's probably ok.  Those groups will, I think, understand.\r\n\r\n### Personal Thoughts\r\n\r\nI've historically been against doing collection subclasses (the second approach), but after thinking about this for a while I think I'm now more in favor.  It seems like maybe we've arrived at the time when the benefits to doing this outweigh the complexity costs.  I think that this is motivated also by the amount of complexity that we're encountering walking down the HLG path.  \r\n\r\nHowever, I do think that this will be hard for us to implement in an incremental way.  *If* we want to go down this path we probably need to think about an approach that lets the current `DataFrame` API persist for backwards compatibility with downstream projects (they wouldn't get any benefits of this system though) and a way where we can move over collection subclasses incrementally.\r\n\r\ncc @rjzamora @jcrist ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/7933/reactions",
        "total_count": 10,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 3,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/7933/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}