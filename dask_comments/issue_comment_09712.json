[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1335839806",
        "html_url": "https://github.com/dask/dask/issues/9712#issuecomment-1335839806",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9712",
        "id": 1335839806,
        "node_id": "IC_kwDOAbcwm85Pn0w-",
        "user": {
            "login": "jrbourbeau",
            "id": 11656932,
            "node_id": "MDQ6VXNlcjExNjU2OTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jrbourbeau",
            "html_url": "https://github.com/jrbourbeau",
            "followers_url": "https://api.github.com/users/jrbourbeau/followers",
            "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}",
            "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions",
            "organizations_url": "https://api.github.com/users/jrbourbeau/orgs",
            "repos_url": "https://api.github.com/users/jrbourbeau/repos",
            "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jrbourbeau/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-02T20:58:48Z",
        "updated_at": "2022-12-02T20:58:48Z",
        "author_association": "MEMBER",
        "body": "Thanks for writing this up @rjzamora -- I look forward to grokking things here\r\n\r\ncc @fjetter as I suspect you'll also find this interesting ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1335839806/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1337619944",
        "html_url": "https://github.com/dask/dask/issues/9712#issuecomment-1337619944",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9712",
        "id": 1337619944,
        "node_id": "IC_kwDOAbcwm85PunXo",
        "user": {
            "login": "fjetter",
            "id": 8629629,
            "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fjetter",
            "html_url": "https://github.com/fjetter",
            "followers_url": "https://api.github.com/users/fjetter/followers",
            "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
            "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
            "organizations_url": "https://api.github.com/users/fjetter/orgs",
            "repos_url": "https://api.github.com/users/fjetter/repos",
            "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fjetter/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-05T15:59:19Z",
        "updated_at": "2022-12-05T15:59:19Z",
        "author_association": "MEMBER",
        "body": "I'm not super happy with file_groups + granularity but I would midn too much.\r\n\r\nRegarding the metadata fragment collection, I'm not too happy about this. I would prefer having `dd.read_parquet` to be entirely lazy. How about moving this out into a dedicated function? This is something typically only power users will use and it is not uncommon in more sophisticated dataset formats to have a `Table` object that you need to run `Table.plan` on before you can do any loading.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1337619944/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1337715585",
        "html_url": "https://github.com/dask/dask/issues/9712#issuecomment-1337715585",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9712",
        "id": 1337715585,
        "node_id": "IC_kwDOAbcwm85Pu-uB",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-05T16:49:02Z",
        "updated_at": "2022-12-05T16:49:02Z",
        "author_association": "MEMBER",
        "body": "Thanks for taking a look at this @fjetter !\r\n\r\n>I'm not super happy with file_groups + granularity but I would midn too much.\r\n\r\nI can certainly understand this perspective.\r\n\r\nFrom your earlier comments in https://github.com/dask/dask/pull/9637, it seems like you would expect the default behavior to be: (1) break all files into row-groups, and then (2) map row-groups into each output partition according to some `chunksize`/`blocksize` argument. Is this correct, or would you expect something different? How would you want the user to specify: \"simple file-to-partition mapping is good for me\"?\r\n\r\n>Regarding the metadata fragment collection, I'm not too happy about this. I would prefer having dd.read_parquet to be entirely lazy.\r\n\r\nI assume you mean that you would prefer for us **not** to call `compute` on `Delayed` objects to collect the metadata fragments from the filesystem? There is no way for us to create a `DataFrame` collection without parsing this metadata (unless we only support a trivial file-to-partition mapping, with no partition-size consideration). Therefore, I'm taking your preference to be that we do the mandatory metadata gathering on the client (serially).\r\n\r\nMinor clarification: We would (probably) only want to do parallel-metadata gathering on remote filesystems, where doing so can easily cut the latency of using `dd.read_parquet` (before computing) by an order of magnitude or more.\r\n\r\n>How about moving this out into a dedicated function? This is something typically only power users will use and it is not uncommon in more sophisticated dataset formats to have a Table object that you need to run Table.plan on before you can do any loading.\r\n\r\nI'm not sure that we can take advantage of an alternative workflow like `Table.plan` without veering from the `pd.read_parquet` call the API is modeled after.  Can you share a usage example for what you have in mind?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1337715585/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1342907582",
        "html_url": "https://github.com/dask/dask/issues/9712#issuecomment-1342907582",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9712",
        "id": 1342907582,
        "node_id": "IC_kwDOAbcwm85QCyS-",
        "user": {
            "login": "fjetter",
            "id": 8629629,
            "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fjetter",
            "html_url": "https://github.com/fjetter",
            "followers_url": "https://api.github.com/users/fjetter/followers",
            "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
            "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
            "organizations_url": "https://api.github.com/users/fjetter/orgs",
            "repos_url": "https://api.github.com/users/fjetter/repos",
            "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fjetter/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-08T15:31:34Z",
        "updated_at": "2022-12-08T15:31:34Z",
        "author_association": "MEMBER",
        "body": "> From your earlier comments in https://github.com/dask/dask/pull/9637, it seems like you would expect the default behavior to be: (1) break all files into row-groups, and then (2) map row-groups into each output partition according to some chunksize/blocksize argument. Is this correct, or would you expect something different? How would you want the user to specify: \"simple file-to-partition mapping is good for me\"?\r\n\r\nI think the default behavior should be to not break up files if possible. Having one large range read instead of many, many small ones is typically preferred.\r\nI think chunksize (or an equivalent, differently named) should work as a \"maximum partition size, if possible\" which we use to break up files in RG batches s.t. we obey to the maximum.\r\nThere is the argument to be had if \"adjacent\" (however you define adjacency\" files should be used to fill up too small buckets. I believe you just removed this feature and I'm ok with this. I'm not sure if this optimization is worth it.\r\n\r\n> How would you want the user to specify: \"simple file-to-partition mapping is good for me\"?\r\n\r\nHow about\r\n\r\n`chunksize=None`\r\n\r\nwhereas the default is something like\r\n\r\n`chunksize=\"200MB\"`\r\n\r\n> I assume you mean that you would prefer for us not to call compute on Delayed objects to collect the metadata fragments from the filesystem? \r\n> There is no way for us to create a DataFrame collection without parsing this metadata (unless we only support a trivial file-to-partition mapping, with no partition-size consideration). Therefore, I'm taking your preference to be that we do the mandatory metadata gathering on the client (serially).\r\n\r\nI'm not sure what kind of metadata files we're talking about here. Are we still writing/reading this \"global metadata file that includes all parquet statistics\" thing? I thought we got rid of this a while ago. I have opinions about this as well but don't want to blow up this conversation even further ;D\r\n\r\n> I'm not sure that we can take advantage of an alternative workflow like Table.plan without veering from the pd.read_parquet call the API is modeled after. Can you share a usage example for what you have in mind?\r\n\r\nWell, a couple of thoughts\r\n\r\n1. I understand that we try to model the pandas API wherever possible. If this restriction is too uncomfortable for this specific API we could consider not to do it. I understand how dask should be a drop in replacement but in reality this call always needs to be adjusted with extra storage kwargs, a different path, etc. I'm not saying we should do this, I'm saying we _could_ if there are reasons for it.\r\n2. What I have in mind is a \"simple\" API that could be using `dd.read_parquet` as is but with way fewer kwargs. this would not do anything fancy but would allow easy onboarding and would more or less map to pandas API\r\n\r\nFor advanced users we could offer a different API. This would also allow us to more easily be compatible with more sophisticated technologies. For an example, see https://iceberg.apache.org/docs/0.13.1/python-api-intro/#scanning which is a workflow that applies filters and selections on a `scan` or `table` object which later is translated into tasks (in our case we would not return tasks but the DataFrame. Similar methods can be applied to control partitioning. \r\nThis object can also be used as a metadata cache in case you want to perform multiple queries in succession. I think exposing a similar kind of object or accepting those of third party libraries would be a nice addition and would separate storage IO from graph computation.\r\n\r\nThis might be out of scope for your proposal so feel free to ignore this section",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1342907582/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1343282526",
        "html_url": "https://github.com/dask/dask/issues/9712#issuecomment-1343282526",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9712",
        "id": 1343282526,
        "node_id": "IC_kwDOAbcwm85QEN1e",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-08T20:00:32Z",
        "updated_at": "2022-12-08T20:00:32Z",
        "author_association": "MEMBER",
        "body": ">I think the default behavior should be to not break up files if possible. Having one large range read instead of many, many small ones is typically preferred.\r\n\r\nThat makes sense. It is certainly most efficient for the engine to read many contiguous row-groups at once (as long as those row-groups fit comfortably in memory). I think this is the goal for default behavior regardless of whether we choose to \u201cbreak up\u201d files or not.\r\n\r\nNote that when I say \u201c\u201dbreak\u201d up files, I just mean that we would parse the size of each row-group in the dataset (which we already need to do to calculate the uncompressed storage size of each file), and use that per-row-group information to make sure that we are not producing output partitions that are larger than the maximum-desired size.\r\n\r\nIn https://github.com/dask/dask/pull/9637, we make the assumption that the size of all row-groups and files in the dataset can be approximated by the metadata in the first file, and we use that metadata to set a maximum row-group count for each partition (split_row_groups).  If split_row_groups ends up being set to 10, for example, then a file with 15 row groups will be mapped into two partitions (one with 10 row-groups, and another with 5).  This algorithm can be tweaked to break the 15 row-group file into 8 and 7 row-group files instead. However, that PR does not address the case that the file and row-group sizes vary significantly across the dataset. \r\n\r\n>I think chunksize (or an equivalent, differently named) should work as a \"maximum partition size, if possible\" which we use to break up files in RG batches s.t. we obey to the maximum.\u2028\u2028\r\n\r\nSound good. This is how things work now (and how things *should* work in the \u201cproposed\u201d API).\r\n\r\n>There is the argument to be had if \"adjacent\" (however you define adjacency\" files should be used to fill up too small buckets. I believe you just removed this feature and I'm ok with this. I'm not sure if this optimization is worth it.\r\n\r\nThis feature improves the performance of hive/directory-partitioned data reading in RAPIDS by orders of magnitude. However, I can understand the interest being more limited for CPU.\r\n\r\n>For advanced users we could offer a different API\u2026 This might be out of scope for your proposal so feel free to ignore this section\r\n\r\n\u2028\u2028I suppose it may **not** be completely out of scope if we can agree on a user-friendly API to cover the \u201cadvanced\u201d partitioning cases that I\u2019m hoping to (somehow/someday) capture. For example, I can imagine a world in which we continue deprecating most \u201cadvanced\u201d options from `dd.read_parquet`, and introduce new `Dataset`/`DatasetScan` classes to make custom partitioning easier to attack. This is just my first (rough) attempt at imagining something like this, but I'm sure I would also be happy with an API like\u2026\r\n\r\n```python\r\n# Initialize a lightweight `ParquetDataset(Dataset)` object\r\ndataset = dd.dataset(\u201cmy/path\u201d, engine=\u201cpyarrow\u201d, format=\u201cparquet\u201d)\r\n\r\n# Could also use `parquet_dataset` as an alias for `dataset(\u2026, format=\u201cparquet\u201d)`\r\ndataset = dd.parquet_dataset(\u201cmy/path\u201d)\r\n\r\n# Scan the dataset to produce a `ParquetDatasetScan(DatasetScan)` object.\r\n#     By default, this would scan the dataset in parallel (using delayed),\r\n#     and would gather row-group statistics\r\nscan = dataset.scan(filters=[(\u201cyear\u201d, \u201c==\u201d, 2022)], granularity=\u201cfile\u201d)\r\n\r\n# Optionally add filters, projections, etc. (Not sure about this?)\r\n#     These methods would simply filter/project the metadata\r\n#     already stored in memory within `ParquetDatasetScan`.\r\n#     The most performant time to specify filters would be in the\r\n#     initial `dataset.scan` operation.\r\nscan = scan.add_filter([(\u201cmonth\u201d, \u201c==\u201d, 12)])\r\n\r\n# Convert a `ParquetDatasetScan` into a DataFrame collection.\r\n#     This method will just use the metadata already stored in\r\n#     memory to create a DataFrame with `from_map`.\r\nddf = scan.to_dataframe(blocksize=\u201c1GB\u201d, groups=[\u201cyear\u201d, \u201cmonth\u201d])\r\n```\r\n\r\nI suppose `ParquetDataset` could also provide a `to_dataframe` method\r\nwith an implicit scan operation, so that\r\n\r\n```python\r\nddf = dd.parquet_dataset(\u201cmy/path\u201d).scan().to_dataframe()\r\n```\r\n\r\ncould also be written as\r\n\r\n```python\r\nddf = dd.parquet_dataset(\u201cmy/path\u201d).to_dataframe()\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1343282526/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]