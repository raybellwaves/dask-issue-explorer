{
    "url": "https://api.github.com/repos/dask/dask/issues/4457",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/4457/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/4457/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/4457/events",
    "html_url": "https://github.com/dask/dask/issues/4457",
    "id": 406105408,
    "node_id": "MDU6SXNzdWU0MDYxMDU0MDg=",
    "number": 4457,
    "title": "Reshuffling large amount of parquet files blows up worker memory",
    "user": {
        "login": "talz",
        "id": 538365,
        "node_id": "MDQ6VXNlcjUzODM2NQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/538365?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/talz",
        "html_url": "https://github.com/talz",
        "followers_url": "https://api.github.com/users/talz/followers",
        "following_url": "https://api.github.com/users/talz/following{/other_user}",
        "gists_url": "https://api.github.com/users/talz/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/talz/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/talz/subscriptions",
        "organizations_url": "https://api.github.com/users/talz/orgs",
        "repos_url": "https://api.github.com/users/talz/repos",
        "events_url": "https://api.github.com/users/talz/events{/privacy}",
        "received_events_url": "https://api.github.com/users/talz/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 6,
    "created_at": "2019-02-03T16:33:24Z",
    "updated_at": "2019-04-30T15:09:39Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I have a large dataset split up to around 400k different parquet files.\r\nI want to reshuffle to group data by a specific column.\r\n\r\nLets say schema is (all columns are text):\r\nusername, first name, last name, location\r\n\r\nAnd files are grouped by last name (i have a file with data for each last name) - file size ranges between 5K and 5M.\r\n\r\nNow I need to group files by location (do bulk operations on each location)\r\nSince the entire dataset doesn't fit in the shared memory of all my machines in the cluster (I have about 40GB shared amongst ~10 workers) - i thought processing chunks of files, reshuffle them and write intermediate results partitioned based on the 'location' column - creating multiple partitions for each column value.\r\nThen go through each location, and repartition them to have better fitting files (less files, not too large)\r\n\r\nBut the problem is that after a few chunks the memory in the workers starts to fill up (causing workers to slow down) until eventual freeze.\r\n\r\nMy code looks like this:\r\n```\r\ndef fdata_read(path, file_data):\r\n    df = pd.read_parquet(path)\r\n    df['first_name'] = file_data.first_name\r\n    df['last_name'] = file_data.last_name\r\n    return df\r\n    \r\ndef process_chunk(file_iterator, n=100):\r\n    file_data = itertools.islice(file_iterator, n)\r\n    if not file_data:\r\n        return None\r\n    df = dd.from_delayed([delayed(fdata_read)(file_data) for file_data in file_list])\r\n    df = df.apply(lambda x: pd.Series({\r\n        'location': x.full_location.split('_')[0].lower(), \r\n        'hash': x.hash[:8],\r\n        'first_name': x.first_name,\r\n        'last_name': x.last_name,\r\n    }), axis=1, meta={'location': 'object', 'hash': 'object', 'first_name': 'object', 'last_name': 'object})\r\n\r\n    \r\n    df = df.set_index('location', drop=False)\r\n    d = df.to_parquet('s3://my_bucket/result', partition_on=[location'], compute=False)\r\n    return d\r\n\r\nwhile True:\r\n   f = process_chunk(file_iterator)\r\n   if not f: break\r\n   f.compute()\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/4457/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/4457/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}