{
    "url": "https://api.github.com/repos/dask/dask/issues/8616",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8616/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8616/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8616/events",
    "html_url": "https://github.com/dask/dask/issues/8616",
    "id": 1112981857,
    "node_id": "I_kwDOAbcwm85CVsFh",
    "number": 8616,
    "title": "[DISCUSSION] Layer-by-Layer Graph Execution",
    "user": {
        "login": "rjzamora",
        "id": 20461013,
        "node_id": "MDQ6VXNlcjIwNDYxMDEz",
        "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rjzamora",
        "html_url": "https://github.com/rjzamora",
        "followers_url": "https://api.github.com/users/rjzamora/followers",
        "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
        "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
        "organizations_url": "https://api.github.com/users/rjzamora/orgs",
        "repos_url": "https://api.github.com/users/rjzamora/repos",
        "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rjzamora/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        },
        {
            "id": 2156573524,
            "node_id": "MDU6TGFiZWwyMTU2NTczNTI0",
            "url": "https://api.github.com/repos/dask/dask/labels/highlevelgraph",
            "name": "highlevelgraph",
            "color": "8c24d6",
            "default": false,
            "description": "Issues relating to HighLevelGraphs."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 10,
    "created_at": "2022-01-24T18:13:42Z",
    "updated_at": "2022-03-14T01:57:06Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "### TLDR\r\n\r\n**Rough Proposal**: In order to overlap graph communication with scheduling and computation, we should enable **partial** task-graph execution. Further details are discussed below, but the ultimate goals is to completely remove the need to send HLGs to the scheduler (while hopefully improving Dask's ability to handle large-scale graphs).\r\n\r\n**NOTE:** The ideas here are mostly a combination of suggestions made in the past by others like @madsbk, @gjoseph92, @ian-r-rose (and probably others)\r\n\r\n## Background\r\n\r\nMost methods in the Dask-Dataframe and Dask-Array APIs produce an abstract `HighLevelGraph` (HLG) instead of a fully-materialized dictionary-based task graph. The general purpose of this HLG approach is two-fold:\r\n\r\n1. To enable critical graph-optimization operations **before** the full (dictionary-based) task graph is materialized. These optimizations include blockwise task fusion, culling, and partial-IO optimizations like column projection (and hopefully predicate pushdown in the near future).\r\n2. To avoid sending large materialized task graphs over the wire from the client process to the scheduler. We expected (and indeed observe) much smaller graph-communication latency when the HLG-to-dictionary materialization was moved from the client to the scheduler.\r\n\r\nAlthough HLGs have demonstrated some success in achieving both of the above goals, there have also been clear challenges:\r\n\r\n- HLG fusion has relied on existing optimizations in `Blockwise`, requiring all \u201cfusible\u201d DataFrame and Array Layers (including IO) to be reformulated as `Blockwise`-based HLG Layers.  The complexity in `Blockwise` has made this transition a bit difficult.  I will not focus on this specific challenge in the remainder of this proposal (but I felt it was important enough to mention).\r\n- The need to deserialize and materialize HLGs on the scheduler has proved to be a complete headache (see [distributed#5581](https://github.com/dask/distributed/issues/5581) and [this summary](https://github.com/dask/dask/pull/8468#discussion_r771752382)). The existing serialization and communication infrastructure was designed with the low-level (dictionary) graph in mind.  We quickly discovered that it can be quite painful to construct a low-level graph on the scheduler without being able to call `pickle.loads` on functions and non-msgpack serializable arguments (a security concern).\r\n- Sending HLGs to the scheduler reduced the graph communication time (important for high-latency client-scheduler connections), but it barely made a dent in typical end-to-end run times for systems with decent client-scheduler latency. This is because large client-side graph materialization costs were simply converted into comparable scheduler-side graph materialization costs.  We originally hoped that Cython or C++ on the scheduler would make this graph-materialization cost negligible, but I am skeptical that this is something we can/should rely on.\r\n\r\nWhat I take away from the challenges above is that: (1) Sending HLGs to the scheduler is a headache, and (2) The only reason we are still doing this is to minimize client-to-scheduler communication latency.\r\n\r\n## Proposal\r\n\r\nAfter a long (but fun) discussion with @madsbk, we generally agreed that we may be able to avoid sending HLGs to the scheduler altogether. Our primary (and probably lone) reason for dealing with HLGs on the scheduler now is graph-communication latency.  Therefore, we will no longer have any motivation to send the HLG to the scheduler if this overhead is sufficiently reduced or hidden.  One possible way to do this is by overlapping graph communication with scheduling and execution.  That is, we may be able to graph communication delays by taking the following steps:\r\n\r\n### Step 1 - Enable partial graph scheduling\r\n\r\nThe first and most-critical step is to ensure that the scheduler is capable of executing a partial graph.  I am hoping someone like @madsbk will have the bandwidth and interest to prototype this machinery, but I would expect the client to be able to submit a graph without specifying the keys it wants to `get`.  Instead, the client should be able to mark specific keys as active dependencies.  Note that I am very open to suggestions here, but the overall goal is to begin assigning tasks to workers before the scheduler has finished receiving the full task graph.\r\n\r\n### Step 2 - Send the materialized graph Layer-by-Layer\r\n\r\nOnce the scheduler has a mechanism for processing partial task-graphs, we will want to add a mechanism in the client to actually send large graphs in phases. For the initial pass, I propose that we simply use the existing HLG `Layer` as a single \u201csub-graph\u201d unit.  In other words, after the HLG optimization pass, we can simply send the materialized graph to the scheduler one `Layer` at a time.  After we confirm that this \u201cworks,\u201d we can avoid the obvious small-message problem by aggregating adjacent `Layers` on the client until an optimal sub-graph size is reached. \r\n\r\nIf we are able to implement these first two steps without critical road blocks, then I suspect we will be highly motivated to discuss the future (or lack thereof) of HLG serialization.  If we can decouple the serialization design from that of HLGs, then I suspect we will find it much easier to improve the two separately.\r\n\r\n## Possible Follow-On Work\r\n\r\nAlthough Layer-by-Layer graph communication/execution may largely reduce our need to send HLGs to the scheduler, we may still want to send something other than the low-level graph is \u201cspecial cases.\u201d  For example, if the user is willing to **opt in** to \u201cfull\u201d (`dumps` and `loads`) pickle support on the scheduler, then we may want to enable the client to pickle a single generator function for an HLG Layer, and the scheduler could simply unpickle this object and then pass in a required set of output keys to get the necessary tasks.  This would allow the scheduler to break up the materialization and scheduling of extremely large graphs (even when a single layer contains billions of keys).  Such an optimization would clearly take considerable design and development effort, but it may provide us with a reasonable path toward the handling of \u201cextreme-scale\u201d graphs (note that this idea belongs entirely to @madsbk, so he makes want to clarify and/or expand)\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8616/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8616/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}