{
    "url": "https://api.github.com/repos/dask/dask/issues/9850",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9850/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9850/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9850/events",
    "html_url": "https://github.com/dask/dask/issues/9850",
    "id": 1549126170,
    "node_id": "I_kwDOAbcwm85cVcoa",
    "number": 9850,
    "title": "dask.dataframe.read_*: change default blocksize to 128 MiB",
    "user": {
        "login": "crusaderky",
        "id": 6213168,
        "node_id": "MDQ6VXNlcjYyMTMxNjg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/crusaderky",
        "html_url": "https://github.com/crusaderky",
        "followers_url": "https://api.github.com/users/crusaderky/followers",
        "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
        "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
        "organizations_url": "https://api.github.com/users/crusaderky/orgs",
        "repos_url": "https://api.github.com/users/crusaderky/repos",
        "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
        "received_events_url": "https://api.github.com/users/crusaderky/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 996497175,
            "node_id": "MDU6TGFiZWw5OTY0OTcxNzU=",
            "url": "https://api.github.com/repos/dask/dask/labels/good%20second%20issue",
            "name": "good second issue",
            "color": "5319e7",
            "default": false,
            "description": "Clearly described, educational, but less trivial than \"good first issue\"."
        },
        {
            "id": 3798602129,
            "node_id": "LA_kwDOAbcwm87iahGR",
            "url": "https://api.github.com/repos/dask/dask/labels/enhancement",
            "name": "enhancement",
            "color": "C2E0C6",
            "default": true,
            "description": "Improve existing functionality or make things work better"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-01-19T13:09:45Z",
    "updated_at": "2023-01-24T15:17:31Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "- Related: https://github.com/dask/dask/issues/9849.\r\n\r\nOversized chunks cause the RAM-per-host requirements of a workload to shoot up.\r\nHigh variance in the chunk size of a dataset confuses the duration prediction algorithm in the distributed scheduler - and probably a few other heuristics that I can't think of right now.\r\n\r\nIt is exceedingly common in real life to have to deal with loading files from disk that are far too large for comfort and/or vary wildly in size among the same dataset. I had recently to load up a dataset where the files varied between 22 MiB and 830 MiB each. Once decompressed, the 830 MiB partitions took in excees of 6 GiB RAM each to load up.\r\n\r\nNew dask users are unlikely to be immediately aware of this problem, which will cause their computatio to crash, and they may not notice that there's a simple parameter that would fix their problem.\r\n*Experienced* dask uses are also not guaranteed to notice that there are a dozen of huge files among a dataset of 1000+ modestly sized ones.\r\n\r\n# Proposed design\r\nIn an effort to improve user experience, I believe we should change the default `blocksize` for all `dask.dataframe.read_*` functions from False (1 file = 1 partition) to 128 MiB, coherently with the default chunk size in dask.array.\r\nNote that 128 MiB is the size on disk, which can be substantially different from the size in memory. However I believe that ending up with somewhat too large or too small, but constrained, partitions is much preferable to having to deal with unconstrained partitions.\r\n\r\n### Special cases\r\n\r\n- `read_parquet` and `read_json` (for non-line json) don't support `blocksize`. It should be added (#9849).\r\n- For the two above file formats, print a warning:\r\n> Some partitions on disk are up to 830 MiB in size; they will be split into 128 MiB ones but only after loading them up at once. This will cause your RAM usage to spike during data load. If you produced this dataset yourself, consider saving it in smaller partitions (e.g. by calling `repartition` just before writing to disk). Explicitly set the blocksize parameter to disable this warning. \r\n\r\n(only print the warning if there are partitions exceeding 256 MiB).\r\n\r\n- `read_hdf` has `chunksize` (number of rows) instead of `blocksize` (number of bytes). We should add a mutually-exclusive `blocksize` parameter, which would crudely estimate `size of 1 row in bytes = size of the file in bytes / number of rows in the file`. The default when neither is explicitly stated should change from `chunksize=1_000_000` to `blocksize=\"128MiB\"`.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9850/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9850/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}