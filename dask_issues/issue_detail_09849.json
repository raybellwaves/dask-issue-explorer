{
    "url": "https://api.github.com/repos/dask/dask/issues/9849",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9849/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9849/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9849/events",
    "html_url": "https://github.com/dask/dask/issues/9849",
    "id": 1549080908,
    "node_id": "I_kwDOAbcwm85cVRlM",
    "number": 9849,
    "title": "Add blocksize to read_parquet and read_json (non-line json)",
    "user": {
        "login": "crusaderky",
        "id": 6213168,
        "node_id": "MDQ6VXNlcjYyMTMxNjg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/crusaderky",
        "html_url": "https://github.com/crusaderky",
        "followers_url": "https://api.github.com/users/crusaderky/followers",
        "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
        "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
        "organizations_url": "https://api.github.com/users/crusaderky/orgs",
        "repos_url": "https://api.github.com/users/crusaderky/repos",
        "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
        "received_events_url": "https://api.github.com/users/crusaderky/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        },
        {
            "id": 3798450420,
            "node_id": "LA_kwDOAbcwm87iZ8D0",
            "url": "https://api.github.com/repos/dask/dask/labels/feature",
            "name": "feature",
            "color": "b0f0fa",
            "default": false,
            "description": "Something is missing"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 13,
    "created_at": "2023-01-19T12:43:17Z",
    "updated_at": "2024-04-01T01:47:14Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "# Business case\r\nIt is exceedingly frequent to have to load oversized partitions from disk, regardless of file format.\r\nThis typically happens due to the partitions on disk being the output of groupby or being created by some algorithm that has no regard for output size.\r\n\r\nA few file formats (csv, line json, fwf, and more) natively support reading only part of the rows. This is reflected by either a blocksize (number of bytes) or chunksize (number of rows) parameter in the matching `dd.read_*` functions.\r\n\r\nThese two file formats however do not allow for partial reads, so there's no option but to load them all in memory at once:\r\n- parquet\r\n- json (not line json)\r\n\r\nIn this case, if you call `repartition()` immediately after `read_parquet` / `read_json`, you will typically end up with a much lower memory usage compared to just doing your whole computation on the original partitions.\r\n\r\nWhen input partitions are not all the same size, you have no other option than to use `repartition(partition_size=...)`, which loads the whole input data in memory in order to recalculate the splits. This is wasteful. `repartition` doesn't really have an option, as it doesn't know what's in the graph below it - but `read_parquet` and `read_json` do.\r\n\r\n# Proposed design\r\n- Add the `blocksize` parameter to `read_parquet`. This will simply measure the size on disk of each partition, load the partition with a single task and, if it exceeds blocksize, split it into equal parts.\r\n- Enhance the `blocksize` parameter in `read_json` to work for non-line json files, exactly like in `read_parquet` above.\r\n\r\n# Alternative design (not recommended)\r\nThere's a long-standing discussion that the graph could be annotated with the expected output size of each task, primarily with the intent of empowering the distributed scheduler to do memory-aware choices. This metadata could be reused to run `repartition(partition_size=...)` without having to compute anything.\r\n\r\nThis approach would however be extremely high effort to implement and reach far out of the scope of this specific problem.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9849/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9849/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}