[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/295281901",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-295281901",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 295281901,
        "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTI4MTkwMQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-04-19T14:05:33Z",
        "updated_at": "2017-04-19T14:05:33Z",
        "author_association": "MEMBER",
        "body": "@TomAugspurger if you find yourself with free time you might find this issue interesting.  I also think that it is valuable for distributing algorithms that you care about.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/295281901/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/305468799",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-305468799",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 305468799,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTQ2ODc5OQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-06-01T11:40:23Z",
        "updated_at": "2017-06-01T11:40:23Z",
        "author_association": "MEMBER",
        "body": "@pitrou if you find yourself with some free time you might find this task interesting.  The speedups can be substantial and this has relevance for future paid work.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/305468799/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/316040984",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-316040984",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 316040984,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjA0MDk4NA==",
        "user": {
            "login": "jni",
            "id": 492549,
            "node_id": "MDQ6VXNlcjQ5MjU0OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/492549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jni",
            "html_url": "https://github.com/jni",
            "followers_url": "https://api.github.com/users/jni/followers",
            "following_url": "https://api.github.com/users/jni/following{/other_user}",
            "gists_url": "https://api.github.com/users/jni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jni/subscriptions",
            "organizations_url": "https://api.github.com/users/jni/orgs",
            "repos_url": "https://api.github.com/users/jni/repos",
            "events_url": "https://api.github.com/users/jni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jni/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-07-18T11:53:11Z",
        "updated_at": "2017-07-18T11:53:11Z",
        "author_association": "CONTRIBUTOR",
        "body": "@mrocklin I followed link to this issue from your SciPy talk (which as usual was full of new and exciting things). I just want to point out a related issue: preemptively determining/rechunking the output array. Perhaps with an \"out=\" kwarg? The use case is computing a (1000, 1000) column correlation matrix from a 100,000 row / 1000 column csv file (let's ignore the format for now ;). After setting chunks to (100000, 1), I couldn't find a way to do this without getting a \"number of chunks increasing by factor of 1000\" warning.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/316040984/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/330827068",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-330827068",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 330827068,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDgyNzA2OA==",
        "user": {
            "login": "lsorber",
            "id": 4543654,
            "node_id": "MDQ6VXNlcjQ1NDM2NTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4543654?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lsorber",
            "html_url": "https://github.com/lsorber",
            "followers_url": "https://api.github.com/users/lsorber/followers",
            "following_url": "https://api.github.com/users/lsorber/following{/other_user}",
            "gists_url": "https://api.github.com/users/lsorber/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lsorber/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lsorber/subscriptions",
            "organizations_url": "https://api.github.com/users/lsorber/orgs",
            "repos_url": "https://api.github.com/users/lsorber/repos",
            "events_url": "https://api.github.com/users/lsorber/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lsorber/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-09-20T11:46:02Z",
        "updated_at": "2017-09-20T11:46:02Z",
        "author_association": "NONE",
        "body": "> This does incur some communication costs up front, but it will generally save us more communication down the line.\r\n\r\nAre there any estimates or measurements of the rechunking cost available at runtime?\r\n\r\nAn `einsum` implementation (#732) with rechunking would be even better, as `tensordot` could then call that instead.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/330827068/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/330829473",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-330829473",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 330829473,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDgyOTQ3Mw==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-09-20T11:57:51Z",
        "updated_at": "2017-09-20T11:57:51Z",
        "author_association": "MEMBER",
        "body": "> Are there any estimates or measurements of the rechunking cost available at runtime?\r\n\r\nWe know the size of every chunk both before and after. Every task has around 100-300us of overhead.  \r\n\r\n> An einsum implementation (#732) with rechunking would be even better, as tensordot could then call that instead.\r\n\r\nAre you saying that `einsum` is likely to be faster than `tensordot` or that it is more general?\r\n\r\nIf you have any interest in implementing `einsum` then that would be quite welcome.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/330829473/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/330851431",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-330851431",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 330851431,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDg1MTQzMQ==",
        "user": {
            "login": "lsorber",
            "id": 4543654,
            "node_id": "MDQ6VXNlcjQ1NDM2NTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4543654?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lsorber",
            "html_url": "https://github.com/lsorber",
            "followers_url": "https://api.github.com/users/lsorber/followers",
            "following_url": "https://api.github.com/users/lsorber/following{/other_user}",
            "gists_url": "https://api.github.com/users/lsorber/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lsorber/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lsorber/subscriptions",
            "organizations_url": "https://api.github.com/users/lsorber/orgs",
            "repos_url": "https://api.github.com/users/lsorber/repos",
            "events_url": "https://api.github.com/users/lsorber/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lsorber/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-09-20T13:29:11Z",
        "updated_at": "2017-09-20T13:29:11Z",
        "author_association": "NONE",
        "body": "> We know the size of every chunk both before and after. Every task has around 100-300us of overhead.\r\n\r\nI was mostly referring to the communication cost of rechunking, is there any data available on that?\r\n\r\nI mean that `einsum` is more general yes, since you can specify both inner and outer products (and it would do singleton expansion -- which `tensordot` doesn't seem to do?). Since that is the case, it makes sense to design rechunking on the `einsum` level and have those benefits flow down to `tensordot` automatically.\r\n\r\nI think dask has a lot of potential and would love to contribute in the near future. Feature requests like this one together with access to some of the more recent scalable linear algebra algorithms could really help scale NumPy work to new levels. I can't promise anything yet though, unfortunately.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/330851431/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/330852628",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-330852628",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 330852628,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDg1MjYyOA==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-09-20T13:33:21Z",
        "updated_at": "2017-09-20T13:33:21Z",
        "author_association": "MEMBER",
        "body": "I suspect that the performance benefits of using level-3 BLAS operations like GEMM will encourage folks to keep tensordot around.  I could be wrong though.  This is something that one would have to take up with numpy.\r\n\r\nCommunication costs differ based on scheduler.  We don't generally have this information while constructing the task graph.  In my experience the computation costs of tensordot operations tend to overwhelm the communication costs, particularly when contractions are involved.  This is generally true of level-3 blas operations.  You can be fairly sloppy.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/330852628/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/330853152",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-330853152",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 330853152,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDg1MzE1Mg==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-09-20T13:35:03Z",
        "updated_at": "2017-09-20T13:35:03Z",
        "author_association": "MEMBER",
        "body": "I think it would be great to have someone pushing on dask.array for numerical linear algebra applications.  I care about this topic, but tend not to pursue it personally just because of other work priorities.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/330853152/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/331106658",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-331106658",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 331106658,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTEwNjY1OA==",
        "user": {
            "login": "lsorber",
            "id": 4543654,
            "node_id": "MDQ6VXNlcjQ1NDM2NTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4543654?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lsorber",
            "html_url": "https://github.com/lsorber",
            "followers_url": "https://api.github.com/users/lsorber/followers",
            "following_url": "https://api.github.com/users/lsorber/following{/other_user}",
            "gists_url": "https://api.github.com/users/lsorber/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lsorber/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lsorber/subscriptions",
            "organizations_url": "https://api.github.com/users/lsorber/orgs",
            "repos_url": "https://api.github.com/users/lsorber/repos",
            "events_url": "https://api.github.com/users/lsorber/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lsorber/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-09-21T09:42:12Z",
        "updated_at": "2017-09-21T09:42:12Z",
        "author_association": "NONE",
        "body": "You can write/implement any `einsum` as a batch of independent GEMMs, so it should be able to benefit from the same fast kernels that `tensordot` does. I also agree that `tensordot` is still worth having for those cases where you don't need the full flexibility of an `einsum`. Under the covers it could simply call `einsum` though.\r\n\r\nI think `einsum` is an operation where dask could really shine as a distributed scheduler, because it is an example of a powerful tool where the distribution and scheduling can make a big difference in performance.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/331106658/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/336341387",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-336341387",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 336341387,
        "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjM0MTM4Nw==",
        "user": {
            "login": "jakirkham",
            "id": 3019665,
            "node_id": "MDQ6VXNlcjMwMTk2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jakirkham",
            "html_url": "https://github.com/jakirkham",
            "followers_url": "https://api.github.com/users/jakirkham/followers",
            "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
            "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
            "organizations_url": "https://api.github.com/users/jakirkham/orgs",
            "repos_url": "https://api.github.com/users/jakirkham/repos",
            "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jakirkham/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2017-10-13T03:35:37Z",
        "updated_at": "2017-10-13T03:35:37Z",
        "author_association": "MEMBER",
        "body": "I don't know if you guys saw this, but I think the plan is for NumPy 1.14.0 to use optimized BLAS kernels when possible. ( https://github.com/numpy/numpy/pull/9425 )",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/336341387/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/383660139",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-383660139",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 383660139,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzY2MDEzOQ==",
        "user": {
            "login": "jakirkham",
            "id": 3019665,
            "node_id": "MDQ6VXNlcjMwMTk2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jakirkham",
            "html_url": "https://github.com/jakirkham",
            "followers_url": "https://api.github.com/users/jakirkham/followers",
            "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
            "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
            "organizations_url": "https://api.github.com/users/jakirkham/orgs",
            "repos_url": "https://api.github.com/users/jakirkham/repos",
            "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jakirkham/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-04-23T17:39:17Z",
        "updated_at": "2018-04-23T17:39:17Z",
        "author_association": "MEMBER",
        "body": "It's worth noting that despite work to make NumPy's `einsum` use optimized BLAS calls when possible, one can still get bad performance in common cases (e.g. `matmul`). ( https://github.com/numpy/numpy/issues/7569 )",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/383660139/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/389376263",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-389376263",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 389376263,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTM3NjI2Mw==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-05-16T02:37:11Z",
        "updated_at": "2018-06-14T12:16:24Z",
        "author_association": "MEMBER",
        "body": "The right place to implement the optimization described in the original post is possibly in atop, rather than tensordot.\r\n\r\nEdit: I no longer believe this to be true because we also need to incorporate the sum into this process, which happens outside of atop for the tree reductions.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/389376263/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/397318234",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-397318234",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 397318234,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzMxODIzNA==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-06-14T14:35:11Z",
        "updated_at": "2018-06-14T14:35:11Z",
        "author_association": "MEMBER",
        "body": "I spent this morning working on this problem with pen and paper.  Here are my thoughts after a few hours:\r\n\r\nOur data gets replicated a number of times equal to the product of the number of chunks along each *outer* dimension.  So considering the following example:\r\n\r\n```python\r\nx = da.ones((1000, 1000, 1000), chunks=(100, 200, 100))\r\ny = da.ones((1000, 1000), chunks=(100, 200))\r\nz = tensordot(x, y, axes=(0, 0))  # this requires 5 * 10 communications per chunk\r\n```\r\n\r\nThen naively each chunk communicates with `5 * 10` other chunks.  In a shared memory situation this isn't so bad, but in a distributed memory situation it's pretty unusable.\r\n\r\nThe solution then, is to decrease the number of chunks in outer (non-contracted) dimensions.  The following would perform better\r\n\r\n```python\r\nx = da.ones((1000, 1000, 1000), chunks=(10, 1000, 500))\r\ny = da.ones((1000, 1000), chunks=(10, 1000))\r\nz = tensordot(x, y, axes=(0, 0))  # this requires 1 * 2 communications per chunk\r\n```\r\n\r\nHowever there are some costs to be aware of here:\r\n\r\n1.  By increasing the chunksize of the outer dimensions we also increase the chunksize of our result, which is only outer dimensions.  This might quickly become too large.\r\n\r\n2.  Our input chunks now have very larger outer dimensions (like our output chunks) but also still have full inner dimensions.  We'll need to rechunk these to be much smaller to compensate.\r\n\r\n3.  If we squeeze our inner dimensions down to 1 then we need to worry about what happens when several of these stack together when performing the sum.  Generally we won't want our inner dimensions to decrease below the `split_every` parameter that controls how many intermediate chunks are used in a tree summation (this is available by `dask.config.get('array.split_every')`.\r\n\r\n4.  The process of rechunking also introduces constraints\r\n    1.  There is a single communication cost across the array (but this is additive rather than multiplicative like with tensordot)\r\n    2.  Rechunking also hurts streamability, or low-memory action.  Consider the following example\r\n\r\n        ```python\r\n        x = da.ones((100, 100), chunks=(50, 2)).rechunk((2, 50))\r\n        ```\r\n\r\n        This requires that full (50, 50) subarrays are in memory at once\r\n\r\nSo a naive solution to this problem might be to rechunk all of the outer dimensions to the full dimension size, and rechunk all inner dimensions to be as small as possible in order to compensate, and hope that both result and all intermediate rechunkings fit comfortably in memory.  This will fail in plenty of cases.\r\n\r\nHowever, we do have one benefit to consider:\r\n\r\n1.  Once all-but-one of the outer dimensions are single chunked, it no longer makes sense to single-chunk the last outer dimension\r\n\r\n    Recall that unpleasant rechunking generally requires a full communication of the data.  This made sense when we were competing against the multiplicative cost of multiplying for every chunked dimension, but once every other outer dimension has a single chunk then the multiplicative factor is `1 * 1 * 1 * 1 ... = 1`, and so we can choose to accept this communication and not rechunk.  \r\n\r\nI anticipate that having freedom along one outer dimension of our choice will be very helpful in the common case.\r\n\r\nSo a pragmatic common-case solution here might be to find the outer dimension that we *least* want to single-chunk, and then try hard to single-chunk all other outer dimensions.  The measurement of chunk-desirability depends both on the amount of data in the array associated to that outer-dimension (each outer dimension is associated to exactly one array) and the current chunking of that array.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/397318234/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/397680125",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-397680125",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 397680125,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzY4MDEyNQ==",
        "user": {
            "login": "convexset",
            "id": 1677933,
            "node_id": "MDQ6VXNlcjE2Nzc5MzM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1677933?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/convexset",
            "html_url": "https://github.com/convexset",
            "followers_url": "https://api.github.com/users/convexset/followers",
            "following_url": "https://api.github.com/users/convexset/following{/other_user}",
            "gists_url": "https://api.github.com/users/convexset/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/convexset/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/convexset/subscriptions",
            "organizations_url": "https://api.github.com/users/convexset/orgs",
            "repos_url": "https://api.github.com/users/convexset/repos",
            "events_url": "https://api.github.com/users/convexset/events{/privacy}",
            "received_events_url": "https://api.github.com/users/convexset/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-06-15T16:49:29Z",
        "updated_at": "2018-06-15T16:49:29Z",
        "author_association": "CONTRIBUTOR",
        "body": "What I'm seeing from this discussion is two major objectives:\r\n - chunking to reduce communication (while keeping to a maximum chunk size)\r\n - chunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory)\r\n\r\nThey seem to be relatively compatible.\r\n\r\nMy proposal would be to ask for a max chunk size option (like `10 MiB`, possibly more readily given) and a output chunking (possibly less readily given), and work with those two objectives. Some rechunking might be needed in post processing to achieve both.\r\n\r\nHmmm....\r\n```\r\nx = da.ones((1000, 1000, 1000), chunks=(100, 200, 100))  # this requires 5 communications per chunk\r\ny = da.ones((1000, 1000), chunks=(100, 200))  # this requires 5 * 10 communications per chunk\r\n\r\nz = tensordot(x, y, axes=(0, 0))\r\n# output: chunks=(200, 100, 200)\r\n```\r\nand\r\n```\r\nx = da.ones((1000, 1000, 1000), chunks=(100, 1000, 500))    # this requires 2 communications per chunk\r\ny = da.ones((1000, 1000), chunks=(100, 500))  # this requires 1 * 2 communications per chunk\r\n\r\nz = tensordot(x, y, axes=(0, 0))\r\n# output: chunks=(1000, 500, 500)... ouch\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/397680125/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/397680510",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-397680510",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 397680510,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzY4MDUxMA==",
        "user": {
            "login": "convexset",
            "id": 1677933,
            "node_id": "MDQ6VXNlcjE2Nzc5MzM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1677933?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/convexset",
            "html_url": "https://github.com/convexset",
            "followers_url": "https://api.github.com/users/convexset/followers",
            "following_url": "https://api.github.com/users/convexset/following{/other_user}",
            "gists_url": "https://api.github.com/users/convexset/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/convexset/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/convexset/subscriptions",
            "organizations_url": "https://api.github.com/users/convexset/orgs",
            "repos_url": "https://api.github.com/users/convexset/repos",
            "events_url": "https://api.github.com/users/convexset/events{/privacy}",
            "received_events_url": "https://api.github.com/users/convexset/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-06-15T16:50:57Z",
        "updated_at": "2018-06-15T16:50:57Z",
        "author_association": "CONTRIBUTOR",
        "body": "There may need to be some norms with regards to what \"level\" of communication is sensible.\r\n\r\nWhile theoretically, increasing chunk sizes does not involve as much communication (\"transmissions\") as reducing chunk sizes, practically, when chunk sizes are reduced, workers might just be reorganizing their memory.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/397680510/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/883025068",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-883025068",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 883025068,
        "node_id": "IC_kwDOAbcwm840oeSs",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T03:34:13Z",
        "updated_at": "2021-07-20T03:38:36Z",
        "author_association": "MEMBER",
        "body": "> 3. If we squeeze our inner dimensions down to 1 then we need to worry about what happens when several of these stack together when performing the sum.  Generally we won't want our inner dimensions to decrease below the `split_every` parameter that controls how many intermediate chunks are used in a tree summation (this is available by `dask.config.get('array.split_every')`.\r\n\r\nCan you clarify what you mean by \"squeeze our inner dimensions down to 1\" @mrocklin. Are you talking about squeezing the whole dimension, or about changing the chunking of that dimension?\r\n\r\nI also don't see any kind of key named `split_every` in the dask config dictionary, so I'm not sure we can rely on this. I do have a key named `split-large-chunks`, but it has a value of None.\r\n\r\n```python\r\ndask.config.get('array.slicing.split-large-chunks') == None  # returns True\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/883025068/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/883044310",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-883044310",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 883044310,
        "node_id": "IC_kwDOAbcwm840oi_W",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T04:31:58Z",
        "updated_at": "2021-07-20T04:31:58Z",
        "author_association": "MEMBER",
        "body": "> Can you clarify what you mean by \"squeeze our inner dimensions down to 1\" @mrocklin. Are you talking about squeezing the whole dimension, or about changing the chunking of that dimension?\r\n\r\n\r\nAll of this will probably make more sense after you've been beaten up a bit by tensordot.  I encourage you to run things and see things fail in different ways.  I'm anticipating issues that may or may not arise after we've solved a couple of problems.   Probably you should ignore what I'm about to say until you've crashed your computer a couple dozen times.  That being said, here we go.\r\n\r\n\r\nI'm saying that as we increase the size of the outer dimensions, a desire to keep modest sized chunks will naturally cause us to choose chunks with smaller sizes along inner dimensions.  I suspect that this will have a negative effect when we choose to perform eventual reductions.  Currently when doing a reduction we first collapse each chunk, and then we collect many of those chunks together.  If each chunk only has size one then we don't get much of a size reduction on that first pass, which could become awkward when we collect many of those chunks together.  I suspect that we might run into situations where this causes us to run out of RAM.  \r\n\r\nSo instead after the first pass we might choose to collect these chunks in groups, and then reduce down each group.   Then we could collect the groups together and reduce them again.  This gradual approach may allow us to perform the reduction in small space, even if we don't benefit much from fat inner dimensions.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/883044310/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/883774876",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-883774876",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 883774876,
        "node_id": "IC_kwDOAbcwm840rVWc",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T23:40:43Z",
        "updated_at": "2021-07-20T23:40:43Z",
        "author_association": "MEMBER",
        "body": "The clarification helps, thank you.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/883774876/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887283400",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887283400",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887283400,
        "node_id": "IC_kwDOAbcwm8404t7I",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T07:34:57Z",
        "updated_at": "2021-07-27T07:34:57Z",
        "author_association": "MEMBER",
        "body": "Copying over comments from https://github.com/dask/dask/issues/7124 that really belong more in this issue thread:\r\n\r\n> Terminology:\r\n> We didn't use these particular phrases in my maths classes, but here's what I think they're being used to mean in these discussions\r\n> \r\n> * inner dimensions <- are summed over by tensordot \r\n> * outer dimensions <- not summed over by tensordot\r\n> \r\n> For tensordot, it looks like the process should go something like this: \r\n> 1. Work out which are the outer dimension axes for arrays a and b \r\n> 2. Use the auto_chunks function to work out what the best chunk size is for the tensordot array output, given the chunk size limit from `dask.config.get('array.chunk-size')`\r\n> 3. Work backwards from this to figure out how the input tensordot arrays should be rechunked for the outer dimension axes. \r\n> 4. Inner dimension axes - do we (a) make no change to the chunking here, (b) rechunk all inner dimensions to a chunksize of 1, or (c) rechunk inner dimensions to some smaller chunksize but ?? what size they should be.\r\n> 5. Rechunk the tensordot input arrays\r\n> 6. Set kwarg `align_arrays=False` when tensordot hands over to blockwise (might need to double check if there is any extra logic in the `unify_chunks` function we now need to take care of in tensordot?)\r\n> \r\n> \r\n> I've written some code for this, while I was trying to work out the process. I'll need a bit more clarity around the questions in point 4 and 6 above. ([Hacky code here](https://gist.github.com/GenevieveBuckley/8a2c5068d065ce81284fe606ac72c77d))\r\n> \r\n> It is clear that the rechunking logic is going to have to happen in tensordot before it gets to blockwise, blockwise just doesn't know enough information to do what we want.\r\n\r\nMatt responds:\r\n> Most of my comments below are relevant for #2225 rather than this issue (I think that that issue is a bigger deal than this one). It's worth noting that these are two different concerns and not necessarily fully compatible.\r\n> \r\n> First, I highly recommend multiplying two matrices two-dimensional matrices together and experimenting with chunking structure before trying to generalize any of this. I also recommend using the dask.distributed scheduler and having the dashboard up. The chunking structure will make a very large impact on if the computation finishes or not. I think that you probably need to feel this pain viscerally a bit before these issues make much sense.\r\n> \r\n> I would also be totally ok with a first pass that only worked in the 2d x 2d case. This is the common case, probably much easier to get right, and much less likely to stall out with a half-finished PR.\r\n\r\nand also\r\n> I think that if you build up a small set of interesting examples that you'll be able to try out different approaches, like what you propose above, and see how they work. I think that this will quickly give you more intuition than anyone here has. The approach that you lay out above sounds sensible to me, but I would bet large amounts of money that it's wrong. I don't think that anyone today knows enough to be able to tell if any approach is solid or not. I'm certainly not able to do so.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887283400/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887375130",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887375130",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887375130,
        "node_id": "IC_kwDOAbcwm8405EUa",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T09:53:03Z",
        "updated_at": "2021-07-27T09:53:03Z",
        "author_association": "MEMBER",
        "body": "Ok, I think I have it now.\r\n\r\nThe approach I outlined above was wrong - you shouldn't focus on the shape of the final output array to work out the optimal chunk sizes. Instead, you should focus on trying to maximize the chunk length/width along the axes that will be summed over by tensordot.\r\n\r\nI've updated [my gist](https://gist.github.com/GenevieveBuckley/8a2c5068d065ce81284fe606ac72c77d) with this demo\r\n```python\r\nimport dask\r\nimport dask.array as da\r\nimport numpy as np\r\n\r\n\r\ndef _inner_axes(a_ndim, b_ndim, axes):\r\n    \"\"\"Given tensordot axes argument, return list of axes to sum over.\"\"\"\r\n    if isinstance(axes, (int, float)):\r\n        if axes == 0:\r\n            inner_axes_a = []\r\n            inner_axes_b = []\r\n        elif axes > 0:\r\n            inner_axes_a = list(range(a_ndim))[-axes:]\r\n            inner_axes_b = list(range(b_ndim))[:axes]\r\n    else:\r\n        axes_a, axes_b = axes\r\n        if isinstance(axes_a, (int, float)):\r\n            axes_a = [axes_a]\r\n        if isinstance(axes_b, (int, float)):\r\n            axes_b = [axes_b]\r\n        inner_axes_a = [i for i in range(a.ndim) if i in axes_a]\r\n        inner_axes_b = [i for i in range(b.ndim) if i in axes_b]\r\n    return inner_axes_a, inner_axes_b\r\n  \r\n  \r\ndef find_optimal_chunks(array, inner_axes, limit=None):\r\n    if limit is None:\r\n        limit = dask.utils.parse_bytes(dask.config.get('array.chunk-size'))\r\n\r\n    inner_chunks = [array.shape[ax] for ax in inner_axes]\r\n    while limit < (np.prod(inner_chunks) * array.dtype.itemsize):\r\n        inner_chunks[np.argmax(inner_chunks)] = np.max(inner_chunks) // 2\r\n\r\n    optimal_chunks = []\r\n    for ax in range(array.ndim):\r\n        if ax in inner_axes:\r\n            idx = inner_axes.index(ax)\r\n            optimal_chunks.append(inner_chunks[idx])\r\n        else:\r\n            optimal_chunks.append('auto')\r\n    return optimal_chunks\r\n  \r\nprint(\"Example\")\r\na = da.ones((100, 10_000_000), chunks=(100, 10_000_000))\r\nb = da.ones((10_000_000, 100), chunks=(10_000_000, 100))\r\n# a = da.ones((50, 20_000_000), chunks=(50, 20_000_000))  # optimal chunks (1, 10_000_000)\r\n# b = da.ones((20_000_000, 50), chunks=(20_000_000, 50))  # optimal_chunks (10_000_000, 1)\r\naxes = [1, 0]\r\n\r\ninner_axes_a, inner_axes_b = _inner_axes(a.ndim, b.ndim, axes)\r\noptimal_chunks_a = find_optimal_chunks(a, inner_axes_a)\r\noptimal_chunks_b = find_optimal_chunks(b, inner_axes_b)\r\nprint(\"optimal_chunks_a:\", optimal_chunks_a)\r\nprint(\"optimal_chunks_b:\", optimal_chunks_b)\r\n\r\naa = a.rechunk(optimal_chunks_a)\r\nbb = b.rechunk(optimal_chunks_b)\r\nz = da.tensordot(aa, bb, axes=axes)\r\nz = z.rechunk(['auto' for _ in range(z.ndim)])\r\nz.compute()  # time consuming, but stays nicely within memeory\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887375130/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887389497",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887389497",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887389497,
        "node_id": "IC_kwDOAbcwm8405H05",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T10:14:22Z",
        "updated_at": "2021-07-27T10:14:22Z",
        "author_association": "MEMBER",
        "body": "One problem:\r\nI put that final rechunk in there because for more commonly used parameters for tensordot (eg: `axes=1` with 2D input arrays), we can often end up with a final output array with chunks equal to a single pixel. That's something to avoid, and popping a rechunk statement in here seems to work well. Except...\r\n\r\nBut there's also the other end of the spectrum, where not very many axes have been summed over, so the output array (and also the output chunks) can be very large. For these cases (eg: `axes=0`), rechunking the final output array `z` is extremely time consuming. What's worse is that it's time consuming while building the task graph, rather than only at computation time. \r\n\r\nSo, what to do?\r\n* Rechunk the output only if the chunk sizes are very small\r\n* Rechunk the output only if it is 2D or smaller (maybe? this is a very indirect way to guess rechunking will take a long time)\r\n* not rechunk the final output at all\r\n* ... something else?\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887389497/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887390611",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887390611",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887390611,
        "node_id": "IC_kwDOAbcwm8405IGT",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T10:15:51Z",
        "updated_at": "2021-07-27T10:15:51Z",
        "author_association": "MEMBER",
        "body": "Also, I haven't looked at how this will interact with `unify_chunks` - possibly poorly under some circumstances. Most likely I shouldn't faff around too much more with this, but instead put in a pull request so we can all play around with it a bit.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887390611/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887473882",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887473882",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887473882,
        "node_id": "IC_kwDOAbcwm8405cba",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T12:34:16Z",
        "updated_at": "2021-07-27T12:34:16Z",
        "author_association": "MEMBER",
        "body": "It's entirely possible that there isn't a perfect chunking that makes this\ncomputation easy, and that we'll have to go with some solution that is the\nleast bad.  It's worth remembering that what we do today is not great, and\nthat any improvement is an improvement :)\n\nOn Tue, Jul 27, 2021 at 5:16 AM Genevieve Buckley ***@***.***>\nwrote:\n\n> Also, I haven't looked at how this will interact with unify_chunks -\n> possibly poorly under some circumstances. Most likely I shouldn't faff\n> around too much more with this, but instead put in a pull request so we can\n> all play around with it a bit.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/2225#issuecomment-887390611>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACKZTAUM56R65EFIORZS5LTZ2BOFANCNFSM4DHZ7XMQ>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887473882/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887899882",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887899882",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887899882,
        "node_id": "IC_kwDOAbcwm8407Ebq",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T23:28:58Z",
        "updated_at": "2021-07-27T23:28:58Z",
        "author_association": "MEMBER",
        "body": "Jeremy summarized these two goals earlier:\r\n\r\n1. chunking to reduce communication (while keeping to a maximum chunk size)\r\n2. chunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory)\r\n\r\nSo far I've optimized for (1) but not necessarily (2). \r\n\r\nAddressing (1) means the computation in tensordot works nicely. Addressing (2) might help for convenience of operations *after* tensordot. Output chunks that are too big or too small will cause problems for subsequent operations. We might like to add some extra rules as a nod towards (2), but not too much because that is likely to impact performance.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887899882/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/887902636",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-887902636",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 887902636,
        "node_id": "IC_kwDOAbcwm8407FGs",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-27T23:37:14Z",
        "updated_at": "2021-07-27T23:37:14Z",
        "author_association": "MEMBER",
        "body": "I think that operations should not prematurely optimize for downstream\ncomputations.  Those computations will know what is best for them.  Let's\nlet them decide.\n\nOn Tue, Jul 27, 2021 at 6:29 PM Genevieve Buckley ***@***.***>\nwrote:\n\n> Jeremy summarized these two goals earlier:\n>\n>    1. chunking to reduce communication (while keeping to a maximum chunk\n>    size)\n>    2. chunk to meet a specified output chunking outcome (while\n>    considering maximum chunk size; so 1 argument on output chunks, another on\n>    max memory)\n>\n> So far I've optimized for (1) but not necessarily (2).\n>\n> Addressing (1) means the computation in tensordot works nicely. Addressing\n> (2) might help for convenience of operations *after* tensordot. Output\n> chunks that are too big or too small will cause problems for subsequent\n> operations. We might like to add some extra rules as a nod towards (2), but\n> not too much because that is likely to impact performance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/2225#issuecomment-887899882>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACKZTBAPB3VBDR2SENJGEDTZ46MJANCNFSM4DHZ7XMQ>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/887902636/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/898260705",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-898260705",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 898260705,
        "node_id": "IC_kwDOAbcwm841il7h",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-13T07:48:46Z",
        "updated_at": "2021-08-13T07:48:46Z",
        "author_association": "MEMBER",
        "body": "Here is a sketch of the continuum of 2D array shapes we can have for a tensordot operation:\r\nhttps://docs.google.com/drawings/d/1qdq60Exvz1GvVIGd54Bo74n99j1k8-QDVxYEZd5uH4A/edit?usp=sharing\r\n\r\nThe three important stages are where:\r\n1.  The output array is smaller than one or both input arrays (the easiest to deal with)\r\n2. The output array size is equal to the largest input array\r\n3. The output array size is larger than both input arrays (the hardest to deal with)\r\n\r\nFrom what I can tell, it's not a case of needing different rules for different situations, it's just that there are no perfect solutions for the harder cases. For (3) we can limit the chunksize of the output array, at the cost of data duplication across the cluster. That's not good, but there isn't another good solution available - the only other alternative will crash the computation as the workers run out of memory.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/898260705/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/898263648",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-898263648",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 898263648,
        "node_id": "IC_kwDOAbcwm841impg",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-13T07:55:02Z",
        "updated_at": "2021-08-13T07:55:02Z",
        "author_association": "MEMBER",
        "body": "There are a couple of general rules that apply everywhere:\r\n* Chunk boundaries should be aligned along the inner axes of the arrays\r\n* We should prioritize square shaped chunks over long and skinny chunks. It's a compromise between low worker memory and reducing data duplication. https://docs.google.com/drawings/d/1TqPAJtWy2PTjUJCFf_TtOVvbkTt37MtiZa52xLG4bzw/edit?usp=sharing\r\n\r\nI think this means that for any tensordot operation `A.B = C`, we find which array shape is largest, A, B, or C. Then we use `normalize_chunks` to find the biggest, squarest chunks for that array. Then, we fill in what the chunk shapes should be for the other arrays (which should be easy enough because all the chunk boundaries need to line up with one another).",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/898263648/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/898406782",
        "html_url": "https://github.com/dask/dask/issues/2225#issuecomment-898406782",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/2225",
        "id": 898406782,
        "node_id": "IC_kwDOAbcwm841jJl-",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-13T11:57:28Z",
        "updated_at": "2021-08-13T11:57:28Z",
        "author_association": "MEMBER",
        "body": "Thank you for sharing these diagrams @GenevieveBuckley .  \r\n\r\n@tomwhite , do these match your operational experience?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/898406782/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]