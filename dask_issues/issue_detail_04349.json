{
    "url": "https://api.github.com/repos/dask/dask/issues/4349",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/4349/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/4349/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/4349/events",
    "html_url": "https://github.com/dask/dask/issues/4349",
    "id": 395414160,
    "node_id": "MDU6SXNzdWUzOTU0MTQxNjA=",
    "number": 4349,
    "title": "TypeError with `store` and distributed scheduler",
    "user": {
        "login": "rainwoodman",
        "id": 138060,
        "node_id": "MDQ6VXNlcjEzODA2MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/138060?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rainwoodman",
        "html_url": "https://github.com/rainwoodman",
        "followers_url": "https://api.github.com/users/rainwoodman/followers",
        "following_url": "https://api.github.com/users/rainwoodman/following{/other_user}",
        "gists_url": "https://api.github.com/users/rainwoodman/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rainwoodman/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rainwoodman/subscriptions",
        "organizations_url": "https://api.github.com/users/rainwoodman/orgs",
        "repos_url": "https://api.github.com/users/rainwoodman/repos",
        "events_url": "https://api.github.com/users/rainwoodman/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rainwoodman/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2019-01-02T23:28:26Z",
    "updated_at": "2021-10-12T07:37:55Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "Here is a stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/protocol/pickle.py\", line 38, in dumps\r\n    result = pickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pdb.py\", line 1667, in main\r\n    pdb._runscript(mainpyfile)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pdb.py\", line 1548, in _runscript\r\n    self.run(statement)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/bdb.py\", line 434, in run\r\n    exec(cmd, globals, locals)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/global/project/projectdirs/m3127/lightcones/hod/SAM_GP18.py\", line 114, in <module>\r\n    main()\r\n  File \"/global/project/projectdirs/m3127/lightcones/hod/SAM_GP18.py\", line 108, in main\r\n    [column], dataset='ELG-SAM_GP18', header=None)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/nbodykit/base/catalog.py\", line 640, in save\r\n    array.store(_ColumnWrapper(bb), regions=(slice(offset, offset + len(array)),))\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/array/core.py\", line 1131, in store\r\n    r = store([self], [target], **kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/array/core.py\", line 866, in store\r\n    result.compute(**kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/base.py\", line 156, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/base.py\", line 397, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/client.py\", line 2306, in get\r\n    actors=actors,\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/client.py\", line 2249, in _graph_to_futures\r\n    'tasks': valmap(dumps_task, dsk3),\r\n  File \"cytoolz/dicttoolz.pyx\", line 165, in cytoolz.dicttoolz.valmap\r\n  File \"cytoolz/dicttoolz.pyx\", line 190, in cytoolz.dicttoolz.valmap\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/worker.py\", line 2765, in dumps_task\r\n    'args': warn_dumps(task[1:])}\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/worker.py\", line 2774, in warn_dumps\r\n    b = dumps(obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/protocol/pickle.py\", line 51, in dumps\r\n    return cloudpickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/cloudpickle/cloudpickle.py\", line 931, in dumps\r\n    cp.dump(obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/cloudpickle/cloudpickle.py\", line 284, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 496, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n```\r\n\r\nThe full line that triggered this error was:\r\n\r\n```\r\n              with ff.create(dataset, dtype, size, Nfile) as bb:\r\n\r\n                    # ensure only the first dimension is chunked\r\n                    # because bigfile only support writing with slices in first dimension.\r\n                    rechunk = dict([(ind, -1) for ind in range(1, array.ndim)])\r\n                    array = array.rechunk(rechunk)\r\n\r\n                    # lock=False to avoid dask from pickling the lock with the object.\r\n                    array.store(_ColumnWrapper(bb), regions=(slice(offset, offset + len(array)),))\r\n```\r\n\r\nIf I add `lock=False` to the argument there is no longer such error. Some digging in the stack frames I realized with the default value of lock=True, a `_thread.lock` object is created for the target object to ensure no two workers attempt to write at the same time.\r\n\r\nThis form of locking is dubious in a cluster environment. Even if implemented correctly (e.g. a lock via the scheduler), the very wide lock also means the entire IO operation will be serialized, and severely limit the IO throughput. These caveats are not stated in the documentation of dask.Array.store.\r\n\r\nFurthermore, when locking is disabled, it is difficult to find a storage backend that correctly implements non-overlapping concurrent write operations of arbitrary striding. Even if one exists, it can hardly be efficient. It is much easier to find some that works properly for a contiguous striding (as simple as POSIX files). It may be beneficial to have a way to inform some guarantee on the kind of strides the `store` function triggers on the target object. In the example above, I used rechunk to rechunk the array; but currently there is no guarantee store will not decide to write the array in sequence of `[:, 0]`, `[:, 1]`, ...\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/4349/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/4349/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}