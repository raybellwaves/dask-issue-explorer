{
    "url": "https://api.github.com/repos/dask/dask/issues/2991",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/2991/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/2991/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/2991/events",
    "html_url": "https://github.com/dask/dask/issues/2991",
    "id": 281425851,
    "node_id": "MDU6SXNzdWUyODE0MjU4NTE=",
    "number": 2991,
    "title": "read_parquet() does not capture divisions when partitioned",
    "user": {
        "login": "gorlins",
        "id": 139286,
        "node_id": "MDQ6VXNlcjEzOTI4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/139286?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gorlins",
        "html_url": "https://github.com/gorlins",
        "followers_url": "https://api.github.com/users/gorlins/followers",
        "following_url": "https://api.github.com/users/gorlins/following{/other_user}",
        "gists_url": "https://api.github.com/users/gorlins/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gorlins/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gorlins/subscriptions",
        "organizations_url": "https://api.github.com/users/gorlins/orgs",
        "repos_url": "https://api.github.com/users/gorlins/repos",
        "events_url": "https://api.github.com/users/gorlins/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gorlins/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 11,
    "created_at": "2017-12-12T15:29:26Z",
    "updated_at": "2021-10-12T04:46:12Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "Reading a dataframe from parquet fails to load the partitions in order or preserve index divisions, while it works if reading the same dataframe from a globstring\r\n\r\nExpected behavior: partitions in the dask frame correspond to ordered partitions of the parquet file, and divisions on the index should be preserved if partitioning and order of partitions does not scramble the index (ie if the partition has the same ordering as the index, is a prefix, etc)\r\n\r\n```python\r\nfrom dask import dataframe as dask_df\r\nimport pandas as pd\r\nfrom datetime import date, timedelta, datetime\r\nimport dask\r\nprint(dask.__version__) # 0.16.0\r\nimport fastparquet\r\nprint(fastparquet.__version__) . # 0.1.3\r\n\r\ndef make_dt(hour):\r\n    return datetime(2017, 1, 1, 0, 0, 0) + timedelta(hours=hour)\r\n\r\ndef make_part(hours):\r\n    return pd.DataFrame([\r\n        dict(\r\n            hour=h,\r\n            dt=make_dt(h),\r\n            date=make_dt(h).date().strftime('%Y-%m-%d')\r\n        )\r\n        for h in hours\r\n    ]\r\n    ).set_index('hour')\r\n\r\nddf = dask_df.from_pandas(make_part(range(480,520)),npartitions=1)\r\n\r\nddf.to_parquet('partition_play', write_index=True, partition_on=['date'])\r\n\r\nback = dask_df.read_parquet('partition_play', index='hour') # hour is actually known as default index\r\n\r\n# back will .compute() correctly, but does not know its divisions even though the parquet file has them in the min/max part of the header\r\n# I think this is because the partitions are not stored in order in the _metadata file\r\nback.known_divisions # False\r\n\r\n# The glob string will read the partitions in order and preserves the index\r\nback_glob = = dask_df.read_parquet('partition_play/date=*/*.parquet')\r\nback_glob.known_divisions # True\r\n\r\n```\r\n\r\nIf the real issue is just row groups in _metadata not being sorted, I believe this could be addressed by just reordering the partitions in read_parquet before checking for an index, but I'm not too familiar w/ how parquet should work in this case",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/2991/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/2991/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}