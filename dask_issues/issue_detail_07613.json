{
    "url": "https://api.github.com/repos/dask/dask/issues/7613",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/7613/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/7613/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/7613/events",
    "html_url": "https://github.com/dask/dask/issues/7613",
    "id": 869198845,
    "node_id": "MDU6SXNzdWU4NjkxOTg4NDU=",
    "number": 7613,
    "title": "Dask crashes or hangs during out-of-core dataframes sort",
    "user": {
        "login": "stephanie-wang",
        "id": 2560516,
        "node_id": "MDQ6VXNlcjI1NjA1MTY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2560516?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/stephanie-wang",
        "html_url": "https://github.com/stephanie-wang",
        "followers_url": "https://api.github.com/users/stephanie-wang/followers",
        "following_url": "https://api.github.com/users/stephanie-wang/following{/other_user}",
        "gists_url": "https://api.github.com/users/stephanie-wang/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/stephanie-wang/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/stephanie-wang/subscriptions",
        "organizations_url": "https://api.github.com/users/stephanie-wang/orgs",
        "repos_url": "https://api.github.com/users/stephanie-wang/repos",
        "events_url": "https://api.github.com/users/stephanie-wang/events{/privacy}",
        "received_events_url": "https://api.github.com/users/stephanie-wang/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 19,
    "created_at": "2021-04-27T19:37:16Z",
    "updated_at": "2021-10-26T20:10:44Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\nThis is essentially the same issue as [this one](https://github.com/dask/community/issues/151) on dask/community, but I thought it would be worth a try to see if anyone can help here. Please let me know if I should close one of these!\r\n\r\n**What happened**: An out-of-core sort of a large dataframe hangs or crashes.\r\n\r\n**What you expected to happen**: The sort should finish.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nI set up a single-node dask distributed cluster on a machine with 32 vCPUs and 128GB RAM. I started a dask worker with this command (note that I used nprocs=8 because this seemed to help reduce some of the errors compared to nprocs=32):\r\n```bash\r\ndask-worker localhost:8786 --nthreads 1 --nprocs=8 --memory-limit=10000000000  # 10GB.\r\n```\r\nThen, I ran this [script](https://gist.github.com/stephanie-wang/f4d061ca237837f79069807cf88e6c14#file-test_sort-py), which is meant to benchmark a full sort of a large dataframe. The script generates partitions on-disk, then shuffles and blocks on the result using this line:\r\n```python\r\nprint(df.set_index('a').head(10, npartitions=-1))  # Force `head` to wait on all partitions.\r\n```\r\n\r\nOn 10GB with 100 (100MB) partitions, I started to get a lot of errors with nprocs=32 ([stdout here](https://gist.github.com/stephanie-wang/f4d061ca237837f79069807cf88e6c14#file-stdout-128gb-ram-10gb-dataset-100mb-partitions-memory-limit-auto)). Then I tried nprocs=8 and the script finished successfully ([results here](https://gist.github.com/stephanie-wang/f4d061ca237837f79069807cf88e6c14#file-output-csv)).\r\n\r\nHowever, I haven't been able to get the script to work with 100GB yet. Here is an example of the [output](https://gist.github.com/stephanie-wang/f4d061ca237837f79069807cf88e6c14#file-stdout-128gb-ram-100gb-dataset-1gb-partitions-memory-limit-10gb-nprocs-8). So far, in addition to changing nprocs and nthreads, I've also tried partitions=100 and 1000 and lowering the `target` and `memory` config parameters described [here](https://distributed.dask.org/en/latest/worker.html#memory-management).\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.4.1\r\n- Python version: 3.7.7\r\n- Operating System: Ubuntu 18.04\r\n- Install method (conda, pip, source): conda\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/7613/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/7613/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}