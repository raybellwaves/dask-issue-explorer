{
    "url": "https://api.github.com/repos/dask/dask/issues/10480",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10480/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10480/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10480/events",
    "html_url": "https://github.com/dask/dask/issues/10480",
    "id": 1872501965,
    "node_id": "I_kwDOAbcwm85vnBzN",
    "number": 10480,
    "title": "Processes scheduler does not work partition wise, leading to OOM",
    "user": {
        "login": "RobbeSneyders",
        "id": 20990866,
        "node_id": "MDQ6VXNlcjIwOTkwODY2",
        "avatar_url": "https://avatars.githubusercontent.com/u/20990866?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/RobbeSneyders",
        "html_url": "https://github.com/RobbeSneyders",
        "followers_url": "https://api.github.com/users/RobbeSneyders/followers",
        "following_url": "https://api.github.com/users/RobbeSneyders/following{/other_user}",
        "gists_url": "https://api.github.com/users/RobbeSneyders/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/RobbeSneyders/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/RobbeSneyders/subscriptions",
        "organizations_url": "https://api.github.com/users/RobbeSneyders/orgs",
        "repos_url": "https://api.github.com/users/RobbeSneyders/repos",
        "events_url": "https://api.github.com/users/RobbeSneyders/events{/privacy}",
        "received_events_url": "https://api.github.com/users/RobbeSneyders/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3880424463,
            "node_id": "LA_kwDOAbcwm87nSpQP",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20triage",
            "name": "needs triage",
            "color": "eeeeee",
            "default": false,
            "description": "Needs a response from a contributor"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2023-08-29T21:26:46Z",
    "updated_at": "2024-01-08T01:47:01Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\nFor some graphs, the processes scheduler does not completely process a partition before starting on a new one, which leads to Out Of Memory issues when processing larger than memory data.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask\r\nimport dask.dataframe as dd\r\n\r\ndataframe = dd.from_dict({\"id\": [0, 1], \"url\": [\"a\", \"b\"]}, npartitions=2)\r\n\r\n\r\ndef process_partition(df):\r\n    counter = 0\r\n    while counter < 100000000:\r\n        counter += 1\r\n    return df\r\n\r\n\r\ndataframe = dataframe.map_partitions(process_partition)\r\n\r\nindex_df = dataframe[[\"id\"]]\r\nurl_df = dataframe[[\"url\"]]\r\nwrite_tasks = [\r\n    dd.to_parquet(index_df, \"/tmp/id\", compute=False),\r\n    dd.to_parquet(url_df, \"/tmp/url\", compute=False)\r\n]\r\n\r\nif __name__ == \"__main__\":\r\n    dask.visualize(*write_tasks, filename=\"graph.png\")\r\n    dd.compute(*write_tasks, scheduler=\"processes\", num_workers=1)\r\n```\r\n\r\nThis example creates a dataset with 2 partitions, and processes it in a way I believe should be embarrassingly parallel. The `map_partitions` is meant to introduce a delay simulating processing. The execution is limited to 1 worker so both partitions need to be processed sequentially on the same worker.\r\n\r\nAfter running the example above, check the creation time of the produced files using:\r\n\r\n```bash\r\nls --full-time /tmp/url\r\n```\r\n\r\nWhen using `scheduler=\"threads\"`, there will be a couple of seconds between both files. When using `scheduler=\"processes\"`, both files are created at the same time, which is at the end of the execution.\r\n\r\nTask graph:\r\n<img src=\"https://github.com/dask/dask/assets/20990866/ca82d25b-1222-47d2-8ac8-6da534c65022\" width=\"200\">\r\n\r\n**Anything else we need to know?**:\r\n\r\nPossibly related: #10080 \r\n\r\n**Environment**:\r\n\r\n- Dask version:\r\n2023.8.1\r\n- Python version:\r\n3.10\r\n- Operating System:\r\nUbuntu 22.04.2\r\n- Install method (conda, pip, source):\r\npip\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10480/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10480/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}