[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1962591486",
        "html_url": "https://github.com/dask/dask/issues/10949#issuecomment-1962591486",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10949",
        "id": 1962591486,
        "node_id": "IC_kwDOAbcwm850-sT-",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-24T19:19:05Z",
        "updated_at": "2024-02-24T19:19:05Z",
        "author_association": "MEMBER",
        "body": "Hi,\r\n\r\nthanks for your report. This doesn't work because the divisions of the DataFrame are unknown after read_parquet. That means we can't efficiently repartition by frequency without scanning the whole Index. I am not against making this work more reliably though in the future.\r\n\r\nI have a question though: This doesn't work either for the current dask.dataframe implementation for me if you disable query planning. Does this work for you?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1962591486/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1962742916",
        "html_url": "https://github.com/dask/dask/issues/10949#issuecomment-1962742916",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10949",
        "id": 1962742916,
        "node_id": "IC_kwDOAbcwm850_RSE",
        "user": {
            "login": "pvaezi",
            "id": 2073051,
            "node_id": "MDQ6VXNlcjIwNzMwNTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2073051?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pvaezi",
            "html_url": "https://github.com/pvaezi",
            "followers_url": "https://api.github.com/users/pvaezi/followers",
            "following_url": "https://api.github.com/users/pvaezi/following{/other_user}",
            "gists_url": "https://api.github.com/users/pvaezi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pvaezi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pvaezi/subscriptions",
            "organizations_url": "https://api.github.com/users/pvaezi/orgs",
            "repos_url": "https://api.github.com/users/pvaezi/repos",
            "events_url": "https://api.github.com/users/pvaezi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pvaezi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-24T22:06:24Z",
        "updated_at": "2024-02-24T22:06:24Z",
        "author_association": "NONE",
        "body": "> Hi,\r\n> \r\n> thanks for your report. This doesn't work because the divisions of the DataFrame are unknown after read_parquet. That means we can't efficiently repartition by frequency without scanning the whole Index. I am not against making this work more reliably though in the future.\r\n> \r\n> I have a question though: This doesn't work either for the current dask.dataframe implementation for me if you disable query planning. Does this work for you?\r\n\r\nThanks for the response, I tried without query planning, still the same issue.\r\n\r\nIt does make sense that index needs to be scanned fully to make repartitioning effective. Can you suggest workarounds? Context, I'm trying to resample dataframe by day and aggregate, if partitions are divided by day, it makes my desired resampling and aggregations much easier and less memory intensive.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1962742916/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1962751030",
        "html_url": "https://github.com/dask/dask/issues/10949#issuecomment-1962751030",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10949",
        "id": 1962751030,
        "node_id": "IC_kwDOAbcwm850_TQ2",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-24T22:55:27Z",
        "updated_at": "2024-02-24T22:55:27Z",
        "author_association": "MEMBER",
        "body": "The most effective way depends a little on where you are reading the data from. You can set ``calculate_divisions=True`` in the ``read_parquet`` call, this will populate the divisions and enable your repartitioning.\r\n\r\nThe scan can be expensive though, this depends a little how many files are in your dataset and where your data is stored (e.g. local or remote like s3). That will get you there though.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1962751030/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1970237478",
        "html_url": "https://github.com/dask/dask/issues/10949#issuecomment-1970237478",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10949",
        "id": 1970237478,
        "node_id": "IC_kwDOAbcwm851b3Am",
        "user": {
            "login": "pvaezi",
            "id": 2073051,
            "node_id": "MDQ6VXNlcjIwNzMwNTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2073051?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pvaezi",
            "html_url": "https://github.com/pvaezi",
            "followers_url": "https://api.github.com/users/pvaezi/followers",
            "following_url": "https://api.github.com/users/pvaezi/following{/other_user}",
            "gists_url": "https://api.github.com/users/pvaezi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pvaezi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pvaezi/subscriptions",
            "organizations_url": "https://api.github.com/users/pvaezi/orgs",
            "repos_url": "https://api.github.com/users/pvaezi/repos",
            "events_url": "https://api.github.com/users/pvaezi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pvaezi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-29T01:38:19Z",
        "updated_at": "2024-02-29T01:38:29Z",
        "author_association": "NONE",
        "body": "Thanks, in general I'm looking to replicate a sql query like below with dask, I had trouble with memory consumption of dask workers, if you can guide me how to properly use resampling with timestamp column, that would be great:\r\n\r\n```\r\n   SELECT\r\n        TIME_BUCKET(INTERVAL 1 DAY, timestamp) AS timestamp,\r\n        col1,\r\n        col2,\r\n        col3,\r\n        sum(col4),\r\n        avg(col5)\r\n    FROM df\r\n    GROUP BY TIME_BUCKET(INTERVAL 1 DAY, timestamp), col1, col2, col3;\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1970237478/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/2037141748",
        "html_url": "https://github.com/dask/dask/issues/10949#issuecomment-2037141748",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10949",
        "id": 2037141748,
        "node_id": "IC_kwDOAbcwm855bFD0",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-04-04T12:56:12Z",
        "updated_at": "2024-04-04T12:56:12Z",
        "author_association": "MEMBER",
        "body": "I think you have to calculate the divisions at some point if you want to resample by day, there isn't really a way around this since we need the information.\r\n\r\nNot sure if you have to resample though, you could use the dt accessor on your timestamp column and round this to day accuracy. Then you can do a regular groupy instead of resampling, which doesn't need to know anything about the divisions.\r\n\r\nIs that helpful?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/2037141748/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]