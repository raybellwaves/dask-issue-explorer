[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/515547894",
        "html_url": "https://github.com/dask/dask/issues/5159#issuecomment-515547894",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5159",
        "id": 515547894,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUxNTU0Nzg5NA==",
        "user": {
            "login": "jcrist",
            "id": 2783717,
            "node_id": "MDQ6VXNlcjI3ODM3MTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2783717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jcrist",
            "html_url": "https://github.com/jcrist",
            "followers_url": "https://api.github.com/users/jcrist/followers",
            "following_url": "https://api.github.com/users/jcrist/following{/other_user}",
            "gists_url": "https://api.github.com/users/jcrist/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jcrist/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jcrist/subscriptions",
            "organizations_url": "https://api.github.com/users/jcrist/orgs",
            "repos_url": "https://api.github.com/users/jcrist/repos",
            "events_url": "https://api.github.com/users/jcrist/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jcrist/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-07-26T18:06:12Z",
        "updated_at": "2019-07-26T18:06:12Z",
        "author_association": "MEMBER",
        "body": "Hello, thanks for filing an issue. For tasks like this it's hard for us to help without a [minimal reproducible example](https://blog.dask.org/2018/02/28/minimal-bug-reports).\r\n\r\nHigh memory usage for tasks like this could be due to several things, but the dataframe partitioning schemes are likely part of it. For parquet what's important is the size of each partition (\"row group\" in parquet parlance), not the size of each file (each file may have one or more partitions). You may try experimenting with manually repartitioning (`.repartition`) after reading if the size of each partition is too large.\r\n\r\nAnother thought would be to switch to using the distributed scheduler (which works fine locally) - this provides better debugging and profiling options with the dashboard (https://distributed.dask.org/en/latest/web.html), and may provide more insights.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/515547894/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/515646970",
        "html_url": "https://github.com/dask/dask/issues/5159#issuecomment-515646970",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5159",
        "id": 515646970,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUxNTY0Njk3MA==",
        "user": {
            "login": "matthewgson",
            "id": 46496637,
            "node_id": "MDQ6VXNlcjQ2NDk2NjM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/46496637?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/matthewgson",
            "html_url": "https://github.com/matthewgson",
            "followers_url": "https://api.github.com/users/matthewgson/followers",
            "following_url": "https://api.github.com/users/matthewgson/following{/other_user}",
            "gists_url": "https://api.github.com/users/matthewgson/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/matthewgson/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/matthewgson/subscriptions",
            "organizations_url": "https://api.github.com/users/matthewgson/orgs",
            "repos_url": "https://api.github.com/users/matthewgson/repos",
            "events_url": "https://api.github.com/users/matthewgson/events{/privacy}",
            "received_events_url": "https://api.github.com/users/matthewgson/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-07-27T03:11:17Z",
        "updated_at": "2019-07-27T03:11:17Z",
        "author_association": "NONE",
        "body": "Please excuse my lack of detailed work in this posting. With my limited experience with python and dask, I wan't sure how to mimic this problem. \r\n\r\nI assumed that the each size of parquet file corresponds to each size of partitions, for I have used dask.to_parquet function to save merged parquet file. (As far as I know, dask.to parquet saves each partitions to each parquet file, but correct me if I am wrong.)\r\n\r\nSince I did not have trouble saving merged1 parquet file, I'll try to reparition them by size(so that it is evenly divided) and try above code again. I'll post the result when it's done.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/515646970/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/516107976",
        "html_url": "https://github.com/dask/dask/issues/5159#issuecomment-516107976",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5159",
        "id": 516107976,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUxNjEwNzk3Ng==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-07-29T18:26:18Z",
        "updated_at": "2019-07-29T18:26:18Z",
        "author_association": "MEMBER",
        "body": "Any updates here @Matthew-gunsuSon?\r\n\r\n> I assumed that the each size of parquet file corresponds to each size of partitions, for I have used dask.to_parquet function to save merged parquet file. (As far as I know, dask.to parquet saves each partitions to each parquet file, but correct me if I am wrong.)\r\n\r\nYes, usually. Specifying `partition_on` may change that a bit.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/516107976/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/516121120",
        "html_url": "https://github.com/dask/dask/issues/5159#issuecomment-516121120",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5159",
        "id": 516121120,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUxNjEyMTEyMA==",
        "user": {
            "login": "matthewgson",
            "id": 46496637,
            "node_id": "MDQ6VXNlcjQ2NDk2NjM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/46496637?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/matthewgson",
            "html_url": "https://github.com/matthewgson",
            "followers_url": "https://api.github.com/users/matthewgson/followers",
            "following_url": "https://api.github.com/users/matthewgson/following{/other_user}",
            "gists_url": "https://api.github.com/users/matthewgson/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/matthewgson/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/matthewgson/subscriptions",
            "organizations_url": "https://api.github.com/users/matthewgson/orgs",
            "repos_url": "https://api.github.com/users/matthewgson/repos",
            "events_url": "https://api.github.com/users/matthewgson/events{/privacy}",
            "received_events_url": "https://api.github.com/users/matthewgson/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-07-29T19:03:26Z",
        "updated_at": "2019-07-31T05:09:03Z",
        "author_association": "NONE",
        "body": "Yeah, seems like uneven partition caused the problem. With even sized partitions it worked.\r\n\r\nUpdate : Dask's uneven repartitioning tends to raise memory error problem. I feel limited when I merge two big data. When I try to repartition a merged file, I get a Memory error, and I wasn't able to save into parquet file. \r\n \r\n<a href=\"https://stackoverflow.com/questions/52642966/repartition-dask-dataframe-to-get-even-partitions\"> Others also report this problem. </a> IMHO it would be convenient if Dask can handle repartitions nicely.  ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/516121120/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]