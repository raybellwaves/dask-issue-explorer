{
    "url": "https://api.github.com/repos/dask/dask/issues/6762",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/6762/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/6762/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/6762/events",
    "html_url": "https://github.com/dask/dask/issues/6762",
    "id": 727074147,
    "node_id": "MDU6SXNzdWU3MjcwNzQxNDc=",
    "number": 6762,
    "title": "memory leak using Dask DataFrame",
    "user": {
        "login": "jameslamb",
        "id": 7608904,
        "node_id": "MDQ6VXNlcjc2MDg5MDQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7608904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jameslamb",
        "html_url": "https://github.com/jameslamb",
        "followers_url": "https://api.github.com/users/jameslamb/followers",
        "following_url": "https://api.github.com/users/jameslamb/following{/other_user}",
        "gists_url": "https://api.github.com/users/jameslamb/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jameslamb/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jameslamb/subscriptions",
        "organizations_url": "https://api.github.com/users/jameslamb/orgs",
        "repos_url": "https://api.github.com/users/jameslamb/repos",
        "events_url": "https://api.github.com/users/jameslamb/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jameslamb/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2020-10-22T05:59:34Z",
    "updated_at": "2021-10-13T07:06:11Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "\ud83d\udc4b hello from Chicago!\r\n\r\n### What happened:\r\n\r\nRunning a very small workflow that created a Dask DataFrame with `dd.read_csv()`, then `persist()`ed it, then called `.compute()` on it, a substantial amount of memory was not freed in the cluster.\r\n\r\nThis memory was not freed even after removing the client that ran the workflow and all references to the Dask DataFrame (by restarting the Jupyter notebook kernel in the notebook client code runs from).\r\n\r\nFor a workflow that had peak memory usage around 10GB, I observed increasing memory usage across runs. The values below are memory usage in the cluster after restarting the kernel in the client notebook.\r\n\r\n* cluster startup: 101.7  MB\r\n* after run 1:  2.45 GB\r\n* after run 2: 2.87 GB\r\n* after run 3: 3.05  GB\r\n\r\n### What you expected to happen:\r\n\r\nBased on my previous experience and the documentation in https://distributed.dask.org/en/latest/memory.html#managing-memory, I expected that any memory allocated for some tasks would be freed after those tasks completed and all client references to those tasks' results were removed.\r\n\r\n### Minimal Complete Verifiable Example:\r\n\r\nI found that the easiest way I could reproduce this was with [`dask_cloudprovider.FargateCluster`](https://cloudprovider.dask.org/en/latest/#fargate-ecs), but I did observe the same behavior on a [`dask_kubernetes.KubeCluster`](https://kubernetes.dask.org/en/latest/kubecluster.html).\r\n\r\n1. create a `conda` environment and Jupyter kernel to work in\r\n<details><summary>env.yml (click me)</summary>\r\n\r\n```yaml\r\nname: daskdev\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - argon2-cffi=20.1.0\r\n  - async_generator=1.10\r\n  - attrs=20.2.0\r\n  - backcall=0.2.0\r\n  - bleach=3.2.1\r\n  - ca-certificates\r\n  - certifi=2020.6.20\r\n  - cffi=1.14.3\r\n  - decorator=4.4.2\r\n  - defusedxml=0.6.0\r\n  - entrypoints=0.3\r\n  - ipykernel=5.3.4\r\n  - ipython=7.18.1\r\n  - ipython_genutils=0.2.0\r\n  - jedi=0.17.2\r\n  - jinja2=2.11.2\r\n  - jsonschema=3.2.0\r\n  - jupyter_client=6.1.7\r\n  - jupyter_core=4.6.3\r\n  - jupyterlab_pygments=0.1.2\r\n  - libedit=3.1.20191231\r\n  - libffi=3.3\r\n  - libsodium=1.0.18\r\n  - markupsafe=1.1.1\r\n  - mistune=0.8.4\r\n  - nb_conda_kernels\r\n  - nbclient=0.5.1\r\n  - nbconvert=6.0.7\r\n  - nbformat=5.0.8\r\n  - ncurses=6.2\r\n  - nest-asyncio=1.4.1\r\n  - notebook=6.1.4\r\n  - openssl=1.1.1h\r\n  - packaging=20.4\r\n  - pandoc=2.11\r\n  - pandocfilters=1.4.2\r\n  - parso=0.7.0\r\n  - pexpect=4.8.0\r\n  - pickleshare=0.7.5\r\n  - pip=20.2.4\r\n  - prometheus_client=0.8.0\r\n  - prompt-toolkit=3.0.8\r\n  - ptyprocess=0.6.0\r\n  - pycparser=2.20\r\n  - pygments=2.7.1\r\n  - pyparsing=2.4.7\r\n  - pyrsistent=0.17.3\r\n  - python=3.8.5\r\n  - python-dateutil=2.8.1\r\n  - pyzmq=19.0.2\r\n  - readline=8.0\r\n  - send2trash=1.5.0\r\n  - setuptools=50.3.0\r\n  - six=1.15.0\r\n  - sqlite=3.33.0\r\n  - terminado=0.9.1\r\n  - testpath=0.4.4\r\n  - tk=8.6.10\r\n  - tornado=6.0.4\r\n  - traitlets=5.0.5\r\n  - wcwidth=0.2.5\r\n  - webencodings=0.5.1\r\n  - wheel=0.35.1\r\n  - xz=5.2.5\r\n  - zeromq=4.3.3\r\n  - zipp=3.3.1\r\n  - zlib=1.2.11\r\n  - pip:\r\n    - adal==1.2.5\r\n    - aiobotocore==1.1.2\r\n    - aiohttp==3.6.3\r\n    - aioitertools==0.7.0\r\n    - applicationinsights==0.11.9\r\n    - async-timeout==3.0.1\r\n    - azure-common==1.1.25\r\n    - azure-core==1.8.2\r\n    - azure-graphrbac==0.61.1\r\n    - azure-identity==1.4.1\r\n    - azure-mgmt-authorization==0.61.0\r\n    - azure-mgmt-containerregistry==2.8.0\r\n    - azure-mgmt-keyvault==2.2.0\r\n    - azure-mgmt-resource==10.2.0\r\n    - azure-mgmt-storage==11.2.0\r\n    - azureml-automl-core==1.16.0\r\n    - azureml-core==1.16.0.post1\r\n    - azureml-dataprep==2.3.4\r\n    - azureml-dataprep-native==23.0.0\r\n    - azureml-dataprep-rslex==1.1.3\r\n    - azureml-dataset-runtime==1.16.0\r\n    - azureml-pipeline==1.16.0\r\n    - azureml-pipeline-core==1.16.0\r\n    - azureml-pipeline-steps==1.16.0\r\n    - azureml-sdk==1.16.0\r\n    - azureml-telemetry==1.16.0\r\n    - azureml-train==1.16.0\r\n    - azureml-train-automl-client==1.16.0\r\n    - azureml-train-core==1.16.0\r\n    -     azureml-train-restclients-hyperdrive==1.16.0\r\n    - backports-tempfile==1.0\r\n    - backports-weakref==1.0.post1\r\n    - blosc==1.9.2\r\n    - botocore==1.17.44\r\n    - chardet==3.0.4\r\n    - click==7.1.2\r\n    - cloudpickle==1.6.0\r\n    - contextlib2==0.6.0.post1\r\n    - cryptography==3.1.1\r\n    - dask==2.30.0\r\n    - dask-cloudprovider==0.4.1\r\n    - dask-glm==0.2.0\r\n    - dask-ml==1.6.0\r\n    - distributed==2.30.0\r\n    - distro==1.5.0\r\n    - docker==4.3.1\r\n    - docutils==0.15.2\r\n    - dotnetcore2==2.1.17\r\n    - fsspec==0.8.4\r\n    - fusepy==3.0.1\r\n    - heapdict==1.0.1\r\n    - idna==2.10\r\n    - isodate==0.6.0\r\n    - jeepney==0.4.3\r\n    - jmespath==0.10.0\r\n    - joblib==0.17.0\r\n    - jsonpickle==1.4.1\r\n    - llvmlite==0.34.0\r\n    - lz4==3.1.0\r\n    - msal==1.5.1\r\n    - msal-extensions==0.2.2\r\n    - msgpack==1.0.0\r\n    - msrest==0.6.19\r\n    - msrestazure==0.6.4\r\n    - multidict==4.7.6\r\n    - multipledispatch==0.6.0\r\n    - ndg-httpsclient==0.5.1\r\n    - numba==0.51.2\r\n    - numpy==1.19.0\r\n    - oauthlib==3.1.0\r\n    - pandas==1.1.0\r\n    - pathspec==0.8.0\r\n    - portalocker==1.7.1\r\n    - psutil==5.7.2\r\n    - pyarrow==1.0.1\r\n    - pyasn1==0.4.8\r\n    - pyjwt==1.7.1\r\n    - pyopenssl==19.1.0\r\n    - pytz==2020.1\r\n    - pyyaml==5.3.1\r\n    - requests==2.24.0\r\n    - requests-oauthlib==1.3.0\r\n    - ruamel-yaml==0.16.12\r\n    - ruamel-yaml-clib==0.2.2\r\n    - s3fs==0.5.1\r\n    - scikit-learn==0.23.2\r\n    - scipy==1.5.3\r\n    - secretstorage==3.1.2\r\n    - sortedcontainers==2.2.2\r\n    - tblib==1.7.0\r\n    - threadpoolctl==2.1.0\r\n    - toolz==0.11.1\r\n    - typing-extensions==3.7.4.3\r\n    - urllib3==1.25.11\r\n    - websocket-client==0.57.0\r\n    - wrapt==1.12.1\r\n    - yarl==1.5.1\r\n    - zict==2.0.0\r\n```\r\n\r\n</details>\r\n\r\n```shell\r\nconda env create --name daskdev --file env.yml\r\nsource activate daskdev\r\nconda install -y nb_conda_kernels\r\npython -m ipykernel install \\\r\n    --user \\\r\n    --name daskdev \\\r\n    --display-name \"Python (daskdev)\"\r\n```\r\n\r\n2. [authenticate with AWS](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-creds), then start run Jupyter Lab\r\n    ```shell\r\n    jupyter lab\r\n    ```\r\n\r\n3. start up a new notebook using that kernel you just created.\r\n\r\n```python\r\nfrom dask_cloudprovider import FargateCluster\r\n\r\n# valid memory + vcpu combinations:\r\n# * https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html\r\ncluster = FargateCluster(\r\n    image=\"daskdev/dask:2.30.0\",\r\n    worker_mem=30720,\r\n    scheduler_mem=30720,\r\n    scheduler_cpu=4096,\r\n    n_workers=1,\r\n    fargate_use_private_ip=False,\r\n    scheduler_timeout=\"45 minutes\",\r\n    environment={\r\n        \"EXTRA_PIP_PACKAGES\": \"s3fs\"\r\n    }\r\n)\r\n\r\nprint(cluster.dashboard_link)\r\nprint(cluster.scheduler_address)\r\n```\r\n\r\n4. Open a new browser window with the result of `print(cluster.dashboard_link)`. Keep it open.\r\n    * record the current memory usage in the cluster. It will probably be on the order of 100 MB\r\n    * ![image](https://user-images.githubusercontent.com/7608904/96830511-27c9ee00-1401-11eb-8228-9c2c38c6aed8.png)\r\n\r\n5. Copy the output of `print(cluster.scheduler_address)`. You'll need it in the next step.\r\n6. in another notebook using the kernel created in step 1, run this code (after pasting in the scheduler address)\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nfrom dask.distributed import Client, wait\r\n\r\nscheduler_address = # PASTE result of cluster.scheduler_address from other noteboook\r\nclient = Client(address=scheduler_address)\r\n\r\ndef get_data_frame():\r\n    ddf = dd.read_csv(\r\n        urlpath=\"s3://nyc-tlc/trip data/yellow_tripdata_2018-1*.csv\",\r\n        storage_options={'anon': True}\r\n    )\r\n   ddf = ddf.persist()\r\n    _ = wait(ddf)\r\n    return ddf\r\n\r\ndef summarize(ddf):\r\n    computed = ddf.compute()\r\n    print(computed.describe())\r\n\r\nddf = get_data_frame()\r\nsummarize(ddf)\r\n```\r\n\r\n7. When `summarize()` finishes running successfully, restart this notebook's kernel. Return to the Dask dashboard, refresh the page (just to be safe), and record the memory usage in the cluster.\r\n8. Repeat steps 5-6 several times. You should see that after each kernel restart on the client notebook, the cluster has higher memory utilization than it did after the previous restart.\r\n\r\nDoing this, I observed the following pattern of memory usage:\r\n\r\n* cluster startup: 101.7  MB\r\n* after run 1:  2.45 GB\r\n* after run 2: 2.87 GB\r\n* after run  3: 3.05  GB\r\n\r\nThe peak memory usage I observed while running the example code was around 10GB, so around 3GB being leaked seems noteworthy.\r\n\r\n### Anything else we need to know?\r\n\r\nI originally started investigating this today because I was running a workload with [`prefect`'s `DaskExecutor`](https://github.com/PrefectHQ/prefect/blob/ebbec05019f054dd80d64cd12750368002aa9fa0/src/prefect/engine/executors/dask.py) and noticed that each run had less memory available to it than the previous one. After coming up with the reproducible example in this issue, I've convinced myself that the problem I saw has nothing to do with how `prefect` does its Dask stuff.\r\n\r\nI did try to do my due diligence looking for other reports of this issue. There are many similar ones, but I didn't see any that involved `s3fs` (which mine indirectly does because it uses `DataFrame.read_csv()`) or which use `DataFrame.persist()`. Related issues that I found:\r\n\r\n* https://github.com/dask/dask/issues/3247\r\n* https://github.com/dask/dask/issues/3530\r\n* https://github.com/dask/dask/issues/5864\r\n* https://github.com/dask/dask/issues/6486\r\n* https://github.com/dask/distributed/issues/2068\r\n* https://github.com/dask/distributed/issues/2757\r\n* https://github.com/dask/distributed/issues/3103\r\n* https://github.com/dask/distributed/issues/3201\r\n* https://stackoverflow.com/questions/64046973/dask-memory-leakage-issue-with-json-and-requests/\r\n\r\nI did also consult [\"Managing Memory\"](https://distributed.dask.org/en/latest/memory.html) from the Dask docs and found that VERY helpful!\r\n\r\n### Environment:\r\n\r\nSee the `env.yml` I provided above for the client environment. The scheduler + workers in my example ran in containers using the `daskdev/dask:2.30.0` docker image.\r\n\r\n- Dask version:\r\n    - `dask` 2.30.0\r\n    - `distributed` 2.30.0\r\n    - `s3fs` 0.5.1\r\n    - `dask-cloudprovider` 0.4.1\r\n- Python version:\r\n    - 3.8\r\n- Operating System:\r\n    - client on macOS 10.14.6, scheduler and workers in daskdev/dask:2.30.0` image.\r\n- Install method (conda, pip, source):\r\n    - `pip`\r\n\r\n<hr>\r\n\r\nThanks for your time and consideration!",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/6762/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/6762/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}