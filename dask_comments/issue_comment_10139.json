[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1496118482",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1496118482",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1496118482,
        "node_id": "IC_kwDOAbcwm85ZLPTS",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-04T14:55:21Z",
        "updated_at": "2023-04-04T14:55:21Z",
        "author_association": "MEMBER",
        "body": "Should we pin the issue? Makes it more visible as it gets pushed down.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1496118482/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1655740906",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1655740906",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1655740906,
        "node_id": "IC_kwDOAbcwm85isJnq",
        "user": {
            "login": "aiudirog",
            "id": 5399935,
            "node_id": "MDQ6VXNlcjUzOTk5MzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5399935?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aiudirog",
            "html_url": "https://github.com/aiudirog",
            "followers_url": "https://api.github.com/users/aiudirog/followers",
            "following_url": "https://api.github.com/users/aiudirog/following{/other_user}",
            "gists_url": "https://api.github.com/users/aiudirog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aiudirog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aiudirog/subscriptions",
            "organizations_url": "https://api.github.com/users/aiudirog/orgs",
            "repos_url": "https://api.github.com/users/aiudirog/repos",
            "events_url": "https://api.github.com/users/aiudirog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aiudirog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-28T13:59:50Z",
        "updated_at": "2023-07-28T13:59:50Z",
        "author_association": "NONE",
        "body": "Please let me know if I should open a separate ticket for this, but it appears the conversion to PyArrow strings is a bit too greedy and is picking up other objects as well:\r\n\r\n```python\r\nIn [10]: import dask.dataframe as ddf\r\n\r\nIn [11]: import pandas as pd\r\n\r\nIn [12]: df = pd.DataFrame({'lists': pd.Series([['a', 'b'], ['c', 'd']])})\r\n\r\nIn [13]: df\r\nOut[13]: \r\n    lists\r\n0  [a, b]\r\n1  [c, d]\r\n\r\nIn [14]: df.dtypes\r\nOut[14]: \r\nlists    object\r\ndtype: object\r\n\r\nIn [15]: ddf.from_pandas(df, npartitions=1).compute()\r\nOut[15]: \r\n        lists\r\n0  ['a', 'b']\r\n1  ['c', 'd']\r\n\r\nIn [16]: ddf.from_pandas(df, npartitions=1).compute().dtypes\r\nOut[16]: \r\nlists    string[pyarrow]\r\ndtype: object\r\n```\r\n\r\nWhile in most cases users should probably be avoiding the object type when possible, I personally feel it's a regression to assume all objects are strings.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1655740906/reactions",
            "total_count": 3,
            "+1": 3,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1655928619",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1655928619",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1655928619,
        "node_id": "IC_kwDOAbcwm85is3cr",
        "user": {
            "login": "j-bennet",
            "id": 637013,
            "node_id": "MDQ6VXNlcjYzNzAxMw==",
            "avatar_url": "https://avatars.githubusercontent.com/u/637013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/j-bennet",
            "html_url": "https://github.com/j-bennet",
            "followers_url": "https://api.github.com/users/j-bennet/followers",
            "following_url": "https://api.github.com/users/j-bennet/following{/other_user}",
            "gists_url": "https://api.github.com/users/j-bennet/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/j-bennet/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/j-bennet/subscriptions",
            "organizations_url": "https://api.github.com/users/j-bennet/orgs",
            "repos_url": "https://api.github.com/users/j-bennet/repos",
            "events_url": "https://api.github.com/users/j-bennet/events{/privacy}",
            "received_events_url": "https://api.github.com/users/j-bennet/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-28T15:57:08Z",
        "updated_at": "2023-07-28T15:57:08Z",
        "author_association": "MEMBER",
        "body": "@aiudirog Thank you for your feedback, yes, this is a known problem. Since Dask (unlike pandas) is lazy, it doesn't load and parse your data until it's time to read it. So it has no way to determine if columns marked with `object` dtype contain strings, or something else. There aren't very many use cases for people to store complex data in a column, so Dask assumes they are strings.\r\n\r\nFor people that do store complex data in a column, it would be advisable to set `dataframe.convert-string` setting to `false`, and for columns that contain strings, explicitly provide `string[pyarrow]` dtype when reading data.\r\n\r\nI hope this helps.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1655928619/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1664877956",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1664877956",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1664877956,
        "node_id": "IC_kwDOAbcwm85jPAWE",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-04T02:17:46Z",
        "updated_at": "2023-08-04T02:27:27Z",
        "author_association": "MEMBER",
        "body": "We ran into a small problem with the new dask arrow strings. \r\nI say \"we\" but really I mean @m-albert and @jakirkham - I didn't have anything to do with this, but I'm copying over the relevant issue here for visibility.\r\n\r\nhttps://github.com/dask/dask-image/issues/335\r\n> (Edited by @m-albert)\r\n> \r\n> In the presence of `pyarrow`, dask by default assumes dataframes of type object to be pyarrow strings (see [dask/dask#10139 (comment)](https://github.com/dask/dask/issues/10139#issuecomment-1655928619)).\r\n> \r\n> This creates problems revealed by failing tests (e.g. `test_dask_image/test_ndmeasure/test_find_objects.py::test_3d_find_objects`)\r\n> \r\n> https://github.com/dask/dask-image/blob/67540af25597f84e4a642d644ba30dce7aebe753/dask_image/ndmeasure/_utils/_find_objects.py#L68-L70\r\n> \r\n> `dd.from_delayed(df1, meta=meta).compute().dtypes`\r\n> \r\n> Working install:\r\n> \r\n> > 0    object\r\n> > 1    object\r\n> > 2    object\r\n> > dtype: object\r\n> \r\n> Failing install:\r\n> \r\n> > 0    string[pyarrow]\r\n> > 1    string[pyarrow]\r\n> > 2    string[pyarrow]\r\n> > dtype: object\r\n> \r\n> The failing test had come up when releasing v2023.08.0 in [conda-forge/dask-image-feedstock#14](https://github.com/conda-forge/dask-image-feedstock/pull/14).\r\n> \r\n> @jakirkham found that `pyarrow` is installed with the [conda distribution of dask](https://github.com/conda-forge/dask-feedstock/blob/89ff1ea9116e0798d9073d819a6b9b22fa1d0917/recipe/meta.yaml#L31), but not when [installing over pip](https://github.com/dask/dask/blob/main/pyproject.toml), where it just part of the `[complete]` target.\r\n> \r\n> Also @jakirkham found that the above described conflicting behaviour can be [turned off using the dask configuration](https://github.com/dask-contrib/dask-sql/commit/af180620216793b7c6125b7db8f98dd0bba038de#diff-a31c7ed5d35f5ed8233994868c54d625b18e6bacb6794344c4531e62bd9dde59R15-R16).\r\n> \r\n> He did this for the tests performed by the dask-image conda feedstock on [v2028.08.0](https://github.com/conda-forge/dask-image-feedstock/pull/14).\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1664877956/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1665090298",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1665090298",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1665090298,
        "node_id": "IC_kwDOAbcwm85jP0L6",
        "user": {
            "login": "jakirkham",
            "id": 3019665,
            "node_id": "MDQ6VXNlcjMwMTk2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jakirkham",
            "html_url": "https://github.com/jakirkham",
            "followers_url": "https://api.github.com/users/jakirkham/followers",
            "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
            "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
            "organizations_url": "https://api.github.com/users/jakirkham/orgs",
            "repos_url": "https://api.github.com/users/jakirkham/repos",
            "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jakirkham/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-04T06:40:23Z",
        "updated_at": "2023-08-04T06:40:23Z",
        "author_association": "MEMBER",
        "body": "We also saw some kind of issue in Dask-SQL related to this change. It popped up when we were trying to wrap up a few fixes and make a release. Unfortunately don't have more details since we haven't had any time to investigate yet. So we disabled it for now ( https://github.com/dask-contrib/dask-sql/pull/1206 ). Just wanted to add that data point here as well",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1665090298/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1665390649",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1665390649",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1665390649,
        "node_id": "IC_kwDOAbcwm85jQ9g5",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-04T10:30:55Z",
        "updated_at": "2023-08-04T10:30:55Z",
        "author_association": "MEMBER",
        "body": "@GenevieveBuckley I don't think that I am following completely. The option is only pulled if the correct Arrow version is installed, so different forms of behaviour are expected. Is there anything else that is not working as expected that we should be aware of?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1665390649/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1665402716",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1665402716",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1665402716,
        "node_id": "IC_kwDOAbcwm85jRAdc",
        "user": {
            "login": "m-albert",
            "id": 12528388,
            "node_id": "MDQ6VXNlcjEyNTI4Mzg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/12528388?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/m-albert",
            "html_url": "https://github.com/m-albert",
            "followers_url": "https://api.github.com/users/m-albert/followers",
            "following_url": "https://api.github.com/users/m-albert/following{/other_user}",
            "gists_url": "https://api.github.com/users/m-albert/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/m-albert/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/m-albert/subscriptions",
            "organizations_url": "https://api.github.com/users/m-albert/orgs",
            "repos_url": "https://api.github.com/users/m-albert/repos",
            "events_url": "https://api.github.com/users/m-albert/events{/privacy}",
            "received_events_url": "https://api.github.com/users/m-albert/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-04T10:41:52Z",
        "updated_at": "2023-08-04T10:41:52Z",
        "author_association": "NONE",
        "body": "@phofl Trying to clarify: I see how different behaviors are expected depending on whether the correct arrow version is installed. Our problem was exactly about that, namely that `pyarrow>7` is not a regular dependency of the dask distribution on pip, but it is on conda. CI testing was performed on pip installed packages, and errors only showed up when testing a conda installation during the release process.\r\n\r\nSo in a way the problem we had was not related to the direct use of arrow strings, but the current mismatch between the pip and conda dask distributions.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1665402716/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1665559544",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1665559544",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1665559544,
        "node_id": "IC_kwDOAbcwm85jRmv4",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-04T12:51:37Z",
        "updated_at": "2023-08-04T12:51:37Z",
        "author_association": "MEMBER",
        "body": "Ah got it. @jrbourbeau can provide more context on the packaging requirements",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1665559544/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1666307201",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1666307201",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1666307201,
        "node_id": "IC_kwDOAbcwm85jUdSB",
        "user": {
            "login": "jakirkham",
            "id": 3019665,
            "node_id": "MDQ6VXNlcjMwMTk2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jakirkham",
            "html_url": "https://github.com/jakirkham",
            "followers_url": "https://api.github.com/users/jakirkham/followers",
            "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
            "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
            "organizations_url": "https://api.github.com/users/jakirkham/orgs",
            "repos_url": "https://api.github.com/users/jakirkham/repos",
            "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jakirkham/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-05T00:38:30Z",
        "updated_at": "2023-08-05T00:45:46Z",
        "author_association": "MEMBER",
        "body": "The `dask` conda-forge package [requires `pyarrow` 7 or later]( https://github.com/conda-forge/dask-feedstock/blame/89ff1ea9116e0798d9073d819a6b9b22fa1d0917/recipe/meta.yaml#L31 ), which means it will pick up the latest `pyarrow` in conda-forge (currently [`pyarrow` 12.0.1]( https://github.com/conda-forge/arrow-cpp-feedstock/blob/a32211f9006476e07db5011058745364314ef0a1/recipe/meta.yaml#L1 )). How would this affect the behavior we are seeing?\n\nEdit: Also noting that `pip install dask[complete]` likely would have similar behavior (as this would install the latest `pyarrow` wheel, which is [currently 12.0.1]( https://pypi.org/project/pyarrow/12.0.1/ )) as [`pyarrow` 7 or later is required by `dask[complete]`]( https://github.com/dask/dask/blob/ca4d1d5830a2efe9650aa6485302193c2aacaf7b/pyproject.toml#L67 )",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1666307201/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1811593878",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1811593878",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1811593878,
        "node_id": "IC_kwDOAbcwm85r-rqW",
        "user": {
            "login": "zmbc",
            "id": 13357648,
            "node_id": "MDQ6VXNlcjEzMzU3NjQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/13357648?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zmbc",
            "html_url": "https://github.com/zmbc",
            "followers_url": "https://api.github.com/users/zmbc/followers",
            "following_url": "https://api.github.com/users/zmbc/following{/other_user}",
            "gists_url": "https://api.github.com/users/zmbc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zmbc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zmbc/subscriptions",
            "organizations_url": "https://api.github.com/users/zmbc/orgs",
            "repos_url": "https://api.github.com/users/zmbc/repos",
            "events_url": "https://api.github.com/users/zmbc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zmbc/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-11-15T00:12:19Z",
        "updated_at": "2023-11-15T16:18:18Z",
        "author_association": "NONE",
        "body": "`string[pyarrow]` [imposes a 2GB limit](https://github.com/apache/arrow/issues/33049), which I have run into when manipulating very large dataframes. Is this expected and if not, is it basically an upstream issue Dask has no control over? I know PyArrow has a `large_string` type but it doesn't seem that Pandas supports it.\r\n\r\nEdit: if useful, here is an example stacktrace:\r\n\r\n<details>\r\n\r\n```\r\nFile .../python3.10/site-packages/dask/dataframe/multi.py:289, in merge_chunk()\r\n    286             else:\r\n    287                 rhs = rhs.assign(**{col: right.astype(dtype)})\r\n--> 289 out = lhs.merge(rhs, *args, **kwargs)\r\n    291 # Workaround for pandas bug where if the left frame of a merge operation is\r\n    292 # empty, the resulting dataframe can have columns in the wrong order.\r\n    293 # https://github.com/pandas-dev/pandas/issues/9937\r\n    294 if len(lhs) == 0:\r\n\r\nFile .../python3.10/site-packages/pyarrow/table.pxi:1043, in pyarrow.lib.ChunkedArray.take()\r\n   1041     ]\r\n   1042     \"\"\"\r\n-> 1043     return _pc().take(self, indices)\r\n   1044 \r\n   1045 def drop_null(self):\r\n\r\nFile .../python3.10/site-packages/pyarrow/compute.py:486, in take()\r\n    446 \"\"\"\r\n    447 Select values (or records) from array- or table-like data given integer\r\n    448 selection indices.\r\n   (...)\r\n    483 ]\r\n    484 \"\"\"\r\n    485 options = TakeOptions(boundscheck=boundscheck)\r\n--> 486 return call_function('take', [data, indices], options, memory_pool)\r\n\r\nFile .../python3.10/site-packages/pyarrow/_compute.pyx:590, in pyarrow._compute.call_function()\r\n    588 \"\"\"\r\n    589 func = _global_func_registry.get_function(name)\r\n--> 590 return func.call(args, options=options, memory_pool=memory_pool,\r\n    591                  length=length)\r\n    592 \r\n\r\nFile .../python3.10/site-packages/pyarrow/_compute.pyx:385, in pyarrow._compute.Function.call()\r\n    383 else:\r\n    384     with nogil:\r\n--> 385         result = GetResultValue(\r\n    386             self.base_func.Execute(c_batch.values, c_options,\r\n    387                                    &c_exec_ctx)\r\n\r\nFile .../python3.10/site-packages/pyarrow/error.pxi:154, in pyarrow.lib.pyarrow_internal_check_status()\r\n    152 cdef api int pyarrow_internal_check_status(const CStatus& status) \\\r\n    153         except -1 nogil:\r\n--> 154     return check_status(status)\r\n    155 \r\n    156 cdef api object pyarrow_internal_convert_status(const CStatus& status):\r\n\r\nFile .../python3.10/site-packages/pyarrow/error.pxi:91, in pyarrow.lib.check_status()\r\n     89     return -1\r\n     90 \r\n---> 91 raise convert_status(status)\r\n     92 \r\n     93 \r\n\r\nArrowInvalid: offset overflow while concatenating arrays\r\n```\r\n\r\n</details>",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1811593878/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1812140951",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1812140951",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1812140951,
        "node_id": "IC_kwDOAbcwm85sAxOX",
        "user": {
            "login": "phofl",
            "id": 61934744,
            "node_id": "MDQ6VXNlcjYxOTM0NzQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/61934744?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/phofl",
            "html_url": "https://github.com/phofl",
            "followers_url": "https://api.github.com/users/phofl/followers",
            "following_url": "https://api.github.com/users/phofl/following{/other_user}",
            "gists_url": "https://api.github.com/users/phofl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/phofl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/phofl/subscriptions",
            "organizations_url": "https://api.github.com/users/phofl/orgs",
            "repos_url": "https://api.github.com/users/phofl/repos",
            "events_url": "https://api.github.com/users/phofl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/phofl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-11-15T10:00:11Z",
        "updated_at": "2023-11-15T10:00:11Z",
        "author_association": "MEMBER",
        "body": "> I know PyArrow has a large_string type but it doesn't seem that Pandas supports it.\r\n\r\npandas does support it, you can do:\r\n\r\n```\r\nimport pyarrow as pa\r\nimport pandas as pd\r\n\r\ndtype = pd.ArrowDtype(pa.large_string())\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1812140951/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1812817180",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1812817180",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1812817180,
        "node_id": "IC_kwDOAbcwm85sDWUc",
        "user": {
            "login": "zmbc",
            "id": 13357648,
            "node_id": "MDQ6VXNlcjEzMzU3NjQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/13357648?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zmbc",
            "html_url": "https://github.com/zmbc",
            "followers_url": "https://api.github.com/users/zmbc/followers",
            "following_url": "https://api.github.com/users/zmbc/following{/other_user}",
            "gists_url": "https://api.github.com/users/zmbc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zmbc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zmbc/subscriptions",
            "organizations_url": "https://api.github.com/users/zmbc/orgs",
            "repos_url": "https://api.github.com/users/zmbc/repos",
            "events_url": "https://api.github.com/users/zmbc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zmbc/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-11-15T16:05:32Z",
        "updated_at": "2023-11-15T16:05:32Z",
        "author_association": "NONE",
        "body": "@phofl Good point! I thought it wouldn't be possible for Pandas to load a Parquet file with that dtype, but I just tested it, and it works.\r\n\r\nNote: The _intended_ limitation of `string[pyarrow]` is that it can only hold 2GB _per Arrow chunk_. Generally Arrow chunks should be invisible to the user (Dask, in this case) unless they are using low-level Arrow APIs. However, the issue I linked above (and [this related one](https://github.com/apache/arrow/issues/25822)) are bugs that cause overflows to happen in practice.\r\n\r\nI tried converting to `large_string[pyarrow]` using a map_partitions within Dask, and it worked but I got an error during later processing -- I don't have the error message handy, but could retrieve it. But again, I'm not sure if this is surprising since Dask hasn't been designed to work with this type.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1812817180/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1812969372",
        "html_url": "https://github.com/dask/dask/issues/10139#issuecomment-1812969372",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10139",
        "id": 1812969372,
        "node_id": "IC_kwDOAbcwm85sD7ec",
        "user": {
            "login": "zmbc",
            "id": 13357648,
            "node_id": "MDQ6VXNlcjEzMzU3NjQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/13357648?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zmbc",
            "html_url": "https://github.com/zmbc",
            "followers_url": "https://api.github.com/users/zmbc/followers",
            "following_url": "https://api.github.com/users/zmbc/following{/other_user}",
            "gists_url": "https://api.github.com/users/zmbc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zmbc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zmbc/subscriptions",
            "organizations_url": "https://api.github.com/users/zmbc/orgs",
            "repos_url": "https://api.github.com/users/zmbc/repos",
            "events_url": "https://api.github.com/users/zmbc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zmbc/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-11-15T17:32:46Z",
        "updated_at": "2023-11-15T17:33:31Z",
        "author_association": "NONE",
        "body": "> I tried converting to large_string[pyarrow] using a map_partitions within Dask, and it worked but I got an error during later processing\r\n\r\nLooked into this a bit more, and it definitely seems there is a bug somewhere -- either in Dask or in PyArrow. The error I got in my real application was `ValueError: ArrowStringArray requires a PyArrow (chunked) array of string type` with this stacktrace:\r\n\r\n<details>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_core.py:405, in handle_unpack_errors()\r\n    404 try:\r\n--> 405     yield\r\n    406 except Reschedule as e:\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_shuffle.py:87, in shuffle_unpack()\r\n     86 with handle_unpack_errors(id):\r\n---> 87     return get_worker_plugin().get_output_partition(\r\n     88         id, barrier_run_id, output_partition\r\n     89     )\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_worker_plugin.py:431, in get_output_partition()\r\n    430 key = thread_state.key\r\n--> 431 return sync(\r\n    432     self.worker.loop,\r\n    433     shuffle_run.get_output_partition,\r\n    434     partition_id=partition_id,\r\n    435     key=key,\r\n    436     meta=meta,\r\n    437 )\r\n\r\nFile .../lib/python3.10/site-packages/distributed/utils.py:434, in sync()\r\n    433 if error is not None:\r\n--> 434     raise error\r\n    435 else:\r\n\r\nFile .../lib/python3.10/site-packages/distributed/utils.py:408, in f()\r\n    407     future = asyncio.ensure_future(awaitable)\r\n--> 408     result = yield future\r\n    409 except Exception as exception:\r\n\r\nFile .../lib/python3.10/site-packages/tornado/gen.py:767, in run()\r\n    766 try:\r\n--> 767     value = future.result()\r\n    768 except Exception as e:\r\n    769     # Save the exception for later. It's important that\r\n    770     # gen.throw() not be called inside this try/except block\r\n    771     # because that makes sys.exc_info behave unexpectedly.\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_core.py:276, in get_output_partition()\r\n    275 await self.flush_receive()\r\n--> 276 return await self._get_output_partition(partition_id, key, **kwargs)\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_shuffle.py:519, in _get_output_partition()\r\n    517         return convert_shards(data, meta)\r\n--> 519     out = await self.offload(_, partition_id, self.meta)\r\n    520 except KeyError:\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_core.py:163, in offload()\r\n    162 with self.time(\"cpu\"):\r\n--> 163     return await asyncio.get_running_loop().run_in_executor(\r\n    164         self.executor, partial(func, *args, **kwargs)\r\n    165     )\r\n\r\nFile .../lib/python3.10/concurrent/futures/thread.py:58, in run()\r\n     57 try:\r\n---> 58     result = self.fn(*self.args, **self.kwargs)\r\n     59 except BaseException as exc:\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_shuffle.py:517, in _()\r\n    516 data = self._read_from_disk((partition_id,))\r\n--> 517 return convert_shards(data, meta)\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_arrow.py:56, in convert_shards()\r\n     54 table = pa.concat_tables(shards)\r\n---> 56 df = from_pyarrow_table_dispatch(meta, table, self_destruct=True)\r\n     57 return df.astype(meta.dtypes, copy=False)\r\n\r\nFile .../lib/python3.10/site-packages/dask/utils.py:642, in __call__()\r\n    641 meth = self.dispatch(type(arg))\r\n--> 642 return meth(arg, *args, **kwargs)\r\n\r\nFile .../lib/python3.10/site-packages/dask/dataframe/backends.py:243, in get_pandas_dataframe_from_pyarrow()\r\n    242 types_mapper = kwargs.pop(\"types_mapper\", default_types_mapper)\r\n--> 243 return table.to_pandas(types_mapper=types_mapper, **kwargs)\r\n\r\nFile .../lib/python3.10/site-packages/pyarrow/array.pxi:884, in pyarrow.lib._PandasConvertible.to_pandas()\r\n    883 )\r\n--> 884 return self._to_pandas(options, categories=categories,\r\n    885                        ignore_metadata=ignore_metadata,\r\n\r\nFile .../lib/python3.10/site-packages/pyarrow/table.pxi:4192, in pyarrow.lib.Table._to_pandas()\r\n   4191 from pyarrow.pandas_compat import table_to_blockmanager\r\n-> 4192 mgr = table_to_blockmanager(\r\n   4193     options, self, categories,\r\n\r\nFile .../lib/python3.10/site-packages/pyarrow/pandas_compat.py:774, in table_to_blockmanager()\r\n    773 columns = _deserialize_column_index(table, all_columns, column_indexes)\r\n--> 774 blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)\r\n    776 axes = [columns, index]\r\n\r\nFile .../lib/python3.10/site-packages/pyarrow/pandas_compat.py:1124, in _table_to_blocks()\r\n   1122 result = pa.lib.table_to_blocks(options, block_table, categories,\r\n   1123                                 list(extension_columns.keys()))\r\n-> 1124 return [_reconstruct_block(item, columns, extension_columns)\r\n   1125         for item in result]\r\n\r\nFile .../lib/python3.10/site-packages/pyarrow/pandas_compat.py:1124, in <listcomp>()\r\n   1122 result = pa.lib.table_to_blocks(options, block_table, categories,\r\n   1123                                 list(extension_columns.keys()))\r\n-> 1124 return [_reconstruct_block(item, columns, extension_columns)\r\n   1125         for item in result]\r\n\r\nFile .../lib/python3.10/site-packages/pyarrow/pandas_compat.py:736, in _reconstruct_block()\r\n    734     raise ValueError(\"This column does not support to be converted \"\r\n    735                      \"to a pandas ExtensionArray\")\r\n--> 736 pd_ext_arr = pandas_dtype.__from_arrow__(arr)\r\n    737 block = _int.make_block(pd_ext_arr, placement=placement)\r\n\r\nFile .../lib/python3.10/site-packages/pandas/core/arrays/string_.py:212, in __from_arrow__()\r\n    210     from pandas.core.arrays.string_arrow import ArrowStringArray\r\n--> 212     return ArrowStringArray(array)\r\n    213 elif self.storage == \"pyarrow_numpy\":\r\n\r\nFile .../lib/python3.10/site-packages/pandas/core/arrays/string_arrow.py:129, in __init__()\r\n    125 if not pa.types.is_string(self._pa_array.type) and not (\r\n    126     pa.types.is_dictionary(self._pa_array.type)\r\n    127     and pa.types.is_string(self._pa_array.type.value_type)\r\n    128 ):\r\n--> 129     raise ValueError(\r\n    130         \"ArrowStringArray requires a PyArrow (chunked) array of string type\"\r\n    131     )\r\n\r\nValueError: ArrowStringArray requires a PyArrow (chunked) array of string type\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n... application code calling set_index on a large dataframe ...\r\n\r\nFile .../lib/python3.10/site-packages/dask/dataframe/core.py:5435, in DataFrame.set_index(***failed resolving arguments***)\r\n   5432 else:\r\n   5433     from dask.dataframe.shuffle import set_index\r\n-> 5435     return set_index(\r\n   5436         self,\r\n   5437         other,\r\n   5438         drop=drop,\r\n   5439         npartitions=npartitions,\r\n   5440         divisions=divisions,\r\n   5441         sort=sort,\r\n   5442         **kwargs,\r\n   5443     )\r\n\r\nFile .../lib/python3.10/site-packages/dask/dataframe/shuffle.py:242, in set_index(df, index, npartitions, shuffle, compute, drop, upsample, divisions, partition_size, sort, **kwargs)\r\n    239     index2 = index\r\n    241 if divisions is None:\r\n--> 242     divisions, mins, maxes, presorted = _calculate_divisions(\r\n    243         df, index2, repartition, npartitions, upsample, partition_size\r\n    244     )\r\n    246     if presorted and npartitions == df.npartitions:\r\n    247         divisions = mins + [maxes[-1]]\r\n\r\nFile .../lib/python3.10/site-packages/dask/dataframe/shuffle.py:54, in _calculate_divisions(df, partition_col, repartition, npartitions, upsample, partition_size, ascending)\r\n     51 maxes = partition_col.map_partitions(M.max)\r\n     53 try:\r\n---> 54     divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)\r\n     55 except TypeError as e:\r\n     56     # When there are nulls and a column is non-numeric, a TypeError is sometimes raised as a result of\r\n     57     # 1) computing mins/maxes above, 2) every null being switched to NaN, and 3) NaN being a float.\r\n     58     # Also, Pandas ExtensionDtypes may cause TypeErrors when dealing with special nulls such as pd.NaT or pd.NA.\r\n     59     # If this happens, we hint the user about eliminating nulls beforehand.\r\n     60     if not is_numeric_dtype(partition_col.dtype):\r\n\r\nFile .../lib/python3.10/site-packages/dask/base.py:628, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    625     postcomputes.append(x.__dask_postcompute__())\r\n    627 with shorten_traceback():\r\n--> 628     results = schedule(dsk, keys, **kwargs)\r\n    630 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_shuffle.py:86, in shuffle_unpack()\r\n     83 def shuffle_unpack(\r\n     84     id: ShuffleId, output_partition: int, barrier_run_id: int\r\n     85 ) -> pd.DataFrame:\r\n---> 86     with handle_unpack_errors(id):\r\n     87         return get_worker_plugin().get_output_partition(\r\n     88             id, barrier_run_id, output_partition\r\n     89         )\r\n\r\nFile .../lib/python3.10/contextlib.py:153, in __exit__()\r\n    151     value = typ()\r\n    152 try:\r\n--> 153     self.gen.throw(typ, value, traceback)\r\n    154 except StopIteration as exc:\r\n    155     # Suppress StopIteration *unless* it's the same exception that\r\n    156     # was passed to throw().  This prevents a StopIteration\r\n    157     # raised inside the \"with\" statement from being suppressed.\r\n    158     return exc is not value\r\n\r\nFile .../lib/python3.10/site-packages/distributed/shuffle/_core.py:411, in handle_unpack_errors()\r\n    409     raise Reschedule()\r\n    410 except Exception as e:\r\n--> 411     raise RuntimeError(f\"P2P shuffling {id} failed during unpack phase\") from e\r\n\r\nRuntimeError: P2P shuffling 9889e954166aa37ea8c6e3c4b76043ec failed during unpack phase\r\n```\r\n\r\n</details>\r\n\r\nI can reproduce the error like this:\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport pandas as pd\r\ndtype = pd.ArrowDtype(pa.large_string())\r\nother_dtype = pd.StringDtype(\"pyarrow\")\r\n\r\ndf = pd.DataFrame({'foo': ['bar', 'baz'], 'bar': ['baz', 'quux']})\r\ndf['foo'] = df.foo.astype(dtype)\r\ndf['bar'] = df.foo.astype(other_dtype)\r\n\r\n# Lightly modified from https://github.com/dask/dask/blob/b2f11d026d2c6f806036c050ff5dbd59d6ceb6ec/dask/dataframe/backends.py#L232-L240\r\ndef default_types_mapper(pyarrow_dtype: pa.DataType) -> object:\r\n    # Avoid converting strings from `string[pyarrow]` to\r\n    # `string[python]` if we have *any* `string[pyarrow]`\r\n    if (\r\n        pyarrow_dtype in {pa.large_string(), pa.string()}\r\n        and pd.StringDtype(\"pyarrow\") in df.dtypes.values\r\n    ):\r\n        return pd.StringDtype(\"pyarrow\")\r\n    return None\r\n\r\n# Emulating https://github.com/dask/dask/blob/b2f11d026d2c6f806036c050ff5dbd59d6ceb6ec/dask/dataframe/backends.py#L243\r\npa.Table.from_pandas(df).to_pandas(types_mapper=default_types_mapper)\r\n```\r\n\r\nIt seems that the failure depends on there being _other_ strings in the DataFrame that are not `large_string[pyarrow]`. Dask then uses a types_mapper that converts `large_string[pyarrow]` to `string[pyarrow]`, which I don't want and also fails.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1812969372/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]