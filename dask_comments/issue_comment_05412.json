[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/531807362",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-531807362",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 531807362,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUzMTgwNzM2Mg==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-09-16T14:39:16Z",
        "updated_at": "2019-09-16T14:39:16Z",
        "author_association": "MEMBER",
        "body": "Most likely dask only supports the case where `skiprows` is a list-like value.\r\n\r\nThis could most likely be supported, but we'll need to take some care to document that the row index resets in each partition.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/531807362/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/532095210",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-532095210",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 532095210,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUzMjA5NTIxMA==",
        "user": {
            "login": "masip85",
            "id": 12863854,
            "node_id": "MDQ6VXNlcjEyODYzODU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12863854?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/masip85",
            "html_url": "https://github.com/masip85",
            "followers_url": "https://api.github.com/users/masip85/followers",
            "following_url": "https://api.github.com/users/masip85/following{/other_user}",
            "gists_url": "https://api.github.com/users/masip85/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/masip85/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/masip85/subscriptions",
            "organizations_url": "https://api.github.com/users/masip85/orgs",
            "repos_url": "https://api.github.com/users/masip85/repos",
            "events_url": "https://api.github.com/users/masip85/events{/privacy}",
            "received_events_url": "https://api.github.com/users/masip85/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-09-17T07:20:55Z",
        "updated_at": "2019-09-17T15:42:51Z",
        "author_association": "NONE",
        "body": "When I try to use large skipping list an error appears: \r\nFor example, I translate to list like this:\r\n\r\n`q = list(filter(lambda i: (i % n != 0 and i > 22)))`\r\n\r\nbeing n = 7 for example, \r\n\r\nThis is not ok:\r\n\r\n`df_ = dd.read_csv(paths[1], skiprows = q)`\r\n\r\n ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-95-b38f4e8874fd> in <module>\r\n      1 df_ = dd.read_csv(paths[0], \r\n      2                     # include_path_column = True,\r\n----> 3                     skiprows = q)\r\n\r\n/opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in read(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\r\n    580             storage_options=storage_options,\r\n    581             include_path_column=include_path_column,\r\n--> 582             **kwargs\r\n    583         )\r\n    584 \r\n\r\n/opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in read_pandas(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\r\n    438     if sample is not False and nparts < lastskiprow + need and len(b_sample) >= sample:\r\n    439         raise ValueError(\r\n--> 440             \"Sample is not large enough to include at least one \"\r\n    441             \"row of data. Please increase the number of bytes \"\r\n    442             \"in `sample` in the call to `read_csv`/`read_table`\"\r\n\r\nValueError: Sample is not large enough to include at least one row of data. Please increase the number of bytes in `sample` in the call to `read_csv`/`read_table`\r\n\r\nBut, then,I read the error conditions and I think,ok : sample = False. This works smoothly until you trying that the skipping list is longer than certain value which I dont know. If I want to use all my list, then it works very very slow. I am talking about jumping from 300ms to 30s.\r\n\r\nThis behaviour doesn't happen if I try same command with pandas. Is an issue of the size of the list. If I slice \"q\", depending if it is large or not this bug appears.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/532095210/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/532373977",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-532373977",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 532373977,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUzMjM3Mzk3Nw==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-09-17T19:48:44Z",
        "updated_at": "2019-09-17T19:48:44Z",
        "author_association": "MEMBER",
        "body": "That error message indicate the problem. You're likely filtering out the\nrows dask is using for it's sample.\n\nDid you try increasing the number of bytes in `sample` as instructed?\n\nOn Tue, Sep 17, 2019 at 2:21 AM Vicente <notifications@github.com> wrote:\n\n> When I try to use large list. For example, I translate to list like this:\n> q = list(filter(lambda i: (i % n != 0 and i > 22)))\n>\n> being n = 7 for example,\n>\n> This is not ok:\n>\n> df_ = dd.read_csv(paths[1], skiprows = q)\n> ------------------------------\n>\n> ValueError Traceback (most recent call last)\n> in\n> 1 df_ = dd.read_csv(paths[0],\n> 2 # include_path_column = True,\n> ----> 3 skiprows = q[0:6000])\n>\n> /opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in\n> read(urlpath, blocksize, collection, lineterminator, compression, sample,\n> enforce, assume_missing, storage_options, include_path_column, **kwargs)\n> 580 storage_options=storage_options,\n> 581 include_path_column=include_path_column,\n> --> 582 **kwargs\n> 583 )\n> 584\n>\n> /opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in\n> read_pandas(reader, urlpath, blocksize, collection, lineterminator,\n> compression, sample, enforce, assume_missing, storage_options,\n> include_path_column, **kwargs)\n> 438 if sample is not False and nparts < lastskiprow + need and\n> len(b_sample) >= sample:\n> 439 raise ValueError(\n> --> 440 \"Sample is not large enough to include at least one \"\n> 441 \"row of data. Please increase the number of bytes \"\n> 442 \"in sample in the call to read_csv/read_table\"\n>\n> ValueError: Sample is not large enough to include at least one row of\n> data. Please increase the number of bytes in sample in the call to\n> read_csv/read_table\n>\n> This behaviour doesn't happen if I try same command with pandas. Is an\n> issue of the size of the list. If I slice q, depending if it is large or\n> not this error appears.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/5412?email_source=notifications&email_token=AAKAOIUKWGYDW74DPXC6PYDQKCANXA5CNFSM4IXCXG5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD63SB2Q#issuecomment-532095210>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAKAOIQQCCLPM7A3OZVOBXTQKCANXANCNFSM4IXCXG5A>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/532373977/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/532546470",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-532546470",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 532546470,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUzMjU0NjQ3MA==",
        "user": {
            "login": "masip85",
            "id": 12863854,
            "node_id": "MDQ6VXNlcjEyODYzODU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12863854?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/masip85",
            "html_url": "https://github.com/masip85",
            "followers_url": "https://api.github.com/users/masip85/followers",
            "following_url": "https://api.github.com/users/masip85/following{/other_user}",
            "gists_url": "https://api.github.com/users/masip85/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/masip85/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/masip85/subscriptions",
            "organizations_url": "https://api.github.com/users/masip85/orgs",
            "repos_url": "https://api.github.com/users/masip85/repos",
            "events_url": "https://api.github.com/users/masip85/events{/privacy}",
            "received_events_url": "https://api.github.com/users/masip85/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-09-18T06:52:13Z",
        "updated_at": "2019-09-18T06:52:53Z",
        "author_association": "NONE",
        "body": "Yes. It's a rare case: I only select 113 cases and avoid 11999880 lines. (I have scripted a function that loads light data and all data, this case is of course the light one). I guess samples are for guessing dtypes. But, I have specified 3 dtypes and parsed the 4th column to datetime.So why is it using sample? I have increased it. It takes too long with this warning:\r\n\r\n> UserWarning: Unexpected behavior can result from passing skiprows when\r\nblocksize is smaller than sample size.\r\n\r\nTakes too long,from ms to 30-40s . Even accomplishing warning condition. \r\n\r\nEven using sample=False, and specifying 4 dtypes (not 3), it takes 30-40s.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/532546470/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/532647243",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-532647243",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 532647243,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUzMjY0NzI0Mw==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-09-18T11:43:39Z",
        "updated_at": "2019-09-18T11:43:39Z",
        "author_association": "MEMBER",
        "body": "What's taking too long? Dask or pandas, reading the CSV?\n\nYou might want to look into alternative file formats if possible. It sounds\nlike there isn't an issue with Dask here.\n\nOn Wed, Sep 18, 2019 at 1:52 AM Vicente <notifications@github.com> wrote:\n\n> That error message indicate the problem. You're likely filtering out the\n> rows dask is using for it's sample. Did you try increasing the number of\n> bytes in sample as instructed?\n> \u2026 <#m_4828267206829962829_>\n> On Tue, Sep 17, 2019 at 2:21 AM Vicente *@*.***> wrote: When I try to use\n> large list. For example, I translate to list like this: q =\n> list(filter(lambda i: (i % n != 0 and i > 22))) being n = 7 for example,\n> This is not ok: df_ = dd.read_csv(paths[1], skiprows = q)\n> ------------------------------ ValueError Traceback (most recent call last)\n> in 1 df_ = dd.read_csv(paths[0], 2 # include_path_column = True, ----> 3\n> skiprows = q[0:6000])\n> /opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in\n> read(urlpath, blocksize, collection, lineterminator, compression, sample,\n> enforce, assume_missing, storage_options, include_path_column, **kwargs)\n> 580 storage_options=storage_options, 581\n> include_path_column=include_path_column, --> 582 **kwargs 583 ) 584\n> /opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in\n> read_pandas(reader, urlpath, blocksize, collection, lineterminator,\n> compression, sample, enforce, assume_missing, storage_options,\n> include_path_column, **kwargs) 438 if sample is not False and nparts <\n> lastskiprow + need and len(b_sample) >= sample: 439 raise ValueError( -->\n> 440 \"Sample is not large enough to include at least one \" 441 \"row of data.\n> Please increase the number of bytes \" 442 \"in sample in the call to\n> read_csv/read_table\" ValueError: Sample is not large enough to include at\n> least one row of data. Please increase the number of bytes in sample in the\n> call to read_csv/read_table This behaviour doesn't happen if I try same\n> command with pandas. Is an issue of the size of the list. If I slice q,\n> depending if it is large or not this error appears. \u2014 You are receiving\n> this because you commented. Reply to this email directly, view it on GitHub\n> <#5412 <https://github.com/dask/dask/issues/5412>?email_source=notifications&email_token=AAKAOIUKWGYDW74DPXC6PYDQKCANXA5CNFSM4IXCXG5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD63SB2Q#issuecomment-532095210>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAKAOIQQCCLPM7A3OZVOBXTQKCANXANCNFSM4IXCXG5A\n> .\n>\n> Yes. It's a rare case: I only select 113 cases and avoid 11999880 lines.\n> (I have scripted a function that loads light data and all data, this case\n> is of course the light). I guess samples are for guessing dtypes. But, I\n> have specified 3 dtypes and parsed the 4th column to datetime.So why is it\n> using sample? I have increased it. It takes too long with this warning:\n>\n> UserWarning: Unexpected behavior can result from passing skiprows when\n> blocksize is smaller than sample size.\n>\n> Takes too long,from ms to 30-40s . Even accomplishing warning condition.\n>\n> Even using sample=False, and specifying 4 dtypes (not 3), it takes 30-40s.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/5412?email_source=notifications&email_token=AAKAOIXGJRBKOMQEHCTCRADQKHF2DA5CNFSM4IXCXG5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD67AHJQ#issuecomment-532546470>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAKAOIWRAG5ETDZBJRMPKRTQKHF2DANCNFSM4IXCXG5A>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/532647243/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/532668799",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-532668799",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 532668799,
        "node_id": "MDEyOklzc3VlQ29tbWVudDUzMjY2ODc5OQ==",
        "user": {
            "login": "masip85",
            "id": 12863854,
            "node_id": "MDQ6VXNlcjEyODYzODU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12863854?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/masip85",
            "html_url": "https://github.com/masip85",
            "followers_url": "https://api.github.com/users/masip85/followers",
            "following_url": "https://api.github.com/users/masip85/following{/other_user}",
            "gists_url": "https://api.github.com/users/masip85/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/masip85/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/masip85/subscriptions",
            "organizations_url": "https://api.github.com/users/masip85/orgs",
            "repos_url": "https://api.github.com/users/masip85/repos",
            "events_url": "https://api.github.com/users/masip85/events{/privacy}",
            "received_events_url": "https://api.github.com/users/masip85/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-09-18T12:48:44Z",
        "updated_at": "2019-09-18T12:48:44Z",
        "author_association": "NONE",
        "body": "Of course there is an issue, I am writing it. XD\r\n\r\nDask takes too long! 0 problems and 0 issues with pandas. No error with pandas using lambda funtion, or long list.Dask hasn't got available lambda function, and for long list it increases from ms to 30s for reading same file/s.\r\n\r\nWell my approach nowdays is reading it completely and no using my skipRows list. After reading I slice the dataframe imitating my list. But, well, I just wanted to report it to you.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/532668799/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/541830636",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-541830636",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 541830636,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU0MTgzMDYzNg==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-10-14T18:06:06Z",
        "updated_at": "2019-10-14T18:06:06Z",
        "author_association": "MEMBER",
        "body": "> for long list it increases from ms to 30s for reading same file/s.\r\n\r\n@masip85 do you have a minimal example to share http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports?\r\n\r\nRight now a sample is always taken, even when `dtype` is specified. That alone doesn't give us enough information to parse all the CSV files.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/541830636/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/543133527",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-543133527",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 543133527,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU0MzEzMzUyNw==",
        "user": {
            "login": "masip85",
            "id": 12863854,
            "node_id": "MDQ6VXNlcjEyODYzODU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12863854?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/masip85",
            "html_url": "https://github.com/masip85",
            "followers_url": "https://api.github.com/users/masip85/followers",
            "following_url": "https://api.github.com/users/masip85/following{/other_user}",
            "gists_url": "https://api.github.com/users/masip85/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/masip85/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/masip85/subscriptions",
            "organizations_url": "https://api.github.com/users/masip85/orgs",
            "repos_url": "https://api.github.com/users/masip85/repos",
            "events_url": "https://api.github.com/users/masip85/events{/privacy}",
            "received_events_url": "https://api.github.com/users/masip85/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-10-17T11:37:19Z",
        "updated_at": "2019-10-17T11:37:19Z",
        "author_association": "NONE",
        "body": "> print(pd.**version**): 0.25.0\r\n> print(dask.**version**): 2.2.0\r\n> \r\n> import pandas as pd\r\n> import dask.dataframe as dd\r\n> \r\n> ```\r\n> n = 1\r\n> df = pd.read_csv(paths[1], \r\n>                     skiprows = lambda i: (i % n != 0 and i > 22) or i<=21)\r\n> ```\r\n> \r\n> This works. But:\r\n> \r\n> ```\r\n> n = 1\r\n> df = dd.read_csv(paths[1], \r\n>                     skiprows = lambda i: (i % n != 0 and i > 22) or i<=21)\r\n> ```\r\n> \r\n> Why do I obtain this error:?\r\n> \r\n> TypeError Traceback (most recent call last)\r\n> in\r\n> 1 df = dd.read_csv(paths[1],\r\n> 2 #include_path_column = True,\r\n> ----> 3 skiprows = lambda i: (i % n != 0 and i > 22)\r\n> 4 or i<=21)\r\n> \r\n> /opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in read(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\r\n> 580 storage_options=storage_options,\r\n> 581 include_path_column=include_path_column,\r\n> --> 582 **kwargs\r\n> 583 )\r\n> 584\r\n> \r\n> /opt/conda/lib/python3.7/site-packages/dask/dataframe/io/csv.py in read_pandas(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\r\n> 366 # be included in the sample. This means that [0,2] will work well,\r\n> 367 # but [0, 440] might not work.\r\n> --> 368 skiprows = set(kwargs.get(\"skiprows\"))\r\n> 369 lastskiprow = max(skiprows)\r\n> 370 # find the firstrow that is not skipped, for use as header\r\n> \r\n> **TypeError: 'function' object is not iterable**\r\n> \r\n> \u200b\r\n\r\nThis explained the initial problem: lambda are not allowed to in _skipRows_. Ok, from this limitation, I try to use a long list which contains a long numeric list which be used as rows to skip in _skipRows_ . That makes the reading very very very slow.\r\n\r\nExample:\r\n\r\n```\r\njump = 10\r\npaths = blued.getFilesPaths(tsIni, tsFin) # just a list of string which contains paths to address\r\ntoGet = range(0, 1200000, jump)\r\ntoSkip = list(filter(lambda i: (i not in toGet and i > 22), range(0,12000000))) # you can create this the way you want, but resulting in a list\r\n                                \r\n    df_ = dd.read_csv(paths, \r\n                      # include_path_column = True,\r\n                      skiprows = toSkip,\r\n                      #skiprows = 22,\r\n                      usecols = ['X_Value',\r\n                                 'Current A',\r\n                                 'Current B',\r\n                                 'VoltageA'],\r\n                      #parse_dates = ['X_Value'], \r\n                      #sample=64000000, # mandatory sample = blocksize if skiprows is present\r\n                      dtype = {\"X_Value\" : \"float64\",\r\n                               \"Current A\" : \"float16\",\r\n                               \"Current B\" : \"float16\", \r\n                               \"Voltage A\" : \"float16\"}\r\n                      #date_parser = lambda x: blued.start_datetime + timedelta(seconds = float(x))\r\n                     )\r\n```\r\n\r\nDepending on the jump number, obviously, _toSkip_ increase. Ok, there is certain length of toSkip which makes reading very slow. Curiously, pandas doesn't have issues with very long list to skip.\r\n(I cannot share data to replicate :( )",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/543133527/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/543166023",
        "html_url": "https://github.com/dask/dask/issues/5412#issuecomment-543166023",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/5412",
        "id": 543166023,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU0MzE2NjAyMw==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-10-17T13:07:16Z",
        "updated_at": "2019-10-17T13:07:16Z",
        "author_association": "MEMBER",
        "body": "> That makes the reading very very very slow.\r\n\r\nProfiling would of course be welcome.\r\n\r\nNote that when `skiprows` is provided, dask will need to read enough data to still get at least the sample bytes excluding the rows in `skiprows`.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/543166023/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]