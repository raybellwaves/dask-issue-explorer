{
    "url": "https://api.github.com/repos/dask/dask/issues/8218",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8218/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8218/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8218/events",
    "html_url": "https://github.com/dask/dask/issues/8218",
    "id": 1016483853,
    "node_id": "I_kwDOAbcwm848llAN",
    "number": 8218,
    "title": "Possible bug: `dask.array.compute()` hanging with `multiprocessing.Pool`",
    "user": {
        "login": "valeriupredoi",
        "id": 28983971,
        "node_id": "MDQ6VXNlcjI4OTgzOTcx",
        "avatar_url": "https://avatars.githubusercontent.com/u/28983971?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/valeriupredoi",
        "html_url": "https://github.com/valeriupredoi",
        "followers_url": "https://api.github.com/users/valeriupredoi/followers",
        "following_url": "https://api.github.com/users/valeriupredoi/following{/other_user}",
        "gists_url": "https://api.github.com/users/valeriupredoi/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/valeriupredoi/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/valeriupredoi/subscriptions",
        "organizations_url": "https://api.github.com/users/valeriupredoi/orgs",
        "repos_url": "https://api.github.com/users/valeriupredoi/repos",
        "events_url": "https://api.github.com/users/valeriupredoi/events{/privacy}",
        "received_events_url": "https://api.github.com/users/valeriupredoi/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862305,
            "node_id": "MDU6TGFiZWwyNDI4NjIzMDU=",
            "url": "https://api.github.com/repos/dask/dask/labels/array",
            "name": "array",
            "color": "006b75",
            "default": false,
            "description": null
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2021-10-05T15:41:54Z",
    "updated_at": "2021-11-15T01:46:41Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\n\r\nCalling `compute()` twice in the same function hangs: first time one calls `compute()` in a straightforward manner, second time via `multiprocessing.Pool`; see example below:\r\n\r\n- if I call `my_func(2)` then I go ahead and execute the mpi block (with the dask cluster commented out), the execution hangs (see stack trace below, after a `KeyboardInterrupt`);\r\n- if I repeat the operation - call `my_func(2)` then execute just the dask cluster block (with the mpi block commented out) all goes through to finish (slowly, but it is expected);\r\n\r\nOutline: I know we should be using the dask distributed client to operate multiprocessing jobs, but unfortunately, our actual code machinery makes use of `multiprocessing.Pool` at the moment and it will take a lot of changes to adapt it to use dask distributed; but we also want to use dask arrays, and finally, compute actual numerical values. Unfortunately, in a particular setup, we are faced with this odd combination of having to call a function twice - once explicitly on one thread and second time over in a `Pool`, and our whole machinery grinds to a halt. Any advice would be highly appreciated :beer: Very many thanks in advance! Cheers\r\n\r\nStack trace after a `KeyboardInterrupt`:\r\n```\r\n^CProcess ForkPoolWorker-2:\r\nProcess ForkPoolWorker-1:\r\nTraceback (most recent call last):\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\r\n    task = get()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/queues.py\", line 366, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\r\n    chunk = read(handle, remaining)\r\nKeyboardInterrupt\r\nTraceback (most recent call last):\r\n  File \"/home/valeriu/dask_fail.py\", line 28, in <module>\r\n    main()\r\n  File \"/home/valeriu/dask_fail.py\", line 19, in main\r\n    x = pool.map(my_func, [2])\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/pool.py\", line 364, in map\r\n    return self._map_async(func, iterable, mapstar, chunksize).get()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/pool.py\", line 765, in get\r\nTraceback (most recent call last):\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\r\n    return list(map(*args))\r\n  File \"/home/valeriu/dask_fail.py\", line 10, in my_func\r\n    s = compute(a)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/site-packages/dask/base.py\", line 570, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/site-packages/dask/threaded.py\", line 79, in get\r\n    results = get_async(\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/site-packages/dask/local.py\", line 506, in get_async\r\n    for key, res_info, failed in queue_get(queue).result():\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/site-packages/dask/local.py\", line 134, in queue_get\r\n    return q.get()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/queue.py\", line 171, in get\r\n    self.not_empty.wait()\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/threading.py\", line 312, in wait\r\n    waiter.acquire()\r\nKeyboardInterrupt\r\n    self.wait(timeout)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/multiprocessing/pool.py\", line 762, in wait\r\n    self._event.wait(timeout)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/threading.py\", line 574, in wait\r\n    signaled = self._cond.wait(timeout)\r\n  File \"/home/valeriu/miniconda3/envs/esmvaltool-standard2/lib/python3.9/threading.py\", line 312, in wait\r\n    waiter.acquire()\r\nKeyboardInterrupt\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask.array as da\r\nfrom dask import compute\r\nfrom dask.distributed import Client\r\nfrom multiprocessing import Pool\r\n\r\n\r\ndef my_func(x1):\r\n    a = da.arange(4)\r\n    print(a)\r\n    s = compute(a)\r\n\r\n\r\ndef main():\r\n    # call function like a peasant\r\n    my_func(2)\r\n\r\n    # mpi block\r\n    pool = Pool(processes=2)\r\n    x = pool.map(my_func, [2])\r\n\r\n    # dask cluster block\r\n    client = Client()\r\n    x = client.map(my_func, [2])\r\n    x = client.gather(x)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version:\r\n```\r\ndask                      2021.9.1           pyhd8ed1ab_0    conda-forge\r\ndask-core                 2021.9.1           pyhd8ed1ab_0    conda-forge\r\n```\r\n- Python version: `3.9.7`\r\n- Operating System:\r\n```\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.7 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n```\r\n- Install method (conda, pip, source): `conda-forge` (see build hash above)\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8218/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8218/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}