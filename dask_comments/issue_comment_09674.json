[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1322438518",
        "html_url": "https://github.com/dask/dask/issues/9674#issuecomment-1322438518",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9674",
        "id": 1322438518,
        "node_id": "IC_kwDOAbcwm85O0s92",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-21T17:50:36Z",
        "updated_at": "2022-11-21T17:50:36Z",
        "author_association": "MEMBER",
        "body": "Dask has historically assumed that most groupby operations can be mapped onto a tree reduction with a small-enough output to fit in a single partition. The `split_out` argument does allow you to get around this, but performance and support depend on the specific application.\r\n\r\nDo you mind sharing what version of dask you are using and what the groupby reduction commands look like? For example, are you using `ddf.groupby(\"time\", sort=True).agg(..., npartitions=n)`?\r\n\r\nI ask these questions, because `split_out>1` performance/support has recently improved for some cases (e.g. `groupby().agg`), and is likely to improve for more cases in the future.  For example, if you are only grouping on a single column, we now use `shuffle=True` in `groupby().agg` to get around the `sort=True` limitation. That said, support is still limited, and your specific application may still be a \"todo.\"\r\n\r\nWhether or not your application can use `split_out>1`/`shuffle=True`, the most performant approach to this problem is to (1) start with data that is already partitioned by the time, and (2) use `map_partitions` to perform embarrassingly parallel groupby aggregations within each partition. For example:\r\n\r\n```python\r\n\r\n# Read in data\r\nddf = dd.read_parquet(...)\r\n\r\n# If data is not already sorted/partitioned by \"time\":\r\nddf = ddf.set_index(\"time\")  # Performs (expensive) global sort\r\n\r\n# Perform distinct groupby in each partition\r\ngrouped = ddf.map_partitions(lambda x: x.groupby(\"time\").sum())\r\n```\r\n\r\nIf your data is not already sorted/partitioned on disk, the best-case scenario is likely:\r\n\r\n```python\r\nddf = dd.read_parquet(...)\r\nddf = ddf.groupby(\"time\", sort=True).agg(\"sum\", split_out=ddf.npartitions)\r\n```\r\n\r\nNote that this last case is not allowed when grouping on multiple columns (yet), or with older versions of Dask.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1322438518/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1323118349",
        "html_url": "https://github.com/dask/dask/issues/9674#issuecomment-1323118349",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9674",
        "id": 1323118349,
        "node_id": "IC_kwDOAbcwm85O3S8N",
        "user": {
            "login": "Arunes007",
            "id": 15686836,
            "node_id": "MDQ6VXNlcjE1Njg2ODM2",
            "avatar_url": "https://avatars.githubusercontent.com/u/15686836?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Arunes007",
            "html_url": "https://github.com/Arunes007",
            "followers_url": "https://api.github.com/users/Arunes007/followers",
            "following_url": "https://api.github.com/users/Arunes007/following{/other_user}",
            "gists_url": "https://api.github.com/users/Arunes007/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Arunes007/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Arunes007/subscriptions",
            "organizations_url": "https://api.github.com/users/Arunes007/orgs",
            "repos_url": "https://api.github.com/users/Arunes007/repos",
            "events_url": "https://api.github.com/users/Arunes007/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Arunes007/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-22T05:48:04Z",
        "updated_at": "2022-11-22T05:48:04Z",
        "author_association": "NONE",
        "body": "The current dask version I am using is `2022.9.2`. Well you are right previously I was using below commands for aggregation\r\n\r\n`ddf.groupby(\"time\", sort=True).sum()`\r\n\r\nWhen I saw it resulted in single partition I used split_out parameter like below\r\n\r\n`ddf = ddf.groupby(\"time\", sort=True).sum(split_out=ddf.npartitions)` \r\n\r\nwhich resulted in below error:\r\n\r\n`NotImplementedError: Cannot guarantee sorted keys for split_out>1. Try using split_out=1, or grouping with sort=False.`\r\n\r\nNext, after some exploration I settled for below steps.\r\n\r\n```\r\ntimestamp_feature_name = [\"time\"]\r\n# cast to datetime\r\ndata[timestamp_feature_name] = data[timestamp_feature_name].astype(\"datetime64[ns]\")\r\n# sort data\r\ndata = data.set_index(timestamp_feature_name)\r\n# aggregation\r\ndata = data.map_partitions(\r\n    lambda part: part.groupby(timestamp_feature_name)[\r\n        self.features_for_training\r\n    ].apply(sum)\r\n)\r\n```\r\nAlthough `set_index` is expensive I needed it for downstream operations such as filling missing timestamps:\r\n\r\n```\r\n# performed train test split\r\n# fill missing time steps\r\ndata = data.resample(self.freq).agg('asfreq').reset_index()   \r\n\r\n# mark values as nan above 99 percentile for numerical columns\r\nfor feature in numerical_features:\r\n    data[data[feature] > data[feature].quantile(0.99).compute().item()] = np.nan\r\n\r\n# fill nan values using forward fill\r\ndata[numerical_features] =  data[numerical_features].fillna(method=\"ffill\")\r\n\r\n# handle edge cases when first and last row could be nan\r\ndata = data.dropna()\r\n\r\n# scaler transformation\r\nscaler = StandardScaler()\r\nscaler.fit(data[numerical_features])\r\ndata[numerical_features] = scaler.transform(data[numerical_features])\r\n\r\n# then transformed to sequences using `sliding_window_view`\r\n\r\n```\r\n\r\nI hope I am following right steps.\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1323118349/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]