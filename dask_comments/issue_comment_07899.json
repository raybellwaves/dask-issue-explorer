[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/880164805",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-880164805",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 880164805,
        "node_id": "MDEyOklzc3VlQ29tbWVudDg4MDE2NDgwNQ==",
        "user": {
            "login": "VibhuJawa",
            "id": 4837571,
            "node_id": "MDQ6VXNlcjQ4Mzc1NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4837571?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/VibhuJawa",
            "html_url": "https://github.com/VibhuJawa",
            "followers_url": "https://api.github.com/users/VibhuJawa/followers",
            "following_url": "https://api.github.com/users/VibhuJawa/following{/other_user}",
            "gists_url": "https://api.github.com/users/VibhuJawa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/VibhuJawa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/VibhuJawa/subscriptions",
            "organizations_url": "https://api.github.com/users/VibhuJawa/orgs",
            "repos_url": "https://api.github.com/users/VibhuJawa/repos",
            "events_url": "https://api.github.com/users/VibhuJawa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/VibhuJawa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-14T19:51:30Z",
        "updated_at": "2021-07-14T19:51:46Z",
        "author_association": "MEMBER",
        "body": "Happy to take this on if the community thinks such an API will be useful broadly. ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/880164805/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/880805298",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-880805298",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 880805298,
        "node_id": "MDEyOklzc3VlQ29tbWVudDg4MDgwNTI5OA==",
        "user": {
            "login": "randerzander",
            "id": 1692914,
            "node_id": "MDQ6VXNlcjE2OTI5MTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1692914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/randerzander",
            "html_url": "https://github.com/randerzander",
            "followers_url": "https://api.github.com/users/randerzander/followers",
            "following_url": "https://api.github.com/users/randerzander/following{/other_user}",
            "gists_url": "https://api.github.com/users/randerzander/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/randerzander/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/randerzander/subscriptions",
            "organizations_url": "https://api.github.com/users/randerzander/orgs",
            "repos_url": "https://api.github.com/users/randerzander/repos",
            "events_url": "https://api.github.com/users/randerzander/events{/privacy}",
            "received_events_url": "https://api.github.com/users/randerzander/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-15T15:45:54Z",
        "updated_at": "2021-07-15T15:46:25Z",
        "author_association": "NONE",
        "body": "> While working on multi-node dask cluster users often don't have the shared storage available in the system. This problem becomes more worse for multi-node cloud systems when intra-node communication is often a bottleneck.\r\n\r\n> We need to write these frames locally to run a bunch of downstream tasks (think running multiple machine learning models on each node).\r\n\r\nTo clarify, this comes up in the context of cloud deployments, where datasets live in cloud storage. For a variety of reasons, reading directly from cloud storage is suboptimal. In several workflows, there's large performance gains by:\r\n\r\na. Copying whole datasets from cloud storage to VM instance attached SSDs.\r\nor\r\nb. Reading initially from cloud storage, doing some processing, and then writing intermediate datasets to VM instance attached SSDs.\r\n\r\nIn both scenarios, these instance SSDs are local only, and so writing to them with say, dd.to_parquet('/local_ssd/dataset') means that each worker will only write a subset of the DataFrame to its local storage. At a later stage in the workflow, if you do `dd.read_parquet('/local_ssd/dataset')`, the client process and worker processes only each have some subset of the data.\r\n\r\nIt'd be useful to support a simpler API that writes the entirety of a distributed DataFrame out to each _node's_ filesystem.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/880805298/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/880957254",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-880957254",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 880957254,
        "node_id": "MDEyOklzc3VlQ29tbWVudDg4MDk1NzI1NA==",
        "user": {
            "login": "jrbourbeau",
            "id": 11656932,
            "node_id": "MDQ6VXNlcjExNjU2OTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jrbourbeau",
            "html_url": "https://github.com/jrbourbeau",
            "followers_url": "https://api.github.com/users/jrbourbeau/followers",
            "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}",
            "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions",
            "organizations_url": "https://api.github.com/users/jrbourbeau/orgs",
            "repos_url": "https://api.github.com/users/jrbourbeau/repos",
            "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jrbourbeau/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-15T19:30:43Z",
        "updated_at": "2021-07-15T19:30:43Z",
        "author_association": "MEMBER",
        "body": "Thanks for raising an issue @VibhuJawa! Perhaps specifying an explicit `file://` protocol in your `df.to_parquet(...)` call would help. That should allow you to specify the directory you would like to write parquet files to relative to the worker's file system.\r\n\r\nFor example, the snippet below reads in a CSV dataset from S3 and writes it out to parquet to the worker's local filesystem. I'm using a Coiled cluster here, but this should work for any Dask cluster where the workers and client don't have a shared filesystem.\r\n\r\n```python\r\nIn [1]: import coiled\r\n\r\nIn [2]: cluster = coiled.Cluster(software=\"coiled/default-py39\", n_workers=10)\r\n\u2838 Creating Cluster. This might take a few minutes...Found software environment build\r\n\r\nIn [3]: from dask.distributed import Client\r\n\r\nIn [4]: client = Client(cluster)\r\n\r\nIn [5]: import dask.dataframe as dd    # Read dataset in from s3\r\n   ...:\r\n   ...: df = dd.read_csv(\r\n   ...:     \"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv\",\r\n   ...:     dtype={\r\n   ...:         \"payment_type\": \"UInt8\",\r\n   ...:         \"VendorID\": \"UInt8\",\r\n   ...:         \"passenger_count\": \"UInt8\",\r\n   ...:         \"RatecodeID\": \"UInt8\",\r\n   ...:     },\r\n   ...:     storage_options={\"anon\": True},\r\n   ...:     blocksize=\"16 MiB\",\r\n   ...: ).persist()\r\n\r\nIn [6]: df.to_parquet(\"file:///tmp\", schema=\"infer\")   # Write dataset out to \"/tmp\" on the worker's local filesystems\r\n```\r\n\r\ncc @martindurant ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/880957254/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/880963927",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-880963927",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 880963927,
        "node_id": "MDEyOklzc3VlQ29tbWVudDg4MDk2MzkyNw==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-15T19:42:14Z",
        "updated_at": "2021-07-15T19:42:14Z",
        "author_association": "MEMBER",
        "body": "Indeed, writing using the existing API and a local path will use each worker's local filesystem, as you desire. The main caveat is that some preparatory operations (e.g., creating directories) are executed by the client, and some completion operations (e.g., creating summary metadata) might be executed by the client. I believe in the case of parquet, you may well be fine on both points, but trying will tell.\r\n\r\n> At a later stage in the workflow, if you do dd.read_parquet('/local_ssd/dataset'), the client process and worker processes only each have some subset of the data.\r\n\r\nThis will definitely now work! The dask object is set up by the client, which scans the path given to find what tasks (often one per datafile) need to be in the graph of execution. It will see no data files. Even if it did, the tasks are not annotated as to which worker contains which file, and tasks will get assigned randomly.\r\n\r\nYou could probably get around this by using `client.scatter(.., broadcast=True)`, where the data is some class that finds data files for you. It's not immediately obvious to me how.\r\n\r\nIf you have all the workers seeing the *same* filesystems, which is not visible to the client, you do have an option: the \"dask://\" filesystem of fsspec. This uses `client.submit` calls to do file listing and such on the client, but uses a target filesystem, in this case local, when executed on a worker.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/880963927/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/881609451",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-881609451",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 881609451,
        "node_id": "IC_kwDOAbcwm840jErr",
        "user": {
            "login": "VibhuJawa",
            "id": 4837571,
            "node_id": "MDQ6VXNlcjQ4Mzc1NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4837571?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/VibhuJawa",
            "html_url": "https://github.com/VibhuJawa",
            "followers_url": "https://api.github.com/users/VibhuJawa/followers",
            "following_url": "https://api.github.com/users/VibhuJawa/following{/other_user}",
            "gists_url": "https://api.github.com/users/VibhuJawa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/VibhuJawa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/VibhuJawa/subscriptions",
            "organizations_url": "https://api.github.com/users/VibhuJawa/orgs",
            "repos_url": "https://api.github.com/users/VibhuJawa/repos",
            "events_url": "https://api.github.com/users/VibhuJawa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/VibhuJawa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-16T17:36:58Z",
        "updated_at": "2021-07-16T17:39:53Z",
        "author_association": "MEMBER",
        "body": "Thanks @martindurant  and @jrbourbeau for replying. \r\n> In [6]: df.to_parquet(\"file:///tmp\", schema=\"infer\")   # Write dataset out to \"/tmp\" on the worker's local filesystems\r\n\r\nThanks. Writing works fine with this and can generally work by creating the required directory.  The major problem is reading after writing. \r\n\r\n\r\n>This will definitely now work! The dask object is set up by the client, which scans the path given to find what tasks (often one per datafile) need to be in the graph of execution. It will see no data files. Even if it did, the tasks are not annotated as to which worker contains which file, and tasks will get assigned randomly.\r\n\r\nI am confused. I don't think this works.  I think you meant to say it does not work.  Tried it on a cluster to be sure too, see below. \r\n\r\n```python\r\ninput_df = dd.read_parquet('/raid/vjawa/output_dir/')\r\n```\r\n\r\n<details>\r\n\r\n```python\r\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/tmp/ipykernel_61230/3343347652.py in <module>\r\n----> 1 len(input_df)\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/core.py in __len__(self)\r\n   3878             return super().__len__()\r\n   3879         else:\r\n-> 3880             return len(s)\r\n   3881 \r\n   3882     def __contains__(self, key):\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/core.py in __len__(self)\r\n    562 \r\n    563     def __len__(self):\r\n--> 564         return self.reduction(\r\n    565             len, np.sum, token=\"len\", meta=int, split_every=False\r\n    566         ).compute()\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/base.py in compute(self, **kwargs)\r\n    284         dask.base.compute\r\n    285         \"\"\"\r\n--> 286         (result,) = compute(self, traverse=False, **kwargs)\r\n    287         return result\r\n    288 \r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/base.py in compute(*args, **kwargs)\r\n    566         postcomputes.append(x.__dask_postcompute__())\r\n    567 \r\n--> 568     results = schedule(dsk, keys, **kwargs)\r\n    569     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    570 \r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/distributed/client.py in get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\r\n   2702                     should_rejoin = False\r\n   2703             try:\r\n-> 2704                 results = self.gather(packed, asynchronous=asynchronous, direct=direct)\r\n   2705             finally:\r\n   2706                 for f in futures.values():\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/distributed/client.py in gather(self, futures, errors, direct, asynchronous)\r\n   2016             else:\r\n   2017                 local_worker = None\r\n-> 2018             return self.sync(\r\n   2019                 self._gather,\r\n   2020                 futures,\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/distributed/client.py in sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\r\n    857             return future\r\n    858         else:\r\n--> 859             return sync(\r\n    860                 self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\r\n    861             )\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    324     if error[0]:\r\n    325         typ, exc, tb = error[0]\r\n--> 326         raise exc.with_traceback(tb)\r\n    327     else:\r\n    328         return result[0]\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/distributed/utils.py in f()\r\n    307             if callback_timeout is not None:\r\n    308                 future = asyncio.wait_for(future, callback_timeout)\r\n--> 309             result[0] = yield future\r\n    310         except Exception:\r\n    311             error[0] = sys.exc_info()\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/tornado/gen.py in run(self)\r\n    760 \r\n    761                     try:\r\n--> 762                         value = future.result()\r\n    763                     except Exception:\r\n    764                         exc_info = sys.exc_info()\r\n\r\n/raid/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/distributed/client.py in _gather(self, futures, errors, direct, local_worker)\r\n   1881                             exc = CancelledError(key)\r\n   1882                         else:\r\n-> 1883                             raise exception.with_traceback(traceback)\r\n   1884                         raise exc\r\n   1885                     if errors == \"skip\":\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/optimization.py in __call__()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/core.py in get()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/core.py in _execute_task()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/core.py in <genexpr>()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/core.py in _execute_task()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/io/parquet/core.py in __call__()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/io/parquet/core.py in read_parquet_part()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/io/parquet/core.py in <listcomp>()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in read_partition()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in _read_table()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in _read_table_from_path()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/fsspec/spec.py in open()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/fsspec/implementations/local.py in _open()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/fsspec/implementations/local.py in __init__()\r\n\r\n/nvme/0/vjawa/conda/envs/dask_dist/lib/python3.8/site-packages/fsspec/implementations/local.py in _open()\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '/raid/vjawa/output_dir/part.367.parquet'\r\n```\r\n\r\n</details>\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/881609451/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/881610378",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-881610378",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 881610378,
        "node_id": "IC_kwDOAbcwm840jE6K",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-16T17:38:47Z",
        "updated_at": "2021-07-16T17:38:47Z",
        "author_association": "MEMBER",
        "body": "Er yeah, read \"will definitely NOT work\"",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/881610378/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/881610654",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-881610654",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 881610654,
        "node_id": "IC_kwDOAbcwm840jE-e",
        "user": {
            "login": "VibhuJawa",
            "id": 4837571,
            "node_id": "MDQ6VXNlcjQ4Mzc1NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4837571?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/VibhuJawa",
            "html_url": "https://github.com/VibhuJawa",
            "followers_url": "https://api.github.com/users/VibhuJawa/followers",
            "following_url": "https://api.github.com/users/VibhuJawa/following{/other_user}",
            "gists_url": "https://api.github.com/users/VibhuJawa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/VibhuJawa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/VibhuJawa/subscriptions",
            "organizations_url": "https://api.github.com/users/VibhuJawa/orgs",
            "repos_url": "https://api.github.com/users/VibhuJawa/repos",
            "events_url": "https://api.github.com/users/VibhuJawa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/VibhuJawa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-16T17:39:18Z",
        "updated_at": "2021-07-16T17:40:53Z",
        "author_association": "MEMBER",
        "body": "> The dask object is set up by the client, which scans the path given to find what tasks (often one per datafile) need to be in the graph of execution. It will see no data files. Even if it did, the tasks are not annotated as to which worker contains which file, and tasks will get assigned randomly\r\n\r\n\r\nThis is one of the major **challenges this API will solve**. If you just create local copies of data on all workers then you don't have to worry about this and your downstream tasks will happily work. \r\n\r\n\r\nA typical large-scale workflow is often broken up into stages where a user writes out the output of each stage to the disk. Something like: \r\n\r\n\r\nStage 1: \r\n1.  Do big ETL job \r\n2. Write  output of stage to disk (`df.to_parquet(xx)``)\r\n\r\nStage 2: \r\n3. Read  output of stage 1 in (`df.read_parquet(...)`)\r\n4. Run some more tasks \r\n\r\nStage x: \r\n5. Run some parallel task across the data that does not require communication across nodes. \r\n\r\n\r\nWe cant do this currently in dask if we don't have shared storage (or if writing to shared storage is prohibitive) . \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/881610654/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/881613451",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-881613451",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 881613451,
        "node_id": "IC_kwDOAbcwm840jFqL",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-16T17:44:22Z",
        "updated_at": "2021-07-16T17:44:22Z",
        "author_association": "MEMBER",
        "body": "> This is one of the major challenges this API will solve\r\n\r\nbut we don't have this API and implementation. It would be like adding a whole cluster-aware storage layer into Dask. Thats what HDFS, gluster, NFS, etc., are for. Or indeed, you may well not really need the local SSD performance and be happy with cloud object storage.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/881613451/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/881674026",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-881674026",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 881674026,
        "node_id": "IC_kwDOAbcwm840jUcq",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-16T19:36:51Z",
        "updated_at": "2021-07-16T19:36:51Z",
        "author_association": "MEMBER",
        "body": "After thinking about this a bit, it seems like we *may* want to realize this feature as a new variation on `persist`.  That is, perhaps we can introduce something like: `client.save(ddf, backend=\"parquet\")`, where we can effectively tell the client that we want to persist the collection to distributed worker-local storage using parquet files.  This API could collect and return a dictionary (or json-file path) mapping every partition to the specific IP and storage location.  A `client.load` API could then convert this mapping into a new-distributed collection.  Since this API would only be intended to save intermediate data between dask runs, we wouldn't need to worry about supporting the immense list of features expected in `to_parquet`/`read_parquet`.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/881674026/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/881687562",
        "html_url": "https://github.com/dask/dask/issues/7899#issuecomment-881687562",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/7899",
        "id": 881687562,
        "node_id": "IC_kwDOAbcwm840jXwK",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-16T20:04:04Z",
        "updated_at": "2021-07-16T20:04:04Z",
        "author_association": "MEMBER",
        "body": "@rjzamora : surely this is possible, but it would leave some edges, for example the lifecycle to clean up files. That could be tied, for instance, to futures the same way that in-memory results are; better if you can also \"publish\" them with the scheduler.\r\n\r\nI wonder, how would such a feature compare to an explicit call for workers to spill data to disk (e.g., `client.persist(.., spill=True)`? That would use a simple and fast encoding, not space-efficient.\r\n\r\nThis also touches on IPC/shared memory, since multiple workers on a node can see the same filesystem.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/881687562/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]