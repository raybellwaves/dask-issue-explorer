{
    "url": "https://api.github.com/repos/dask/dask/issues/10721",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10721/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10721/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10721/events",
    "html_url": "https://github.com/dask/dask/issues/10721",
    "id": 2048254802,
    "node_id": "I_kwDOAbcwm856FeNS",
    "number": 10721,
    "title": "`test_split_adaptive_aggregate_files` failing on main",
    "user": {
        "login": "fjetter",
        "id": 8629629,
        "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fjetter",
        "html_url": "https://github.com/fjetter",
        "followers_url": "https://api.github.com/users/fjetter/followers",
        "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
        "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
        "organizations_url": "https://api.github.com/users/fjetter/orgs",
        "repos_url": "https://api.github.com/users/fjetter/repos",
        "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
        "received_events_url": "https://api.github.com/users/fjetter/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1887344368,
            "node_id": "MDU6TGFiZWwxODg3MzQ0MzY4",
            "url": "https://api.github.com/repos/dask/dask/labels/tests",
            "name": "tests",
            "color": "a0f9b4",
            "default": false,
            "description": "Unit tests and/or continuous integration"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2023-12-19T09:24:43Z",
    "updated_at": "2023-12-20T14:36:36Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "I only saw this pop up once so far but the `test_split_adaptive_aggregate_files` was failing on main\r\n\r\n\r\nhttps://github.com/dask/dask/actions/runs/7226871563/job/19693365591\r\n\r\n\r\n```python-traceback\r\n        ddf1.to_parquet(\r\n            str(tmpdir),\r\n            engine=write_engine,\r\n            partition_on=partition_on,\r\n            write_index=False,\r\n        )\r\n        with pytest.warns(FutureWarning, match=\"Behavior may change\"):\r\n            ddf2 = dd.read_parquet(\r\n                str(tmpdir),\r\n                engine=read_engine,\r\n                blocksize=blocksize,\r\n                split_row_groups=\"adaptive\",\r\n                aggregate_files=aggregate_files,\r\n            )\r\n    \r\n        # Check that files where aggregated as expected\r\n        if aggregate_files == \"a\":\r\n            assert ddf2.npartitions == 3\r\n        elif aggregate_files == \"b\":\r\n            assert ddf2.npartitions == 6\r\n    \r\n        # Check that the final data is correct\r\n        df2 = ddf2.compute().sort_values([\"c\", \"d\"])\r\n        df1 = df1.sort_values([\"c\", \"d\"])\r\n>       assert_eq(df1[[\"c\", \"d\"]], df2[[\"c\", \"d\"]], check_index=False)\r\n\r\ndask\\dataframe\\io\\tests\\test_parquet.py:3134: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\na =            c   d\r\n0   0.001898  15\r\n1   0.007386  30\r\n2   0.028712  14\r\n3   0.038563  44\r\n4   0.044161   6\r\n..       ...  ..\r\n95  0.948483  83\r\n96  0.959911  49\r\n97  0.970251  32\r\n98  0.995297  82\r\n99  0.999792  65\r\n\r\n[100 rows x 2 columns]\r\nb =            c   d\r\n0   0.007386  30\r\n1   0.044161   6\r\n2   0.049735  76\r\n3   0.083297  57\r\n4   0.095994  47\r\n..       ...  ..\r\n92  0.940960  97\r\n93  0.948483  83\r\n94  0.959911  49\r\n95  0.995297  82\r\n96  0.999792  65\r\n\r\n[97 rows x 2 columns]\r\ncheck_names = True, check_dtype = True, check_divisions = True\r\ncheck_index = False, sort_results = True, scheduler = 'sync', kwargs = {}\r\n\r\n    def assert_eq(\r\n        a,\r\n        b,\r\n        check_names=True,\r\n        check_dtype=True,\r\n        check_divisions=True,\r\n        check_index=True,\r\n        sort_results=True,\r\n        scheduler=\"sync\",\r\n        **kwargs,\r\n    ):\r\n        if check_divisions:\r\n            assert_divisions(a, scheduler=scheduler)\r\n            assert_divisions(b, scheduler=scheduler)\r\n            if hasattr(a, \"divisions\") and hasattr(b, \"divisions\"):\r\n                at = type(np.asarray(a.divisions).tolist()[0])  # numpy to python\r\n                bt = type(np.asarray(b.divisions).tolist()[0])  # scalar conversion\r\n                assert at == bt, (at, bt)\r\n        assert_sane_keynames(a)\r\n        assert_sane_keynames(b)\r\n        a = _check_dask(\r\n            a, check_names=check_names, check_dtypes=check_dtype, scheduler=scheduler\r\n        )\r\n        b = _check_dask(\r\n            b, check_names=check_names, check_dtypes=check_dtype, scheduler=scheduler\r\n        )\r\n        if hasattr(a, \"to_pandas\"):\r\n            a = a.to_pandas()\r\n        if hasattr(b, \"to_pandas\"):\r\n            b = b.to_pandas()\r\n    \r\n        a, b = _maybe_convert_string(a, b)\r\n    \r\n        if isinstance(a, (pd.DataFrame, pd.Series)) and sort_results:\r\n            a = _maybe_sort(a, check_index)\r\n            b = _maybe_sort(b, check_index)\r\n        if not check_index:\r\n            a = a.reset_index(drop=True)\r\n            b = b.reset_index(drop=True)\r\n        if isinstance(a, pd.DataFrame):\r\n>           tm.assert_frame_equal(\r\n                a, b, check_names=check_names, check_dtype=check_dtype, **kwargs\r\nE               AssertionError: DataFrame are different\r\nE               \r\nE               DataFrame shape mismatch\r\nE               [left]:  (100, 2)\r\nE               [right]: (97, 2)\r\n```\r\n\r\nI find this failure particularly concerning since this looks like data loss is possible even if it doesn't happen reliably. @rjzamora do you have time to poke at this?",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10721/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10721/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}