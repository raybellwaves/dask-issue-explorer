{
    "url": "https://api.github.com/repos/dask/dask/issues/8800",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8800/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8800/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8800/events",
    "html_url": "https://github.com/dask/dask/issues/8800",
    "id": 1166448525,
    "node_id": "I_kwDOAbcwm85FhpeN",
    "number": 8800,
    "title": "`groupby.transform` failing on dataframe with duplicates in column",
    "user": {
        "login": "guidocioni",
        "id": 12760310,
        "node_id": "MDQ6VXNlcjEyNzYwMzEw",
        "avatar_url": "https://avatars.githubusercontent.com/u/12760310?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/guidocioni",
        "html_url": "https://github.com/guidocioni",
        "followers_url": "https://api.github.com/users/guidocioni/followers",
        "following_url": "https://api.github.com/users/guidocioni/following{/other_user}",
        "gists_url": "https://api.github.com/users/guidocioni/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/guidocioni/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/guidocioni/subscriptions",
        "organizations_url": "https://api.github.com/users/guidocioni/orgs",
        "repos_url": "https://api.github.com/users/guidocioni/repos",
        "events_url": "https://api.github.com/users/guidocioni/events{/privacy}",
        "received_events_url": "https://api.github.com/users/guidocioni/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 3798450413,
            "node_id": "LA_kwDOAbcwm87iZ8Dt",
            "url": "https://api.github.com/repos/dask/dask/labels/bug",
            "name": "bug",
            "color": "faadaf",
            "default": true,
            "description": "Something is broken"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2022-03-11T13:59:56Z",
    "updated_at": "2022-03-24T22:43:31Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I'm trying to process parquet files stored in AWS S3.\r\n\r\nThe files are read simply with\r\n```python\r\nwith Client(n_workers=6) as client:\r\n    df = dd.read_parquet('s3://lightnings_*.gzip.parquet')\r\n    df['time_plot'] = df.time.dt.round(freq='15 min')\r\n```\r\nIt looks like this \r\n\r\n| time                             |   latitude |   longitude | time_plot                 |\r\n|:---------------------------------|-----------:|------------:|:--------------------------|\r\n| 2021-04-01 00:00:30.688104+00:00 |    36.4179 |     34.1496 | 2021-04-01 00:00:00+00:00 |\r\n| 2021-04-01 00:25:24.591956+00:00 |    37.7171 |    -10.604  | 2021-04-01 00:30:00+00:00 |\r\n\r\nThere are some duplicated rows (not shown here)!\r\n\r\nWhen trying to apply a `groupby` operation\r\n\r\n```python\r\n    df['seconds_elapsed'] = df.groupby(\"time_plot\").time.transform(\r\n            lambda x: (x - x.min()).dt.seconds)\r\n```\r\n\r\nI get an error: `ValueError: cannot reindex on an axis with duplicate labels`.\r\nThe same code runs fine on a normal `pandas.DataFrame` by calling `.compute()` before. \r\n\r\nAs I thought the problem was related to the duplicates I tried to remove them before calling the `groupby.transform`, i.e.\r\n\r\n```python\r\n    df = df.drop_duplicates(subset=['time'], split_out=6)\r\n```\r\n\r\nThis takes a long time, uses a lot of memory and still does not fix the problem as I still get the error reported before. Note that I had to use `split_out` otherwise the workers were running out of memory. \r\n\r\nAs workaround I'm currently loading the dataset into memory (`df.compute()`) before passing it to the `groupby` operations, but this is quite slow as this is applied serially so it would be good to adapt the workflow with Dask. \r\n\r\nAnyone has suggestions on what I'm doing wrong? ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8800/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8800/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}