[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/423507208",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-423507208",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 423507208,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzUwNzIwOA==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-09-21T11:58:18Z",
        "updated_at": "2018-09-21T11:58:18Z",
        "author_association": "MEMBER",
        "body": "My guess is that this isn't a bug with grouping, but rather an issue where you're running out of RAM and so getting heavily degraded performance.  Strings are unforunately expensive to store in memory in Python and Pandas' lack of a text type kills us.  \r\n\r\nSome things you could try to test this theory:\r\n\r\n1.  Have lots of groups but use fewer rows and see how things behave\r\n2.  Store your data in an efficient on-disk format like parquet, and then do the groupby-aggregation from parquet, rather than having everything sitting in memory\r\n3.  Watch the diagnostic dashboard to see if its writing/reading from disk (look for orange rectangles in the task stream plot) and also check out the profile page after execution to see what is taking up time.  See http://dask.pydata.org/en/latest/diagnostics-distributed.html\r\n\r\nAlso, just to clarify things, persist and compute are orthogonal to the local and distributed schedulers.  There is no reason to change to use `persist` and `print(len(.))` when using the distributed scheduler.  Your code shouldn't have to change.  Compute will work exactly as it did before.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/423507208/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/424719145",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-424719145",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 424719145,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDcxOTE0NQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-09-26T13:40:50Z",
        "updated_at": "2018-09-26T13:40:50Z",
        "author_association": "MEMBER",
        "body": "Any update on this @jangorecki ?  Were any of those suggestions helpful?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/424719145/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/424762755",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-424762755",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 424762755,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDc2Mjc1NQ==",
        "user": {
            "login": "jangorecki",
            "id": 3627377,
            "node_id": "MDQ6VXNlcjM2MjczNzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3627377?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jangorecki",
            "html_url": "https://github.com/jangorecki",
            "followers_url": "https://api.github.com/users/jangorecki/followers",
            "following_url": "https://api.github.com/users/jangorecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/jangorecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jangorecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jangorecki/subscriptions",
            "organizations_url": "https://api.github.com/users/jangorecki/orgs",
            "repos_url": "https://api.github.com/users/jangorecki/repos",
            "events_url": "https://api.github.com/users/jangorecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jangorecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-09-26T15:37:44Z",
        "updated_at": "2018-09-26T15:37:44Z",
        "author_association": "NONE",
        "body": "I run those queries with parquet on 1e7 and 1e8 only.\r\nI could not read 1e9 parquet in dask saved by spark due to\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/user/git/db-benchm\r\nark/G1_1e9_1e2.parq/_metadata/_metadata'                                \r\n```\r\nand I couldn't create it from dask as reading csv was running out of memory.\r\nTimings are as follows:\r\n1e7\r\nfewer 6.6s\r\nmore 8.1s\r\n1e8\r\nfewer 61.5s\r\nmore 216.4s\r\nSo there is slowdown, but on those datasizes we surely haven't run out of memory (datasize is 5GB, machine 125GB). I am not surprised there is a slowdown instead of speed up as we moved from in-memory processing to on-disk processing.\r\nGoing back to main problem... it already manifests on 1e8 with more groups and in-memory processing, where lack of memory surely is not a problem. Current dask in-memory timing 148s is sub-optimal. AFAIU tuning data storage, etc. will not help much, thus we shouldn't focus on this but rather improve in-memory 1e8 timing.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/424762755/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/429547829",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-429547829",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 429547829,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU0NzgyOQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-10-13T14:46:52Z",
        "updated_at": "2018-10-13T14:46:52Z",
        "author_association": "MEMBER",
        "body": "@martindurant please see the parquet error above.  It looks like the current experience of writing from spark and reading from dask still isn't smooth for novice users.\r\n\r\n@jangorecki you might find this notebook helpful for your benchmarks: https://gist.github.com/c0b84b689238ea46cf9aa1c79155fe34\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/429547829/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/429551111",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-429551111",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 429551111,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU1MTExMQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-10-13T15:33:15Z",
        "updated_at": "2018-10-13T15:33:15Z",
        "author_association": "MEMBER",
        "body": "My timings for 1e7 are as follows:\r\n\r\n1e7\r\nfewer 3s (with the distributed scheduler) \r\nfewer 0.4s (with the threaded scheduler (which is default, just never create a client))\r\nmore 5s\r\n\r\n> but on those datasizes we surely haven't run out of memory (datasize is 5GB, machine 125GB)\r\n\r\nCreating all of the columns and storing them in memory is something like 200MB on disk as parquet or 5GB in RAM using Python object dtype.  You really shouldn't underestimate the performance cost of using Python object dtypes in Pandas.  They operate at Python speeds (rather than C like the rest of Pandas) and take up a ton of memory.  In this situation I would strongly recommend \r\n\r\n1.  Not persisting all of the unnecessary columns in RAM \r\n2.  Using categorical dtypes\r\n\r\nIf we do this then I can easily run the 1e8 system on my laptop (16GB of RAM).  Here is a run through with the threaded scheduler:\r\n\r\nhttps://gist.github.com/34e987777b19b6f6039e8e6c44dd7fcc\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/429551111/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/429551178",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-429551178",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 429551178,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU1MTE3OA==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-10-13T15:34:13Z",
        "updated_at": "2018-10-13T15:34:13Z",
        "author_association": "MEMBER",
        "body": "1e8\r\n\r\nfewer: 0.57s\r\nmore: 220s\r\n\r\nStill not great for many groups, as you said above",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/429551178/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/429564625",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-429564625",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 429564625,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU2NDYyNQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-10-13T18:29:03Z",
        "updated_at": "2018-10-13T18:29:03Z",
        "author_association": "MEMBER",
        "body": "@TomAugspurger you might want to run the notebook above and look at the `/profile` page output (also happy to help with this if you're unfamiliar with it).  There are some surprisingly long durations coming up in Pandas that I'm not able to easily explain.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/429564625/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/429567882",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-429567882",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 429567882,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU2Nzg4Mg==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-10-13T19:14:51Z",
        "updated_at": "2018-10-13T19:14:51Z",
        "author_association": "MEMBER",
        "body": "Anything in particular that stands out duration-wise? (I only tried the default values in the notebook. Haven't played around with others).\r\n\r\nOne thing that looked somewhat odd was the `.agg({'v1':'sum', 'v3':'mean'})` is that pandas is spending time in `_factorize_array` on both the `_get_compress_labels` side (the actual determination of groups, which isn't surprising) and the in the actual `agg` side. I haven't looked at whether there's actually duplicative computation yet though.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/429567882/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/429569220",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-429569220",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 429569220,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU2OTIyMA==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-10-13T19:34:10Z",
        "updated_at": "2018-10-13T19:34:10Z",
        "author_association": "MEMBER",
        "body": "I'm seeing us spend 35% of our time in\npandas/core/arrays/categorical.py::is_dtype_equal, in particular calling\nreturn hash(self.dtype) == hash(other.dtype)\n\nGenerally though, things do seem surprisingly slow relative to lower\ncardinality groupbys\n\nOn Sat, Oct 13, 2018 at 3:14 PM Tom Augspurger <notifications@github.com>\nwrote:\n\n> Anything in particular that stands out duration-wise? (I only tried the\n> default values in the notebook. Haven't played around with others).\n>\n> One thing that looked somewhat odd was the .agg({'v1':'sum', 'v3':'mean'})\n> is that pandas is spending time in _factorize_array on both the\n> _get_compress_labels side (the actual determination of groups, which\n> isn't surprising) and the in the actual agg side. I haven't looked at\n> whether there's actually duplicative computation yet though.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/4001#issuecomment-429567882>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AASszOGSFxmTJsn0sL9sJVOJuwdixpPoks5ukjusgaJpZM4WzgYh>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/429569220/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/457832759",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-457832759",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 457832759,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ1NzgzMjc1OQ==",
        "user": {
            "login": "jangorecki",
            "id": 3627377,
            "node_id": "MDQ6VXNlcjM2MjczNzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3627377?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jangorecki",
            "html_url": "https://github.com/jangorecki",
            "followers_url": "https://api.github.com/users/jangorecki/followers",
            "following_url": "https://api.github.com/users/jangorecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/jangorecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jangorecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jangorecki/subscriptions",
            "organizations_url": "https://api.github.com/users/jangorecki/orgs",
            "repos_url": "https://api.github.com/users/jangorecki/repos",
            "events_url": "https://api.github.com/users/jangorecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jangorecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-01-26T13:51:21Z",
        "updated_at": "2019-01-27T07:17:31Z",
        "author_association": "NONE",
        "body": "Just to highlight this issue I can point to this report https://h2oai.github.io/db-benchmark/ (click **\"5 GB\"** tab)\r\nwhere dask is fastest solution when there are few groups (question 1) but is terribly slow for many groups (question 3). ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/457832759/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/467054431",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-467054431",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 467054431,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2NzA1NDQzMQ==",
        "user": {
            "login": "alex959595",
            "id": 3375124,
            "node_id": "MDQ6VXNlcjMzNzUxMjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3375124?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alex959595",
            "html_url": "https://github.com/alex959595",
            "followers_url": "https://api.github.com/users/alex959595/followers",
            "following_url": "https://api.github.com/users/alex959595/following{/other_user}",
            "gists_url": "https://api.github.com/users/alex959595/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alex959595/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alex959595/subscriptions",
            "organizations_url": "https://api.github.com/users/alex959595/orgs",
            "repos_url": "https://api.github.com/users/alex959595/repos",
            "events_url": "https://api.github.com/users/alex959595/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alex959595/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-25T15:28:40Z",
        "updated_at": "2019-02-26T13:13:19Z",
        "author_association": "NONE",
        "body": "By default Dask combines all group by apply chunks a single Pandas dataframe output no matter the size resulting in.  As shown in the Many groups example in the link. https://examples.dask.org/dataframes/02-groupby.html.\r\n\r\nBy using the parameter _split_out_ you should be able to control this size. \r\n\r\nIt also automatically combines the resulting aggregation chucks (every 8 chunks by default).  FYI: This is an arbitrary value set in dask/dataframe/core.py line 3595. This value is fine for most cases but if the aggregation chunks are large, it could take a substantial amount of time to combine them, (especially if you need to transfer chunk from another worker. \r\n\r\nI think you can ensure output of the groupby chunks don\u2019t get to big by setting _split_out=4_  and __split_every_ _=False__.\r\n\r\n`df.groupby('id').x.mean(split_out=4, split_every=False)`",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/467054431/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/467446577",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-467446577",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 467446577,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2NzQ0NjU3Nw==",
        "user": {
            "login": "jangorecki",
            "id": 3627377,
            "node_id": "MDQ6VXNlcjM2MjczNzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3627377?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jangorecki",
            "html_url": "https://github.com/jangorecki",
            "followers_url": "https://api.github.com/users/jangorecki/followers",
            "following_url": "https://api.github.com/users/jangorecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/jangorecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jangorecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jangorecki/subscriptions",
            "organizations_url": "https://api.github.com/users/jangorecki/orgs",
            "repos_url": "https://api.github.com/users/jangorecki/repos",
            "events_url": "https://api.github.com/users/jangorecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jangorecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-26T13:54:32Z",
        "updated_at": "2019-02-26T13:54:52Z",
        "author_association": "NONE",
        "body": "@alex959595 thanks for suggestion. Any idea why this is not optimised internally? So the aggregation API could be data agnostic, relying only on metadata (schema).",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/467446577/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/467491773",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-467491773",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 467491773,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2NzQ5MTc3Mw==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-26T15:48:25Z",
        "updated_at": "2019-02-26T15:48:25Z",
        "author_association": "MEMBER",
        "body": "So that Dask doesn't have to look at all of your data before coming up with\na plan.  One of the costs of laziness is poor planning.\n\nOn Tue, Feb 26, 2019 at 5:54 AM Jan Gorecki <notifications@github.com>\nwrote:\n\n> @alex959595 <https://github.com/alex959595> thanks for suggestion. Any\n> idea why this is not optimised internally? So the aggregation API can be\n> data agnostic, relying only on metadata (schema).\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/4001#issuecomment-467446577>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AASszLFj5NE4bxfM4CtuaOpD3jo1thRYks5vRTyZgaJpZM4WzgYh>\n> .\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/467491773/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/647441452",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-647441452",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 647441452,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY0NzQ0MTQ1Mg==",
        "user": {
            "login": "PalakHarwani",
            "id": 26281530,
            "node_id": "MDQ6VXNlcjI2MjgxNTMw",
            "avatar_url": "https://avatars.githubusercontent.com/u/26281530?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PalakHarwani",
            "html_url": "https://github.com/PalakHarwani",
            "followers_url": "https://api.github.com/users/PalakHarwani/followers",
            "following_url": "https://api.github.com/users/PalakHarwani/following{/other_user}",
            "gists_url": "https://api.github.com/users/PalakHarwani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PalakHarwani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PalakHarwani/subscriptions",
            "organizations_url": "https://api.github.com/users/PalakHarwani/orgs",
            "repos_url": "https://api.github.com/users/PalakHarwani/repos",
            "events_url": "https://api.github.com/users/PalakHarwani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PalakHarwani/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-06-22T10:52:22Z",
        "updated_at": "2020-06-22T10:52:22Z",
        "author_association": "NONE",
        "body": "I have a huge csv file(~400 GB) and I'm reading it through dask. I want to group the data by a column and apply a function to the grouped dataframes. It works perfectly fine with smaller csv files. Is there a way to use split_out with custom functions or any other workaround for this? ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/647441452/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/647463811",
        "html_url": "https://github.com/dask/dask/issues/4001#issuecomment-647463811",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/4001",
        "id": 647463811,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY0NzQ2MzgxMQ==",
        "user": {
            "login": "jangorecki",
            "id": 3627377,
            "node_id": "MDQ6VXNlcjM2MjczNzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3627377?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jangorecki",
            "html_url": "https://github.com/jangorecki",
            "followers_url": "https://api.github.com/users/jangorecki/followers",
            "following_url": "https://api.github.com/users/jangorecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/jangorecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jangorecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jangorecki/subscriptions",
            "organizations_url": "https://api.github.com/users/jangorecki/orgs",
            "repos_url": "https://api.github.com/users/jangorecki/repos",
            "events_url": "https://api.github.com/users/jangorecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jangorecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-06-22T11:42:00Z",
        "updated_at": "2020-06-22T12:29:46Z",
        "author_association": "NONE",
        "body": "@PalakHarwani I haven't used `split_out` function, but there are tricks you can use, you may know them already but just in case. Thanks to @ravwojdyla who [contributed](https://github.com/h2oai/db-benchmark/pull/144/files) them recently to my project.\r\n- use local distributed process-pool scheduler\r\n- use less parallel tasks and leverage local processing\r\n\r\nThat translates to\r\n```py\r\nfrom dask import distributed\r\nclient = distributed.Client(processes=True, silence_logs=logging.ERROR)\r\ndk.config.set({\"optimization.fuse.ave-width\": 20})\r\n```\r\nSo just by using \"distributed` on a single machine you can deal with high cardinality queries much faster.\r\nEven on a small 0.5GB data some queries got more than 10x speed up. There are queries that got slower now (i.e. low cardinality groupby), but the gains are much bigger.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/647463811/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]