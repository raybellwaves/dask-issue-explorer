{
    "url": "https://api.github.com/repos/dask/dask/issues/4449",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/4449/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/4449/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/4449/events",
    "html_url": "https://github.com/dask/dask/issues/4449",
    "id": 405750722,
    "node_id": "MDU6SXNzdWU0MDU3NTA3MjI=",
    "number": 4449,
    "title": "Sizeof fails on GeoDataFrame when passing kwarg to map_partitions",
    "user": {
        "login": "arredond",
        "id": 19406854,
        "node_id": "MDQ6VXNlcjE5NDA2ODU0",
        "avatar_url": "https://avatars.githubusercontent.com/u/19406854?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/arredond",
        "html_url": "https://github.com/arredond",
        "followers_url": "https://api.github.com/users/arredond/followers",
        "following_url": "https://api.github.com/users/arredond/following{/other_user}",
        "gists_url": "https://api.github.com/users/arredond/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/arredond/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/arredond/subscriptions",
        "organizations_url": "https://api.github.com/users/arredond/orgs",
        "repos_url": "https://api.github.com/users/arredond/repos",
        "events_url": "https://api.github.com/users/arredond/events{/privacy}",
        "received_events_url": "https://api.github.com/users/arredond/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 9,
    "created_at": "2019-02-01T15:32:02Z",
    "updated_at": "2019-04-30T15:11:09Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "Almost definitely related to #3972 and #3976, but I'm not too sure since the technical details there are above me.\r\n\r\nI found this bug when passing a GeoDataFrame as a keyword argument to `dataframe.map_partitions`. Interestingly enough, this only happened **after I updated to Dask 1.1 from Dask 0.17**. I suspect this is related to the changes in https://github.com/dask/dask/pull/3998\r\n\r\nWeirdly enough, Dask seems to manage Dataframes containing shapely objects just fine, but once a GeoDataFrame is passed, Dask is unable to determine its `sizeof` and fails.\r\n\r\nHere's a reproducible example with two cases:\r\n- Case 1: Make a Pandas DF with a Shapely geometry column, convert to Dask DF and pass to **map_partitions** to buffer geometries. Specify `geometry` as `str` in the meta.\r\n- Case 2: Same, but use **map_partitions** with a different function that takes a polygon GeoDataFrame and keeps only the point inside the polygon.\r\n\r\n```python\r\ndf = pd.DataFrame({\r\n    'id': [0, 1],\r\n    'geometry': [Point(0,0), Point(2,2)]\r\n})\r\nddf = dd.from_pandas(df, npartitions=2).copy()\r\n\r\n# CASE 1\r\n## This works fine\r\ndef buffer_points(d):\r\n    gdf = gpd.GeoDataFrame(d)\r\n    gdf['geometry'] = gdf.buffer(0.1)\r\n    return gdf\r\n\r\nddf.map_partitions(\r\n    buffer_points,\r\n    meta=[('id', str), ('geometry', str)]\r\n).compute()\r\n\r\n# CASE 2\r\ndef intersect_points(d, other_gdf):\r\n    gdf = gpd.GeoDataFrame(d)\r\n    sjoin = gpd.sjoin(gdf, other_gdf, op='within')\r\n    sjoin.rename(index=str, columns={'id_left': 'id'}, inplace=True)\r\n    \r\n    return sjoin[['id', 'geometry']]\r\n\r\np1_buffered = gpd.GeoDataFrame({\r\n    'id': [0],\r\n    'geometry': [Point(0,0).buffer(0.1)]\r\n})\r\n\r\n# Only Pandas works fine\r\nprint(intersect_points(df, p1_buffered))\r\n\r\n# Using Dask fails\r\nddf.map_partitions(\r\n    intersect_points,\r\n    other_gdf=p1_buffered,\r\n    meta=[('id', str), ('geometry', str)]\r\n)\r\n\r\n# Error is `ValueError: 'values' must be a NumPy array.`\r\n```\r\n\r\nThis issue is easy to work around (just pass a Pandas' (not GeoPandas') DF as kwarg and convert to GeoPandas inside the **map_partitions** function), but it may help to solve #3972 and #3976 or other users.\r\n\r\nThe full stack trace is:\r\n\r\n<details>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-79-a27b715cad40> in <module>\r\n      2     intersect_points,\r\n      3     other_gdf=p1_buffered,\r\n----> 4     meta=[('id', str), ('geometry', str)]\r\n      5 )\r\n\r\n/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py in map_partitions(self, func, *args, **kwargs)\r\n    558         >>> ddf.map_partitions(func).clear_divisions()  # doctest: +SKIP\r\n    559         \"\"\"\r\n--> 560         return map_partitions(func, self, *args, **kwargs)\r\n    561 \r\n    562     @insert_meta_param_description(pad=12)\r\n\r\n/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py in map_partitions(func, *args, **kwargs)\r\n   3705 \r\n   3706     # Normalize keyword arguments\r\n-> 3707     kwargs2 = {k: normalize_arg(v) for k, v in kwargs.items()}\r\n   3708 \r\n   3709     assert callable(func)\r\n\r\n/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py in <dictcomp>(.0)\r\n   3705 \r\n   3706     # Normalize keyword arguments\r\n-> 3707     kwargs2 = {k: normalize_arg(v) for k, v in kwargs.items()}\r\n   3708 \r\n   3709     assert callable(func)\r\n\r\n/usr/local/lib/python3.6/site-packages/dask/array/core.py in normalize_arg(x)\r\n    301     elif isinstance(x, str) and re.match(r'_\\d+', x):\r\n    302         return delayed(x)\r\n--> 303     elif sizeof(x) > 1e6:\r\n    304         return delayed(x)\r\n    305     else:\r\n\r\n/usr/local/lib/python3.6/site-packages/dask/utils.py in __call__(self, arg, *args, **kwargs)\r\n    411         \"\"\"\r\n    412         meth = self.dispatch(type(arg))\r\n--> 413         return meth(arg, *args, **kwargs)\r\n    414 \r\n    415     @property\r\n\r\n/usr/local/lib/python3.6/site-packages/dask/sizeof.py in sizeof_default(o)\r\n     18 @sizeof.register(object)\r\n     19 def sizeof_default(o):\r\n---> 20     return getsizeof(o)\r\n     21 \r\n     22 \r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/base.py in __sizeof__(self)\r\n    115         \"\"\"\r\n    116         if hasattr(self, 'memory_usage'):\r\n--> 117             mem = self.memory_usage(deep=True)\r\n    118             if not is_scalar(mem):\r\n    119                 mem = mem.sum()\r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/frame.py in memory_usage(self, index, deep)\r\n   2584         \"\"\"\r\n   2585         result = Series([c.memory_usage(index=False, deep=deep)\r\n-> 2586                          for col, c in self.iteritems()], index=self.columns)\r\n   2587         if index:\r\n   2588             result = Series(self.index.memory_usage(deep=deep),\r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/frame.py in <listcomp>(.0)\r\n   2584         \"\"\"\r\n   2585         result = Series([c.memory_usage(index=False, deep=deep)\r\n-> 2586                          for col, c in self.iteritems()], index=self.columns)\r\n   2587         if index:\r\n   2588             result = Series(self.index.memory_usage(deep=deep),\r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/series.py in memory_usage(self, index, deep)\r\n   3911         212\r\n   3912         \"\"\"\r\n-> 3913         v = super(Series, self).memory_usage(deep=deep)\r\n   3914         if index:\r\n   3915             v += self.index.memory_usage(deep=deep)\r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/base.py in memory_usage(self, deep)\r\n   1408         v = self.array.nbytes\r\n   1409         if deep and is_object_dtype(self) and not PYPY:\r\n-> 1410             v += lib.memory_usage_of_objects(self.array)\r\n   1411         return v\r\n   1412 \r\n\r\npandas/_libs/lib.pyx in pandas._libs.lib.memory_usage_of_objects()\r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/arrays/numpy_.py in __getitem__(self, item)\r\n    224         result = self._ndarray[item]\r\n    225         if not lib.is_scalar(result):\r\n--> 226             result = type(self)(result)\r\n    227         return result\r\n    228 \r\n\r\n/usr/local/lib/python3.6/site-packages/pandas/core/arrays/numpy_.py in __init__(self, values, copy)\r\n    129             values = values._ndarray\r\n    130         if not isinstance(values, np.ndarray):\r\n--> 131             raise ValueError(\"'values' must be a NumPy array.\")\r\n    132 \r\n    133         if values.ndim != 1:\r\n\r\nValueError: 'values' must be a NumPy array.\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/4449/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/4449/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}