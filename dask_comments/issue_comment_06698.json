[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/702941205",
        "html_url": "https://github.com/dask/dask/issues/6698#issuecomment-702941205",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6698",
        "id": 702941205,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcwMjk0MTIwNQ==",
        "user": {
            "login": "jsignell",
            "id": 4806877,
            "node_id": "MDQ6VXNlcjQ4MDY4Nzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4806877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jsignell",
            "html_url": "https://github.com/jsignell",
            "followers_url": "https://api.github.com/users/jsignell/followers",
            "following_url": "https://api.github.com/users/jsignell/following{/other_user}",
            "gists_url": "https://api.github.com/users/jsignell/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jsignell/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jsignell/subscriptions",
            "organizations_url": "https://api.github.com/users/jsignell/orgs",
            "repos_url": "https://api.github.com/users/jsignell/repos",
            "events_url": "https://api.github.com/users/jsignell/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jsignell/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-02T20:24:04Z",
        "updated_at": "2020-10-02T20:24:04Z",
        "author_association": "MEMBER",
        "body": "Thanks for opening this issue and offering to do the work! This is a perfect place to discuss how you'd go about adding `duplicated` to `dask.dataframe`. My first thought is that it might not be super straightforward since you can't do the operation on each chunk and then bring those results together at the end with some aggregation (like you could for `sum` for example). \r\n\r\nHowever I suspect this type of operation is most common in sorted dataframes, so you might start by only supporting those and use a `map_overlap` to make sure that you are not neglecting possible duplicates that exist on the edges of chunks. \r\n\r\nIf this still sounds fun/worth it, you can use `fillna` as inspiration: https://github.com/dask/dask/blob/a17ef6ebcfa434f3b6364ec11ca1b940c91a6f57/dask/dataframe/core.py#L1246",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/702941205/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]