{
    "url": "https://api.github.com/repos/dask/dask/issues/9861",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9861/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9861/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9861/events",
    "html_url": "https://github.com/dask/dask/issues/9861",
    "id": 1552207204,
    "node_id": "I_kwDOAbcwm85chM1k",
    "number": 9861,
    "title": "Issue writing parquet files on s3 in append mode with pyarrow",
    "user": {
        "login": "antoinebon",
        "id": 3825783,
        "node_id": "MDQ6VXNlcjM4MjU3ODM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3825783?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/antoinebon",
        "html_url": "https://github.com/antoinebon",
        "followers_url": "https://api.github.com/users/antoinebon/followers",
        "following_url": "https://api.github.com/users/antoinebon/following{/other_user}",
        "gists_url": "https://api.github.com/users/antoinebon/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/antoinebon/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/antoinebon/subscriptions",
        "organizations_url": "https://api.github.com/users/antoinebon/orgs",
        "repos_url": "https://api.github.com/users/antoinebon/repos",
        "events_url": "https://api.github.com/users/antoinebon/events{/privacy}",
        "received_events_url": "https://api.github.com/users/antoinebon/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2023-01-22T17:19:18Z",
    "updated_at": "2024-03-25T01:45:35Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n`dask.DataFrame.to_parquet` fails under the following conditions:\r\n1. Target filesystem is `s3fs.S3FileSystem`\r\n2. the target `path` doesn't exists on S3\r\n3. `to_parquet` parameter `append` is `True`\r\n4. the parquet engine is `pyarrow`\r\n\r\nNote: if the target file system is a `LocalFileSytem` then the write succeeds, therefore I would expect the same for a `S3FileSystem`\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\ndf = dd.from_pandas(pd.DataFrame({\"a\": [1,2,3]}), npartitions=1)\r\ndf.to_parquet(\"/tmp/test.parquet\", append=True, engine=\"pyarrow\")  # -> works\r\ndf.to_parquet(\"s3://my-test-bucket/test.parquet\", append=False, engine=\"pyarrow\")  #-> works even if \"s3://my-test-bucket/test.parquet\" doesn't exist\r\ndf.to_parquet(\"s3://my-test-bucket/test.parquet\", append=True, engine=\"pyarrow\")  #-> works if \"s3://my-test-bucket/test.parquet\" exists\r\ndf.to_parquet(\"s3://my-test-bucket/test.parquet\", append=True, engine=\"pyarrow\")  #-> fails if \"s3://my-test-bucket/test.parquet\" doesn't exist, see traceback below\r\n\r\n```\r\n\r\nTraceback:\r\n```\r\n~/.venv/lib/python3.10/site-packages/dask/dataframe/core.py in to_parquet(self, path, *args, **kwargs)\r\n   5279         from dask.dataframe.io import to_parquet\r\n   5280 \r\n-> 5281         return to_parquet(self, path, *args, **kwargs)\r\n   5282 \r\n   5283     def to_orc(self, path, *args, **kwargs):\r\n\r\n~/.venv/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py in to_parquet(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, **kwargs)\r\n    938     # Engine-specific initialization steps to write the dataset.\r\n    939     # Possibly create parquet metadata, and load existing stuff if appending\r\n--> 940     i_offset, fmd, metadata_file_exists, extra_write_kwargs = engine.initialize_write(\r\n    941         df,\r\n    942         fs,\r\n\r\n~/.venv/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py in initialize_write(cls, df, fs, path, append, partition_on, ignore_divisions, division_info, schema, index_cols, **kwargs)\r\n    625         if append:\r\n    626             # Extract metadata and get file offset if appending\r\n--> 627             ds = pa_ds.dataset(path, filesystem=_wrapped_fs(fs), format=\"parquet\")\r\n    628             i_offset = len(ds.files)\r\n    629             if i_offset > 0:\r\n\r\n~/.venv/lib/python3.10/site-packages/pyarrow/dataset.py in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n    750 \r\n    751     if _is_path_like(source):\r\n--> 752         return _filesystem_dataset(source, **kwargs)\r\n    753     elif isinstance(source, (tuple, list)):\r\n    754         if all(_is_path_like(elem) for elem in source):\r\n\r\n~/.venv/lib/python3.10/site-packages/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    442         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\r\n    443     else:\r\n--> 444         fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n    445 \r\n    446     options = FileSystemFactoryOptions(\r\n\r\n~/.venv/lib/python3.10/site-packages/pyarrow/dataset.py in _ensure_single_source(path, filesystem)\r\n    418         paths_or_selector = [path]\r\n    419     else:\r\n--> 420         raise FileNotFoundError(path)\r\n    421 \r\n    422     return filesystem, paths_or_selector\r\n\r\nFileNotFoundError: my-test-bucket/test.parquet\r\n\r\n```\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2023.1.0\r\n- Python version: 3.10\r\n- pyarrow version: 10.0.1\r\n- fsspec version: 2022.11.0\r\n- s3fs version: 2022.11.0\r\n- Operating System: Ubuntu\r\n- Install method (conda, pip, source): pip\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9861/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9861/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}