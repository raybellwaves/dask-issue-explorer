{
    "url": "https://api.github.com/repos/dask/dask/issues/7885",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/7885/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/7885/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/7885/events",
    "html_url": "https://github.com/dask/dask/issues/7885",
    "id": 941278602,
    "node_id": "MDU6SXNzdWU5NDEyNzg2MDI=",
    "number": 7885,
    "title": "Mixed dataframe - array graph optimization (column projection pushdown not working if final result is an array)",
    "user": {
        "login": "jorisvandenbossche",
        "id": 1020496,
        "node_id": "MDQ6VXNlcjEwMjA0OTY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1020496?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jorisvandenbossche",
        "html_url": "https://github.com/jorisvandenbossche",
        "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
        "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
        "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
        "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
        "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
        "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 2156573524,
            "node_id": "MDU6TGFiZWwyMTU2NTczNTI0",
            "url": "https://api.github.com/repos/dask/dask/labels/highlevelgraph",
            "name": "highlevelgraph",
            "color": "8c24d6",
            "default": false,
            "description": "Issues relating to HighLevelGraphs."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2021-07-10T16:06:37Z",
    "updated_at": "2021-10-12T07:22:28Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "From https://github.com/geopandas/dask-geopandas/issues/78, but I made a small standalone example to illustrate the issue (see below for the full example workflow).  \r\nIf you have a workflow where you read a DataFrame from a file (eg parquet, which supports column selection), select a subset of the columns, and then perform an operation which results in an Array and not Series/DataFrame, the \"column projection\" pushdown (`project_columns` method of the `DataFrameIOLayer`, used in the `optimize_dataframe_getitem` function which is called in the general graph optimization step for DataFrames) is not working. \r\n\r\nTo the extent I understand the graph optimization and debugged this use case, the reason this doesn't work in this case is because the final collection object of the workflow is a dask Array (although all layers of the HighLevelGraph except for the last ones are still DataFrame-based), and thus the optimization step at\r\n\r\nhttps://github.com/dask/dask/blob/2a0020daa5ca960d9ae7f898b181d28693367a2f/dask/base.py#L333-L338\r\n\r\nuses the Array's optimize function (`Array.__dask_optimize__` -> `dask.array.optimization.optimize`) to optimize the full graph instead of the dask DataFrame version (and it's the dataframe version that has the logic to call `optimize_dataframe_getitem` and optimize this).\r\n\r\nWe can probably work around this in dask-geopandas (eg return a 4-element Series instead of 4-element array for the `total_bounds` attribute in question), but would such mixed-layer optimizations be in scope for dask?\r\n\r\n----\r\n\r\n**Full example**\r\n\r\nConsider the following dask dataframe from a Parquet dataset:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\ndf = pd.DataFrame(np.random.randn(100, 3), columns=[\"a\", \"b\", \"c\"])\r\nddf = dd.from_pandas(df, npartitions=2)\r\nddf.to_parquet(\"test_column_projection\")\r\n\r\nddf = dd.read_parquet(\"test_column_projection/\", engine=\"pyarrow\")\r\n```\r\n\r\nand I create a reduction operation (example here is to calculate min and max of one column):\r\n\r\n```python\r\ndef reduce_chunk(s):\r\n    return pd.Series([s.min(), s.max()], index=[\"min\", \"max\"])\r\n\r\ndef reduce_aggregate(df):\r\n    return pd.Series([df[\"min\"].min(), df[\"max\"].max()], index=[\"min\", \"max\"])\r\n\r\nresult = ddf[\"a\"].reduction(reduce_chunk, aggregate=reduce_aggregate)\r\n```\r\n\r\nThanks to the column projection pushdown in the graph optimization, the `getitem` layer ensures that the Parquet reader only reads column \"a\". To ensure this for illustration purposes, I added a print statement to the parquet reading function:\r\n\r\n```python\r\nfrom dask.dataframe.io.parquet import arrow\r\n\r\ndef log_columns_keyword(f):\r\n    \"\"\"Print the value passed for the columns to read\"\"\"\r\n    def wrapper(*args, **kwargs):\r\n        print(f.__name__, \"columns:\", args[3])\r\n        return f(*args, **kwargs)\r\n    return wrapper\r\n\r\narrow._read_table_from_path = log_columns_keyword(arrow._read_table_from_path)\r\n```\r\n\r\nwhich gives:\r\n```\r\nIn [4]: result.compute(scheduler=\"synchronous\")\r\n_read_table_from_path columns: ['a', '__null_dask_index__']\r\n_read_table_from_path columns: ['a', '__null_dask_index__']\r\nOut[4]: \r\nmin   -1.962085\r\nmax    2.065056\r\ndtype: float64\r\n```\r\n\r\nHowever, if I now rewrite the reduction to return an array instead of a Series:\r\n\r\n```python\r\ndef reduce_chunk2(s):\r\n    return np.array([s.min(), s.max()])\r\n\r\ndef reduce_aggregate2(arr):\r\n    return np.array([arr[0::2].min(), arr[1::2].max()])\r\n\r\nresult2 = ddf[\"a\"].reduction(reduce_chunk2, aggregate=reduce_aggregate2)\r\n```\r\n\r\nwe get:\r\n\r\n```\r\nIn [6]: result2.compute(scheduler=\"synchronous\")\r\n_read_table_from_path columns: ['a', 'b', 'c', '__null_dask_index__']\r\n_read_table_from_path columns: ['a', 'b', 'c', '__null_dask_index__']\r\nOut[6]: array([-1.96208513,  2.06505574])\r\n```\r\n\r\nSo here it is reading all columns from the Parquet file, even though I am only calculating something on `ddf[\"a\"]`.\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/7885/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/7885/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}