{
    "url": "https://api.github.com/repos/dask/dask/issues/4869",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/4869/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/4869/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/4869/events",
    "html_url": "https://github.com/dask/dask/issues/4869",
    "id": 451220632,
    "node_id": "MDU6SXNzdWU0NTEyMjA2MzI=",
    "number": 4869,
    "title": "Groupby NUnique is slow and possibly buggy",
    "user": {
        "login": "bluecoconut",
        "id": 916073,
        "node_id": "MDQ6VXNlcjkxNjA3Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/916073?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bluecoconut",
        "html_url": "https://github.com/bluecoconut",
        "followers_url": "https://api.github.com/users/bluecoconut/followers",
        "following_url": "https://api.github.com/users/bluecoconut/following{/other_user}",
        "gists_url": "https://api.github.com/users/bluecoconut/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/bluecoconut/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/bluecoconut/subscriptions",
        "organizations_url": "https://api.github.com/users/bluecoconut/orgs",
        "repos_url": "https://api.github.com/users/bluecoconut/repos",
        "events_url": "https://api.github.com/users/bluecoconut/events{/privacy}",
        "received_events_url": "https://api.github.com/users/bluecoconut/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 17,
    "created_at": "2019-06-02T18:34:00Z",
    "updated_at": "2023-10-28T13:10:41Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "## Overview \r\nI ran into some issues where groupby nunique was bottle-necking my computations. In chasing things, I found that by re-implementing and I was able to get over 100x improvement in speed in lots of test cases, but I'm unsure of the extra requirements to merge this / generalize my solution to work for all dask groupby nunique. (not sure current status / support on multi-index groupbys)\r\n\r\nThe core of the implementation is to use `groupby` on a multi index (including the values), and `.sum()` and `.count()` rather than `.apply(M.drop_duplicates)`. \r\n\r\nAlso note: `.apply(M.drop_duplicates)` seems to have a bug as well: https://github.com/pandas-dev/pandas/issues/26515\r\n\r\n## Slow Example\r\n\r\n```\r\nfrom dask.datasets import timeseries\r\ndf = timeseries()\r\nsuper_slow = df['y'].groupby(df['x']).nunique()\r\nresult = super_slow.compute()\r\n```\r\nNote: I've never actually let this finish, it seems (on my computer) to take over 10 minutes and still hasn't completed.\r\n\r\n## Faster, bespoke implementation\r\n<details><summary>Code Here</summary>\r\n<p>\r\n\r\n### ACA methods for nunique\r\n```\r\nimport pandas as pd\r\nfrom dask.dataframe.core import aca\r\n\r\ndef split_out_on_column(df, col_name='Unknown'):\r\n    return df[col_name]\r\n\r\n\r\ndef unique_count_chunk(target_series, groupby_series, rename='__count'):\r\n    grouped = target_series.groupby([groupby_series, target_series]).count().rename(rename)\r\n    return grouped.reset_index()\r\n\r\n\r\ndef unique_count_combine(df, groupby_name='groupby_col', target_name='target_col', rename='__count'):\r\n    grouped = df.groupby([groupby_name, target_name])[rename].sum().rename(rename)\r\n    return grouped.reset_index()\r\n\r\n\r\ndef nunique_aggregate(df, groupby_name='groupby_col', target_name='target_col', rename='__count'):\r\n    grouped = df.groupby([groupby_name, target_name])[rename].sum().rename(rename)\r\n    return grouped.groupby(groupby_name).count().astype('Int64').rename(rename)\r\n\r\n\r\ndef series_groupby_nunique(series_groupby, rename='__count', split_out=None, split_every=None):\r\n    if isinstance(series_groupby.index, list):\r\n        raise NotImplementedError(\"Multi-column groupby not supported in series-nunique\")\r\n    groupby_name = series_groupby.index.name\r\n    target_name = series_groupby.obj.name\r\n    meta = pd.Series([], dtype='Int64', name=rename,\r\n                     index=pd.Series([], dtype=series_groupby.obj.dtype, name=groupby_name))\r\n    return aca([series_groupby.obj, series_groupby.index],\r\n               token='series-groupby-nunique',\r\n               chunk=unique_count_chunk,\r\n               chunk_kwargs={'rename': rename},\r\n               aggregate=nunique_aggregate,\r\n               aggregate_kwargs={'groupby_name': groupby_name, 'target_name': target_name, 'rename': rename},\r\n               combine=unique_count_combine,\r\n               combine_kwargs={'groupby_name': groupby_name, 'target_name': target_name, 'rename': rename},\r\n               meta=meta,\r\n               split_every=split_every, split_out=split_out,\r\n               split_out_setup=split_out_on_column, split_out_setup_kwargs={'col_name': groupby_name})\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nSome aspects that make the above bespoke: \r\n1) Not allowing multiple column groupbys, this is just a series groupby series.\r\n2) Reliance on pandas to make the meta (not sure best practice here)\r\n3) Using `Int64` as my dtype since I prefer ints that can be nulled. Not sure of the implications of this / what dask standards are?\r\n4) Used `reset_index` on each level, since `concat` on `MultiIndex` seems to cause a memory leak? Was blowing out RAM on large `split_out=100`, but by resetting index and just passing around dataframes instead of indexed series to be combined, everything behaved well in terms of RAM.\r\n5) Not tested against categorical columns which have some different groupby behaviors.\r\n\r\n```\r\nfrom dask.datasets import timeseries\r\ndf = timeseries()\r\nmuch_faster = series_groupby_nunique(df['y'].groupby(df['x']))\r\nresult = much_faster.compute()\r\n```\r\n\r\n## Question:\r\n\r\nWhat was the reasoning behind using `.apply(drop_duplicates)` as part of the `nunique` groupby agg?\r\n\r\nDoes the faster implementation above seem like a reasonable alternative, or am I missing an edge case / have a clear failure in my implementation? Should I work towards putting this in as a PR?",
    "closed_by": {
        "login": "bluecoconut",
        "id": 916073,
        "node_id": "MDQ6VXNlcjkxNjA3Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/916073?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bluecoconut",
        "html_url": "https://github.com/bluecoconut",
        "followers_url": "https://api.github.com/users/bluecoconut/followers",
        "following_url": "https://api.github.com/users/bluecoconut/following{/other_user}",
        "gists_url": "https://api.github.com/users/bluecoconut/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/bluecoconut/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/bluecoconut/subscriptions",
        "organizations_url": "https://api.github.com/users/bluecoconut/orgs",
        "repos_url": "https://api.github.com/users/bluecoconut/repos",
        "events_url": "https://api.github.com/users/bluecoconut/events{/privacy}",
        "received_events_url": "https://api.github.com/users/bluecoconut/received_events",
        "type": "User",
        "site_admin": false
    },
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/4869/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/4869/timeline",
    "performed_via_github_app": null,
    "state_reason": "reopened"
}