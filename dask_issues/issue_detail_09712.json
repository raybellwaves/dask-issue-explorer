{
    "url": "https://api.github.com/repos/dask/dask/issues/9712",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9712/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9712/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9712/events",
    "html_url": "https://github.com/dask/dask/issues/9712",
    "id": 1473401217,
    "node_id": "I_kwDOAbcwm85X0lGB",
    "number": 9712,
    "title": "[DISCUSSION] Rethinking default read_parquet partitioning",
    "user": {
        "login": "rjzamora",
        "id": 20461013,
        "node_id": "MDQ6VXNlcjIwNDYxMDEz",
        "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rjzamora",
        "html_url": "https://github.com/rjzamora",
        "followers_url": "https://api.github.com/users/rjzamora/followers",
        "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
        "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
        "organizations_url": "https://api.github.com/users/rjzamora/orgs",
        "repos_url": "https://api.github.com/users/rjzamora/repos",
        "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rjzamora/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 5,
    "created_at": "2022-12-02T20:20:13Z",
    "updated_at": "2024-04-22T01:46:21Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "This is mostly a brain dump, so I realize this may be difficult to process. With that said, I welcome any thoughts and feedback (both positive and negative)\r\n\r\nI spent some time yesterday working through the following exercise: **If I were defining and writing the `dask.dataframe.read_parquet` API from scratch, what would it look like?**\r\n\r\nI experimented for a few hours before arriving at an API that was not all that different from what we have today. The key difference was that my \"ideal\" API was prioritizing **efficient*** partition sizes, while `dd.read_parquet` is only prioritizing metadata-processing latency.  That is, `dd.read_parquet` is doing its very best to avoid processing Parquet metadata, just in case the data is in remote storage.  Although this decision makes sense in context, I believe we are much better off prioritizing efficient partition sizes, and warning (and instructing) the user (ahead of time) when we expect the defaults to case pain.\r\n\r\nTo better-prioritize efficient partitioning, I suspect we could make the following changes:\r\n\r\n- Remove `split_row_groups` completely\r\n  - This is the most-significant **breaking** change considered here. However, it is probably possible to translate most `split_row_groups` uses into *other* options (clearing a manageable path for deprecation)\r\n- Remove `aggregate_files` completely\r\n  - This option is already \"pre-deprecated,\" so this should be manageable\r\n- I'd deprecate the current meaning of `chunksize`\r\n  - It should mean \"the maximum-desired **row-count** in a partition\" to be more consistent with similarly-named arguments in pandas/pyarrow\r\n  - The default for `chunksize` (or maybe `chunk_size`) should be `None`\r\n  - The existing `chunksize` option is already \"pre-deprecated,\" so this may be manageable\r\n- I'd add a new `blocksize` argument\r\n  - **Meaning**: \"the maximum desired (uncompressed) storage size to map to each partition\"\r\n  - The default for `blocksize` would be set by the engine (so it could be larger in dask_cudf)\r\n  - `chunksize` would take precedence over `blocksize` (since row-count is a more-specific criteria)\r\n- I'd add a `granularity` argument\r\n  - **Options**: `\"row-group\"`, `\"file\"`, `(\"row-group\", <int>)`, or `(\"file\", <int>)`\r\n    - **Note**: I'm not crazy about the `(\"row-group\", <int>)` or `(\"file\", <int>)` options, but this would allow us to avoid breaking existing code that uses `split_row_groups=<int>`\r\n  - **Meaning**: What kind of \"metadata fragments\" should we work with when processing metadata and generating a \"partitioning plan\"\r\n  - The default would be `\"file\"` when both `blocksize` and `chunksize` are set to `None`, and `\"row-group\"` otherwise\r\n- I'd add a `file_groups` argument\r\n  - **Meaning**: Whether any directory-partitioned columns should be used to prevent two row-groups or files from being mapped to the same output DataFrame partition. For example, the user might specify something like groups=[\"year\", \"month\"] (leaving out \"day\") to indicate that they want every row in a given partition to correspond to the same year and month.\r\n  - Default would be `None`\r\n  - `True` could be taken to mean that an output partition can only map to a single file (i.e. \"all files are a distinct group\")\r\n- I'd use `dask.delayed` to eagerly collect \"metadata-fragment\" records from the dataset in parallel\r\n  - `granularity` would determine if each metadata-fragment record corresponds to a file, row-group, or integer-stride of files/row-groups\r\n  - These records would be used (on the client) to construct a partitioning plan that satisfies `chunksize`, `blocksize` and `file_groups`\r\n  - For large datasets (where we known that there are **many** records to collect) on remote storage, I would raise a `UserWarning` to suggest that the user set `blocksize=None` or use `granularity=\"file\"` if latency is too high and/or files are sufficiently small (this warning would be silenced if any of the partitioning arguments were explicitly set)\r\n    - This would allow us to prioritize good default partition sizes, and leave it up to the user to opt into a simple per-file mapping if they must avoid parsing metadata at all costs",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9712/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9712/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}