{
    "url": "https://api.github.com/repos/dask/dask/issues/3514",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/3514/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/3514/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/3514/events",
    "html_url": "https://github.com/dask/dask/issues/3514",
    "id": 324621533,
    "node_id": "MDU6SXNzdWUzMjQ2MjE1MzM=",
    "number": 3514,
    "title": "Very large dask arrays",
    "user": {
        "login": "mrocklin",
        "id": 306380,
        "node_id": "MDQ6VXNlcjMwNjM4MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mrocklin",
        "html_url": "https://github.com/mrocklin",
        "followers_url": "https://api.github.com/users/mrocklin/followers",
        "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
        "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
        "organizations_url": "https://api.github.com/users/mrocklin/orgs",
        "repos_url": "https://api.github.com/users/mrocklin/repos",
        "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mrocklin/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862305,
            "node_id": "MDU6TGFiZWwyNDI4NjIzMDU=",
            "url": "https://api.github.com/repos/dask/dask/labels/array",
            "name": "array",
            "color": "006b75",
            "default": false,
            "description": null
        },
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 23,
    "created_at": "2018-05-19T11:03:46Z",
    "updated_at": "2021-10-12T05:27:41Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "When dealing with very large datasets sometimes the task graph handling becomes a bottleneck.  What are some ways in which this can be addressed medium-to-long term.\r\n\r\nAs an example consider a petabyte array cut into gigabyte chunks.  This means that each elementwise operation on the array generates a million new tasks.  If each task takes up something like 1ms of overhead, then each operation generates around 16 minutes of overhead on the client and scheduler (assuming distributed for the time being).  If we're doing operations with a hundred or so operations then we're waiting a day or two with just overhead.\r\n\r\nHow can we address this situation?\r\n\r\n1.  **Reduce per-task overhead**:  1ms today is actually fairly pessimistic, we're closer to 200-300us, but this requires some effort on our part to ensure that all tasks are cheaply serializable.  This means no dynamic functions, or anything that strictly requires cloudpickle or generates a nontrivial amount of data in serialized form.\r\n2.  **Fuse tasks**: Currently when we do `2 * x + 1` both the `2 * ` and `+ 1` operations generate a million tasks.  This hurts both when we construct the dask arrays, and when we send them to the scheduler.  Ideally we would pre-fuse operations like these, so that we only ever generated one million tasks that did both the `2 *` and `+ 1`.  This requires that the things we operate on at first to behave differently than dask.arrays as they are currently designed.  We need some sort of higher-level array construct.  This might either be a fully symbolic array (simliar to SymPy) or it might be a modification to dask.array to support [atop fusion](https://github.com/dask/dask/issues/2538).\r\n3.  **Overlap overhead with computation**: the 16 minutes of overhead might not be that big of a deal if the computation itself is large.  In this case it's mostly a UX problem as people feel concerned that their computation hasn't started yet.  If we're able to break apart the graph into reasonable chunks then we might be able to stream the graph to the scheduler in pieces.  Doing this cheaply and reliably is likely to be very hard though.\r\n4.  **Use much larger tasks**: Everything becomes easier if we make our tasks larger.  Memory and compute power continues to grow.  ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/3514/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/3514/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}