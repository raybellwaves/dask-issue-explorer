{
    "url": "https://api.github.com/repos/dask/dask/issues/10853",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10853/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10853/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10853/events",
    "html_url": "https://github.com/dask/dask/issues/10853",
    "id": 2095721593,
    "node_id": "I_kwDOAbcwm8586ix5",
    "number": 10853,
    "title": "SeriesGroupBy.agg does not accept `Aggregate` when paired with `'median'`",
    "user": {
        "login": "flying-sheep",
        "id": 291575,
        "node_id": "MDQ6VXNlcjI5MTU3NQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/291575?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/flying-sheep",
        "html_url": "https://github.com/flying-sheep",
        "followers_url": "https://api.github.com/users/flying-sheep/followers",
        "following_url": "https://api.github.com/users/flying-sheep/following{/other_user}",
        "gists_url": "https://api.github.com/users/flying-sheep/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/flying-sheep/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/flying-sheep/subscriptions",
        "organizations_url": "https://api.github.com/users/flying-sheep/orgs",
        "repos_url": "https://api.github.com/users/flying-sheep/repos",
        "events_url": "https://api.github.com/users/flying-sheep/events{/privacy}",
        "received_events_url": "https://api.github.com/users/flying-sheep/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3880424463,
            "node_id": "LA_kwDOAbcwm87nSpQP",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20triage",
            "name": "needs triage",
            "color": "eeeeee",
            "default": false,
            "description": "Needs a response from a contributor"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-01-23T10:11:35Z",
    "updated_at": "2024-01-23T10:16:05Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\nSee title and example code. This is *not* #10836.\r\nThat issue can be worked around by using a list instead of `kwargs`, but then I run into this:\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\ncustom_sum = dd.Aggregation(\r\n    name='custom_sum',\r\n    chunk=lambda s: s.sum(),\r\n    agg=lambda s0: s0.sum()\r\n)\r\n\r\ndf = pd.DataFrame(dict(a=[1, 1, 1, 1, 1], g=[5, 6, 6, 6, 7]))\r\nddf = dd.from_pandas(df, npartitions=2)\r\n\r\nddf.groupby('g')['a'].agg([\"median\"], shuffle=True).compute()  # works\r\nddf.groupby('g')['a'].agg([custom_sum], shuffle=True).compute()  # works\r\nddf.groupby('g')['a'].agg([custom_sum, 'median'], shuffle=True).compute()  # broken\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n<details>\r\n<summary>Traceback</summary>\r\n\r\n```pytb\r\nIn [4]: ddf.groupby('g')['a'].agg([custom_sum, 'median'], shuffle=True).compute()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/utils.py:194, in raise_on_meta_error(funcname, udf)\r\n    193 try:\r\n--> 194     yield\r\n    195 except Exception as e:\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7103, in _emulate(func, udf, *args, **kwargs)\r\n   7102 with raise_on_meta_error(funcname(func), udf=udf), check_numeric_only_deprecation():\r\n-> 7103     return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/groupby.py:439, in _groupby_aggregate_spec(df, spec, levels, dropna, sort, observed, **kwargs)\r\n    438 observed = {\"observed\": observed} if observed is not None else {}\r\n--> 439 return df.groupby(level=levels, sort=sort, **observed, **dropna).aggregate(\r\n    440     spec, **kwargs\r\n    441 )\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1445, in DataFrameGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)\r\n   1444 op = GroupByApply(self, func, args=args, kwargs=kwargs)\r\n-> 1445 result = op.agg()\r\n   1446 if not is_dict_like(func) and result is not None:\r\n   1447     # GH #52849\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py:178, in Apply.agg(self)\r\n    176 elif is_list_like(func):\r\n    177     # we require a list, but not a 'str'\r\n--> 178     return self.agg_list_like()\r\n    180 if callable(func):\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py:311, in Apply.agg_list_like(self)\r\n    304 \"\"\"\r\n    305 Compute aggregation in the case of a list-like argument.\r\n    306 \r\n   (...)\r\n    309 Result of aggregation.\r\n    310 \"\"\"\r\n--> 311 return self.agg_or_apply_list_like(op_name=\"agg\")\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py:1353, in GroupByApply.agg_or_apply_list_like(self, op_name)\r\n   1350 with com.temp_setattr(\r\n   1351     obj, \"as_index\", True, condition=hasattr(obj, \"as_index\")\r\n   1352 ):\r\n-> 1353     keys, results = self.compute_list_like(op_name, selected_obj, kwargs)\r\n   1354 result = self.wrap_results_list_like(keys, results)\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py:370, in Apply.compute_list_like(self, op_name, selected_obj, kwargs)\r\n    365 args = (\r\n    366     [self.axis, *self.args]\r\n    367     if include_axis(op_name, colg)\r\n    368     else self.args\r\n    369 )\r\n--> 370 new_res = getattr(colg, op_name)(func, *args, **kwargs)\r\n    371 results.append(new_res)\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:255, in SeriesGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)\r\n    254 kwargs[\"engine_kwargs\"] = engine_kwargs\r\n--> 255 ret = self._aggregate_multiple_funcs(func, *args, **kwargs)\r\n    256 if relabeling:\r\n    257     # columns is not narrowed by mypy from relabeling flag\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:360, in SeriesGroupBy._aggregate_multiple_funcs(self, arg, *args, **kwargs)\r\n    359         key = base.OutputKey(label=name, position=idx)\r\n--> 360         results[key] = self.aggregate(func, *args, **kwargs)\r\n    362 if any(isinstance(x, DataFrame) for x in results.values()):\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:292, in SeriesGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)\r\n    291 try:\r\n--> 292     return self._python_agg_general(func, *args, **kwargs)\r\n    293 except KeyError:\r\n    294     # KeyError raised in test_groupby.test_basic is bc the func does\r\n    295     #  a dictionary lookup on group.name, but group name is not\r\n    296     #  pinned in _python_agg_general, only in _aggregate_named\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:325, in SeriesGroupBy._python_agg_general(self, func, *args, **kwargs)\r\n    324 obj = self._obj_with_exclusions\r\n--> 325 result = self.grouper.agg_series(obj, f)\r\n    326 res = obj._constructor(result, name=obj.name)\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/ops.py:849, in BaseGrouper.agg_series(self, obj, func, preserve_dtype)\r\n    847     preserve_dtype = True\r\n--> 849 result = self._aggregate_series_pure_python(obj, func)\r\n    851 if len(obj) == 0 and len(result) == 0 and isinstance(obj.dtype, ExtensionDtype):\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/ops.py:877, in BaseGrouper._aggregate_series_pure_python(self, obj, func)\r\n    876 for i, group in enumerate(splitter):\r\n--> 877     res = func(group)\r\n    878     res = extract_result(res)\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:322, in SeriesGroupBy._python_agg_general.<locals>.<lambda>(x)\r\n    321     warn_alias_replacement(self, orig_func, alias)\r\n--> 322 f = lambda x: func(x, *args, **kwargs)\r\n    324 obj = self._obj_with_exclusions\r\n\r\nTypeError: 'Aggregation' object is not callable\r\n```\r\n\r\n> The above exception was the direct cause of the following exception:\r\n\r\n```pytb\r\nValueError                                Traceback (most recent call last)\r\nCell In[4], line 1\r\n----> 1 ddf.groupby('g')['a'].agg([custom_sum, 'median'], shuffle=True).compute()\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/groupby.py:3043, in SeriesGroupBy.agg(self, arg, split_every, split_out, shuffle, **kwargs)\r\n   3041 @_aggregate_docstring(based_on=\"pd.core.groupby.SeriesGroupBy.agg\")\r\n   3042 def agg(self, arg=None, split_every=None, split_out=1, shuffle=None, **kwargs):\r\n-> 3043     return self.aggregate(\r\n   3044         arg=arg,\r\n   3045         split_every=split_every,\r\n   3046         split_out=split_out,\r\n   3047         shuffle=shuffle,\r\n   3048         **kwargs,\r\n   3049     )\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/groupby.py:3019, in SeriesGroupBy.aggregate(self, arg, split_every, split_out, shuffle, **kwargs)\r\n   3015 @_aggregate_docstring(based_on=\"pd.core.groupby.SeriesGroupBy.aggregate\")\r\n   3016 def aggregate(\r\n   3017     self, arg=None, split_every=None, split_out=1, shuffle=None, **kwargs\r\n   3018 ):\r\n-> 3019     result = super().aggregate(\r\n   3020         arg=arg,\r\n   3021         split_every=split_every,\r\n   3022         split_out=split_out,\r\n   3023         shuffle=shuffle,\r\n   3024         **kwargs,\r\n   3025     )\r\n   3026     if self._slice:\r\n   3027         try:\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/groupby.py:2327, in _GroupBy.aggregate(self, arg, split_every, split_out, shuffle, **kwargs)\r\n   2316 if shuffle:\r\n   2317     # Shuffle-based aggregation\r\n   2318     #\r\n   (...)\r\n   2324     # If we have a median in the spec, we cannot do an initial\r\n   2325     # aggregation.\r\n   2326     if has_median:\r\n-> 2327         result = _shuffle_aggregate(\r\n   2328             chunk_args,\r\n   2329             chunk=_non_agg_chunk,\r\n   2330             chunk_kwargs={\r\n   2331                 \"key\": [\r\n   2332                     c for c in _obj.columns.tolist() if c not in group_columns\r\n   2333                 ],\r\n   2334                 **self.observed,\r\n   2335                 **self.dropna,\r\n   2336             },\r\n   2337             aggregate=_groupby_aggregate_spec,\r\n   2338             aggregate_kwargs={\r\n   2339                 \"spec\": arg,\r\n   2340                 \"levels\": _determine_levels(self.by),\r\n   2341                 **self.observed,\r\n   2342                 **self.dropna,\r\n   2343             },\r\n   2344             token=\"aggregate\",\r\n   2345             split_every=split_every,\r\n   2346             split_out=split_out,\r\n   2347             shuffle_method=shuffle,\r\n   2348             sort=self.sort,\r\n   2349         )\r\n   2350     else:\r\n   2351         result = _shuffle_aggregate(\r\n   2352             chunk_args,\r\n   2353             chunk=_groupby_apply_funcs,\r\n   (...)\r\n   2372             sort=self.sort,\r\n   2373         )\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/groupby.py:3314, in _shuffle_aggregate(args, chunk, aggregate, token, chunk_kwargs, aggregate_kwargs, split_every, split_out, sort, ignore_index, shuffle_method)\r\n   3306     result = chunked.shuffle(\r\n   3307         chunked.index,\r\n   3308         ignore_index=ignore_index,\r\n   3309         npartitions=shuffle_npartitions,\r\n   3310         shuffle_method=shuffle_method,\r\n   3311     )\r\n   3313 # Aggregate\r\n-> 3314 result = result.map_partitions(aggregate, **aggregate_kwargs)\r\n   3316 if convert_back_to_series:\r\n   3317     result = result[\"__series__\"].rename(series_name)\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/core.py:1152, in _Frame.map_partitions(self, func, *args, **kwargs)\r\n   1024 @insert_meta_param_description(pad=12)\r\n   1025 def map_partitions(self, func, *args, **kwargs):\r\n   1026     \"\"\"Apply Python function on each DataFrame partition.\r\n   1027 \r\n   1028     Note that the index and divisions are assumed to remain unchanged.\r\n   (...)\r\n   1150     None as the division.\r\n   1151     \"\"\"\r\n-> 1152     return map_partitions(func, self, *args, **kwargs)\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7173, in map_partitions(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\r\n   7166         raise ValueError(\r\n   7167             f\"{e}. If you don't want the partitions to be aligned, and are \"\r\n   7168             \"calling `map_partitions` directly, pass `align_dataframes=False`.\"\r\n   7169         ) from e\r\n   7171 dfs = [df for df in args if isinstance(df, _Frame)]\r\n-> 7173 meta = _get_meta_map_partitions(args, dfs, func, kwargs, meta, parent_meta)\r\n   7174 if all(isinstance(arg, Scalar) for arg in args):\r\n   7175     layer = {\r\n   7176         (name, 0): (\r\n   7177             apply,\r\n   (...)\r\n   7181         )\r\n   7182     }\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7284, in _get_meta_map_partitions(args, dfs, func, kwargs, meta, parent_meta)\r\n   7280     parent_meta = dfs[0]._meta\r\n   7281 if meta is no_default:\r\n   7282     # Use non-normalized kwargs here, as we want the real values (not\r\n   7283     # delayed values)\r\n-> 7284     meta = _emulate(func, *args, udf=True, **kwargs)\r\n   7285     meta_is_emulated = True\r\n   7286 else:\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7102, in _emulate(func, udf, *args, **kwargs)\r\n   7097 def _emulate(func, *args, udf=False, **kwargs):\r\n   7098     \"\"\"\r\n   7099     Apply a function using args / kwargs. If arguments contain dd.DataFrame /\r\n   7100     dd.Series, using internal cache (``_meta``) for calculation\r\n   7101     \"\"\"\r\n-> 7102     with raise_on_meta_error(funcname(func), udf=udf), check_numeric_only_deprecation():\r\n   7103         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n\r\nFile /usr/lib/python3.11/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\r\n    153     value = typ()\r\n    154 try:\r\n--> 155     self.gen.throw(typ, value, traceback)\r\n    156 except StopIteration as exc:\r\n    157     # Suppress StopIteration *unless* it's the same exception that\r\n    158     # was passed to throw().  This prevents a StopIteration\r\n    159     # raised inside the \"with\" statement from being suppressed.\r\n    160     return exc is not value\r\n\r\nFile ~/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/utils.py:215, in raise_on_meta_error(funcname, udf)\r\n    206 msg += (\r\n    207     \"Original error is below:\\n\"\r\n    208     \"------------------------\\n\"\r\n   (...)\r\n    212     \"{2}\"\r\n    213 )\r\n    214 msg = msg.format(f\" in `{funcname}`\" if funcname else \"\", repr(e), tb)\r\n--> 215 raise ValueError(msg) from e\r\n\r\nValueError: Metadata inference failed in `_groupby_aggregate_spec`.\r\n```\r\n\r\n> You have supplied a custom function and Dask is unable to \r\n> determine the type of output that that function returns. \r\n> \r\n> To resolve this please provide a meta= keyword.\r\n> The docstring of the Dask function you ran should have more information.\r\n> \r\n> Original error is below:\r\n\r\n```pytb\r\n------------------------\r\nTypeError(\"'Aggregation' object is not callable\")\r\n\r\nTraceback:\r\n---------\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 194, in raise_on_meta_error\r\n    yield\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/core.py\", line 7103, in _emulate\r\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/dask/dataframe/groupby.py\", line 439, in _groupby_aggregate_spec\r\n    return df.groupby(level=levels, sort=sort, **observed, **dropna).aggregate(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py\", line 1445, in aggregate\r\n    result = op.agg()\r\n             ^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py\", line 178, in agg\r\n    return self.agg_list_like()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py\", line 311, in agg_list_like\r\n    return self.agg_or_apply_list_like(op_name=\"agg\")\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py\", line 1353, in agg_or_apply_list_like\r\n    keys, results = self.compute_list_like(op_name, selected_obj, kwargs)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/apply.py\", line 370, in compute_list_like\r\n    new_res = getattr(colg, op_name)(func, *args, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py\", line 255, in aggregate\r\n    ret = self._aggregate_multiple_funcs(func, *args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py\", line 360, in _aggregate_multiple_funcs\r\n    results[key] = self.aggregate(func, *args, **kwargs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py\", line 292, in aggregate\r\n    return self._python_agg_general(func, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py\", line 325, in _python_agg_general\r\n    result = self.grouper.agg_series(obj, f)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/ops.py\", line 849, in agg_series\r\n    result = self._aggregate_series_pure_python(obj, func)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/ops.py\", line 877, in _aggregate_series_pure_python\r\n    res = func(group)\r\n          ^^^^^^^^^^^\r\n  File \"/home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/test.py3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py\", line 322, in <lambda>\r\n    f = lambda x: func(x, *args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\r\n```\r\n\r\n</details>\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.1.0\r\n- Python version: 3.11\r\n- Operating System: Arch Linux\r\n- Install method (conda, pip, source): pip, source\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10853/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10853/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}