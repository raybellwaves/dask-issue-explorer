[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/900305350",
        "html_url": "https://github.com/dask/dask/issues/8045#issuecomment-900305350",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8045",
        "id": 900305350,
        "node_id": "IC_kwDOAbcwm841qZHG",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-17T13:36:37Z",
        "updated_at": "2021-08-17T13:36:37Z",
        "author_association": "MEMBER",
        "body": "As you can see, @lmmx , it's not a simple problem, and there's a reason that it's been around a long time. Your suggestions may be interesting, but hard to evaluate without some case studies. Providing custom offsets for read_block that have been obtained in some other way is certainly doable, and providing custom validation of blocks is not totally unreasonable either. However, the read_csv function is already massively overloaded with arguments, so I fear that any additions would be very carefully scrutinised.\r\n\r\nI have an orthogonal suggestion from another project I am working on, [fsspec-reference-maker](https://github.com/intake/fsspec-reference-maker). The idea is, to make a \"references\" file (JSON), in which each entry corresponds to a specific bytes block of some other URL. This set of reference blocks can be viewed as a filesystem via fsspec - which dask already understands. Thus, you only need to find valid offsets and encode them, and then everything else will work out fine - without touching Dask's code. I imagine you could add a CSV module to fsspec-reference-maker which uses pandas (or `csv` directly)  to stream through the target file and identify valid offsets on the way, with any custom validation. This only need happen once, and once you have the references JSON, Dask need never know that something special is going on.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/900305350/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/902256015",
        "html_url": "https://github.com/dask/dask/issues/8045#issuecomment-902256015",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8045",
        "id": 902256015,
        "node_id": "IC_kwDOAbcwm841x1WP",
        "user": {
            "login": "lmmx",
            "id": 2979452,
            "node_id": "MDQ6VXNlcjI5Nzk0NTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2979452?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lmmx",
            "html_url": "https://github.com/lmmx",
            "followers_url": "https://api.github.com/users/lmmx/followers",
            "following_url": "https://api.github.com/users/lmmx/following{/other_user}",
            "gists_url": "https://api.github.com/users/lmmx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lmmx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lmmx/subscriptions",
            "organizations_url": "https://api.github.com/users/lmmx/orgs",
            "repos_url": "https://api.github.com/users/lmmx/repos",
            "events_url": "https://api.github.com/users/lmmx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lmmx/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-19T21:20:28Z",
        "updated_at": "2021-08-19T21:25:34Z",
        "author_association": "CONTRIBUTOR",
        "body": "I'm with you on all of the above. Regarding not overloading `read_csv`, I propose that the routine I'm outlining here can reuse the `sample_rows=10` argument.\r\n\r\nThat said, not sure about how else this check could be switched off (if it isn't switched off, then rows would be read from every offset, greedily, which may be undesirable for known-valid CSVs, e.g. those without multiline strings within rows). However I'll see what it looks like, I suspect any buffering 'cost' would be negligible so it wouldn't matter.\r\n\r\nIf no new options are to be added to `read_csv`, then partitioning a CSV with 2 or more will incur a reading cost of 10 rows of bytes per partition, using the csv module (not pandas). I hope this can be done efficiently and will need to benchmark to make sure I don't cause a performance regression.\r\n\r\nTo be able to turn it off, I would suggest adding a `sample_partition_rows=10` argument, such that `sample_partition_rows=0` would turn off this check (but for now will operate based on reusing `sample_rows`).\r\n\r\n> **Content warning(?)** -- long documentation follows in the rest of this post, perhaps only primarily of interest during implementation. Please don't feel obliged to wade through if the level of detail is too much, I just needed to flesh it out to be sure enough how to implement it.\r\n\r\nPlease do feel free to pick up on any or none of the discussion below :smiley_cat:\r\n\r\n## Part 0: Background on how pandas parses CSVs\r\n\r\n<details><summary>Click for background on how pandas parses CSVs</summary>\r\n\r\n<p>\r\n\r\n[The `lineterminator` argument to `dask.dataframe.io.csv.read_pandas`](https://github.com/dask/dask/blob/e6b9e8983b26389c74036aeec615ac0e5ef34c65/dask/dataframe/io/csv.py#L470-L473) is already copied from `pandas.read_csv` (see [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)) so there is room to also use the arguments -- even if simply extracted [copied] from kwargs -- for quoting:\r\n\r\n- `quotechar='\"'`\r\n- `quoting=0`\r\n- `doublequote=True`\r\n\r\nN.B. `doublequote` does **not** toggle the `quotechar` as being the double quotation mark character(!)\r\n\r\n> **quotechar** : str (length 1), optional\r\n>     The character used to denote the start and end of a quoted item. Quoted\r\n>     items can include the delimiter and it will be ignored.\r\n>\r\n> **quoting** : int or csv.QUOTE_* instance, default 0\r\n>     Control field quoting behavior per `csv.QUOTE_*` constants. Use one of\r\n>     QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\r\n>\r\n> **doublequote** : bool, default `True`\r\n>    When quotechar is specified and quoting is not `QUOTE_NONE`, indicate\r\n>    whether or not to interpret two consecutive quotechar elements INSIDE a\r\n>    field as a single `quotechar` element.\r\n\r\nand the code itself takes the following approach:\r\n\r\n- `read_csv(file_or_buf, **kwargs)` --> `_read(file_or_buf, kwds=kwargs)` --> `TextFileReader(file_or_buf, kwds=kwds)`\r\n- [Regardless of engine] the `parser_defaults` keys are [iterated through](https://github.com/pandas-dev/pandas/blob/b706e4a3853f9daaf744e14c8cfadb89db723c85/pandas/io/parsers/readers.py#L822-L829) ([including](https://github.com/pandas-dev/pandas/blob/b706e4a3853f9daaf744e14c8cfadb89db723c85/pandas/io/parsers/base_parser.py#L75-L77) the 3 mentioned above) and used to populate the `options` dict [initialised as `{}`] which is returned into the init method of TextFileReader and then assigned on its `options` attribute after [passing through](https://github.com/pandas-dev/pandas/blob/master/pandas/io/parsers/readers.py#L806) its `_clean_options` method\r\n  - in the `_clean_options` method, [if the `quotechar` is non-ASCII](https://github.com/pandas-dev/pandas/blob/b706e4a3853f9daaf744e14c8cfadb89db723c85/pandas/io/parsers/readers.py#L918-L930) then the C engine will fallback to the Python engine\r\n\r\nTo take the example of PythonParser (which must presumably match the procedure of the C parser, but is readable in the Python code)\r\n- `self.quoting` is [checked](https://github.com/pandas-dev/pandas/blob/b706e4a3853f9daaf744e14c8cfadb89db723c85/pandas/io/parsers/python_parser.py#L972) not to be `csv.QUOTING_NONE` in `_rows_to_cols`,\r\n- which [sets `alldata` in the `read` method](https://github.com/pandas-dev/pandas/blob/b706e4a3853f9daaf744e14c8cfadb89db723c85/pandas/io/parsers/python_parser.py#L278)\r\n- which [for whichever parser engine] `TextFileReader` [wraps through its `read` method](https://github.com/pandas-dev/pandas/blob/b706e4a3853f9daaf744e14c8cfadb89db723c85/pandas/io/parsers/readers.py#L1047) \r\n\r\n</p>\r\n\r\n</details>\r\n\r\n## Part 1: Problem statement\r\n\r\nI've thought about how it would be achieved, and I suggest sampling from an identical number of rows as sampled for the file head (`sample` in `read_bytes`), at the tail of each partition [backwards from each candidate newline delimiter], to consume an identical number of rows up to the row-delimiting newline that first follows the end of the partition (`sample_rows=10`).\r\n\r\nI've also road tested some approaches to going about this, and concluded that pandas will not be able to achieve it (the Python csv module is needed instead, due to the way pandas conflates absent and empty values as \"missing\"/NaN).\r\n\r\n<details><summary>Click for further details on the pandas parser (and its shortcomings)</summary>\r\n\r\n<p>\r\n\r\nIt's important to go to the lengths of 10 rows, so as to be sure to avoid any potential edge case in which a quoted multiline field value could mimic one or more entire rows. This is already the default for sampling rows at the file head, as in the following edge case:\r\n\r\n```py\r\n>>> pd.read_csv(io.BytesIO(b'intA,intB,strC\\n1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"'))\r\n   intA  intB                    strC\r\n0     1     2                   hello\r\n1     3     4                   world\r\n2     5     6  foo\\n7,8,bar\\n9,10,baz\r\n```\r\n\r\nin which the final row [index 2] expands to:\r\n\r\n```\r\n5,6,\"foo\r\n7,8,bar\r\n9,10,baz\"\r\n```\r\n\r\nwhich dask could, by some odd chance, land on and, since it fulfils the requirements of the row, assume came from:\r\n\r\n```py\r\n>>> pd.read_csv(io.BytesIO(b'intA,intB,strC\\n7,8,bar'))\r\n   intA  intB strC\r\n0     7     8  bar\r\n```\r\n\r\nwhich would quietly induce an error upon parsing at the next partition's start (when the `quotechar` would be encountered, giving the parser EOF error this issue opened with).\r\n\r\nNote that this edge case relies on the validity of inconsistent quoting of a column. This corresponds to the Python csv module docs (\u201cfield\u201d means \u201citem in a row\u201d):\r\n\r\n> csv.QUOTE_MINIMAL\u00b6\r\n> Instructs writer objects to only quote those fields which contain special characters such as delimiter, quotechar or any of the characters in `lineterminator`.\r\n\r\nI think sampling 10 rows from the tail of each partition per file is adequate to catch any such edge case and so to validate the next partition.\r\n\r\nI think they should be sampled **backwards from the first row-delimiting newline after the partition end**, as is done for the head [except the head is sampled forwards, only once per file].\r\n\r\nThis check can be skipped for the final partition -- as of course there is no choice of newline delimiters following the EOF to iterate past so the EOF itself must be accepted as a row-delimiting newline, else the file is corrupt/invalid [out of scope of this feature]\r\n\r\n- A file of 1 partition does not have to check a partition tail,\r\n- A file of 2 partitions only has to check the first partition's tail.\r\n\r\nIt should also fail fast (i.e. move to the next `lineseparator` delimiter ASAP), by catching the same pandas parser error containing the 'EOF' substring as done when computing the `head`:\r\n\r\nhttps://github.com/dask/dask/blob/e6b9e8983b26389c74036aeec615ac0e5ef34c65/dask/dataframe/io/csv.py#L593-L596\r\n\r\nbut where the file head routine raises a ValueError from the caught exception, the partition tail routine should instead move onto the next candidate `lineseparator` delimiter (with `seek_delimiter`) and retry, only failing if it reaches EOF (i.e. the routine has proceeded through the entirety of the final partition without finding a new row)\r\n\r\n- This would mean that what was thought to be the final partition was actually entirely one of the earlier examples of a misleading column. This could only conceivably happen if the partitions were very small and the quoted column value was very long [spanning many lines]. I wouldn't expect this to be cause for concern.\r\n\r\nAnother major difference between the file head and partition tail routines: the partition tails are to be read backwards [from the first newline after the offset, and if that isn't a row-delimiting newline then the next after that, etc], whereas the file head is simply read forwards.\r\n\r\nOne aspect which I can't see any problem with (other than it's weird) is that the only sensible way to read backwards is to also reverse the bytes buffer.\r\n\r\nLuckily I recently wrote [just such a routine](https://github.com/lmmx/wikitransp/blob/386bf26055661c9e13225c622d1844fc5b938116/src/wikitransp/scraper/buf_grep.py#L10-L101) for iterating backwards through newlines in a buffer which can be adapted for this task. I'm confident it is correct, and it is well documented and commented for clarity (I will remove the type annotations when reusing it to fit the dask house style).\r\n\r\n- I can't think of any of the data types which would not be appropriate to do this for \r\n  - strings (including categoricals), integers, floats, etc are all reversible and still valid\r\n  - datetime dtype cannot be created without converters [to my knowledge/my attempts to produce them]\r\n  - \"valid\" here only means syntax, not semantics (the correct number of columns per row)\r\n\r\ne.g. take the previous example, and pretend that the dataframe there was the sample (giving us the column names)\r\n\r\n```py\r\n>>> bytestr = b'intA,intB,strC\\n1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"'\r\n>>> pd.read_csv(io.BytesIO(bytestr))\r\n   intA  intB                    strC\r\n0     1     2                   hello\r\n1     3     4                   world\r\n2     5     6  foo\\n7,8,bar\\n9,10,baz\r\n>>> pd.read_csv(io.BytesIO(bytestr[::-1]))\r\n  zab,01,9\\nrab,8,7\\noof     6     5\r\n0                  dlrow     4     3\r\n1                  olleh     2     1\r\n2                   Crts  Btni  Atni\r\n```\r\n\r\n- Note here how the first partition in a file runs the risk of including the 1st line of a file (the header). This however can be easily detected: if the tail partition reads back to the start of the file (easy to check from offset and the tail buffer length), then only in this case would it be acceptable to read the 10 partition tail rows going forward instead of reverse (however in that case surely you'd just use the sample instead, as it'd be equivalent)\r\n\r\nNow ignore the header row and just look at the first 2 rows of data, and pretend that the dataframe earlier was the sample (so we now have the column names)\r\n\r\n```py\r\n>>> rows_bytestr = b'1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"'\r\n>>> rev_rows_b = rows_bytestr[::-1]\r\n>>> pd.read_csv(io.BytesIO(rev_rows_b), header=None).iloc[::-1, ::-1].T.reset_index(drop=True).T.reset_index(drop=True)\r\n   0  1                       2\r\n0  1  2                   olleh\r\n1  3  4                   dlrow\r\n2  5  6  zab,01,9\\nrab,8,7\\noof\r\n```\r\n\r\nNote that the column `names` should _not_ be passed to `read_csv` here: if they are, invalid lines will be salvaged (padded with NaN) whereas without providing column names parser errors are thrown as you would want in a validation step.\r\n\r\n```py\r\n>>> bad_rows_bytestr = rows_bytestr + b\"\\nJUST_ONE_COLUMN_NOT_THREE\"\r\n>>> bad_rev_b = bad_rows_bytestr[::-1]\r\n>>> pd.read_csv(io.BytesIO(bad_rev_b), header=None).iloc[::-1, ::-1].T.reset_index(drop=True).T.reset_index(drop=True)\r\nTraceback (most recent call last):\r\n...\r\n  File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 2, saw 3\r\n```\r\n\r\nwhereas if you provide names (let's imagine they come from the sample):\r\n\r\n```py\r\n>>> sample_df = pd.read_csv(io.BytesIO(bytestr))\r\n>>> cols_rev = sample_df.columns[::-1]\r\n>>> pd.read_csv(io.BytesIO(bad_rev_b), names=cols_rev, header=None).iloc[::-1, ::-1].T.reset_index(drop=True).T.reset_index(drop=True)\r\n     0    1                          2\r\n0  1.0  2.0                      olleh\r\n1  3.0  4.0                      dlrow\r\n2  5.0  6.0     zab,01,9\\nrab,8,7\\noof\r\n3  NaN  NaN  EERHT_TON_NMULOC_ENO_TSUJ\r\n```\r\n\r\n- The invalid single column line gets padded with `NaN` rather than throwing an error!\r\n- integers got quietly coerced to floats\r\n\r\nSo don't pass in column names to this validation step.\r\n\r\nLet's say we reach our target number of rows (instead of 10, let's call it 3, so the above example from `rev_rows_b` is completed), and the next newline only has part of a row (again due to an intra-row linesep):\r\n\r\n```py\r\n>>> extra_line_bytestr = b'\\n5,6,\"foo\\n'\r\n>>> overfull_rows_bytestr = rows_bytestr + extra_line_bytestr\r\n>>> overfull_rows_bytestr # 3 valid rows and an invalid (partial) row\r\nb'1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"\\n5,6,\"foo\\n'\r\n>>> pd.read_csv(io.BytesIO(overfull_rows_bytestr), header=None)\r\nTraceback (most recent call last):\r\n...\r\n  File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 3\r\n```\r\n\r\nAs expected, there is a parser error, and this can be overcome by stopping early:\r\n\r\n```py\r\n>>> pd.read_csv(io.BytesIO(overfull_rows_bytestr), header=None, nrows=3)\r\n```\r\n\r\nIn reverse however, the invalid rows are accepted!\r\n\r\n```\r\n>>> rev_over_rows_b = overfull_rows_bytestr[:-1][::-1] + b\"\\n\"\r\n>>> rev_over_rows_b # an invalid (partial) row and 3 valid rows\r\nb'oof\",6,5\\n\"zab,01,9\\nrab,8,7\\noof\",6,5\\ndlrow,4,3\\nolleh,2,1\\n'\r\n>>> pd.read_csv(io.BytesIO(rev_over_rows_b), header=None).iloc[::-1, ::-1].T.reset_index(drop=True).T.reset_index(drop=True)\r\n   0  1                       2\r\n0  1  2                   olleh\r\n1  3  4                   dlrow\r\n2  5  6  zab,01,9\\nrab,8,7\\noof\r\n3  5  6                    oof\"\r\n```\r\n\r\n- This appears to be a bug in pandas: in reverse, you can parse the invalid row (even though it has a lone, unquoted character)\r\n- This could very trivially be checked for (an unescaped quotechar should not be found in any row)\r\n  - I would rather not do this kind of validation when pandas should do it to be honest\r\n\r\n**Since neither the start nor end delimiter can be known, I don't know if this can be so neatly resolved** (TBC)\r\n\r\nThis is not something unique to the first row, it can be pushed onto the second row\r\n\r\n```py\r\n>>> another_valid_row = b\"a,0,0\\n\"\r\n>>> pd.read_csv(io.BytesIO(another_valid_row + rev_over_rows_b), header=None)\r\n                        0  1  2\r\n0                       a  0  0\r\n1                    oof\"  6  5\r\n2  zab,01,9\\nrab,8,7\\noof  6  5\r\n3                   dlrow  4  3\r\n4                   olleh  2  1\r\n```\r\n\r\nIt does seem to be due to being directly before a row with [properly paired] `quotechar`, which perhaps confuses the parser:\r\n\r\n```py\r\n>>> spaced_b = rows_bytestr + b\"\\n\" + another_valid_row[:-1] + extra_line_bytestr\r\n>>> pd.read_csv(io.BytesIO(spaced_b), header=None)\r\nTraceback (most recent call last):\r\n...\r\n  File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 4\r\n\r\n```\r\n\r\nWhen parsed in the forward direction, the partial row is forcibly accepted: this means you do not want to decide if a row is valid or not by putting it at the start of a dataframe\r\n\r\nI don't really understand why this is not a parser error:\r\n\r\n```py\r\n>>> pd.read_csv(io.BytesIO(b'foo\"\\n'), header=None)                 \r\n      0\r\n0  foo\"\r\n```\r\n\r\nHowever the padding only seems to come after a valid row\r\n\r\n```py\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\n'), header=None)\r\n       0      1    2\r\n0  hello  world  etc\r\n1   foo\"    NaN  NaN\r\n>>> pd.read_csv(io.BytesIO(b'foo\"\\nhello,world,etc\\n'), header=None)\r\nTraceback (most recent call last):\r\n...\r\n  File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 2, saw 3\r\n```\r\n\r\nThe fact that they're being `NaN`-padded means they're interpreted as \"blank\", rather than invalid:\r\n\r\n```py\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\n'), header=None, na_filter=False)\r\n       0      1    2\r\n0  hello  world  etc\r\n1   foo\"            \r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), header=None, na_filter=False)\r\n       0      1    2\r\n0  hello  world  etc\r\n1   foo\"            \r\n2    etc    etc  etc\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), header=None, na_filter=False)[2]\r\n0    etc\r\n1       \r\n2    etc\r\nName: 2, dtype: object\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), header=None, na_filter=False)[0][1]\r\n'foo\"'\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), header=None, na_filter=False)[0][0]\r\n'hello'\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), header=None, na_filter=False)[2][0]\r\n'etc'\r\n>>> pd.read_csv(io.BytesIO(b'hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), header=None, na_filter=False)[2][1]\r\n''\r\n```\r\n\r\n- This [doesn't seem to be something pandas is capable of](https://stackoverflow.com/questions/39293118/how-to-get-pandas-to-throw-exception-or-otherwise-note-if-some-fields-are-miss#comment65955466_39293296)\r\n\r\n---\r\n\r\nThere are 2 options then:\r\n\r\n1. 'Manually' validate the input (check if the `quotechar` is in the dataframe values: this is always invalid!)\r\n2. Use the Python csv module (I expect it will be more reliable at validation)\r\n\r\nThe 2nd option isn\u2019t hard:\r\n\r\n```py\r\n>>> import csv\r\n>>> r = csv.reader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n')\r\n>>> for row in r: print(row)\r\n['hello', 'world','etc']\r\n['foo']\r\n['etc', 'etc', 'etc']\r\n>>> r = csv.reader(io.StringIO('foo\"\\n'))\r\n['foo\"']\r\n>>> r = csv.reader(io.StringIO('intA,intB,strC\\n1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"'))\r\n>>> for row in r: print(row)\r\n['intA', 'intB', 'strC'],\r\n['1', '2', 'hello'],\r\n['3', '4', 'world']\r\n['5', '6', 'foo\\n7,8,bar\\n9,10,baz']\r\n>>> r = csv.reader(io.StringIO('1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"\\n5,6,\"foo\\n'))\r\n['1', '2', 'hello'],\r\n['3', '4', 'world']\r\n['5', '6', 'foo\\n7,8,bar\\n9,10,baz']\r\n['5', '6', 'foo\\n']\r\n```\r\n\r\nThis is not actually any more valid: note that the `quotechar` that is \"left open\" on the last line is stripped, whereas pandas leaves it in (where it can at least be identified)\r\n\r\n---\r\n\r\nI expect this routine will look similar to how the `head` is made ([in the `if sample` block at the end of `read_bytes`](https://github.com/dask/dask/blob/e6b9e8983b26389c74036aeec615ac0e5ef34c65/dask/bytes/core.py#L141-L164), which is read by `reader` through a `BytesIO` buffer \u21e2 `head`)\r\n\r\nThe output from this routine is as [outlined for `fsspec-reference-maker`](https://github.com/intake/fsspec-reference-maker/issues/66#issuecomment-900738600):\r\n\r\n> So the _only_ task you need to accomplish, is to find a set of offset/size pairs that can load the data.\r\n\r\n...and from this generate a filespec to pass to a to-be-written CSV module in `fsspec-reference-maker`.\r\n\r\nThe offsets returned from `read_byte` are a guide as to the positions _after_ which to seek to delimiters. So in that respect nothing has changed except we are refining this \"seek to delimiters\" part, making it \"seek to `lineseparator` delimiters which are also row delimiters\" (or equivalently \"`lineseparator` delimiters which are not within a row column\"), and validating that they are indeed row delimiters, by 10 consecutive rows [positive lookbehind condition].\r\n\r\nNote that the routine for sampling the head, per file, works by sampling more and more of the buffer. The value of `sample` (256,000) is the minimum number of bytes to read until finding the `lineseparator`, which in `read_bytes` is referred to as the delimiter, [or if None is given], else it's the chunk size read at each iteration until finding the `lineseparator` delimiter [or the EOF]:\r\n\r\nhttps://github.com/dask/dask/blob/e6b9e8983b26389c74036aeec615ac0e5ef34c65/dask/bytes/core.py#L146-L162\r\n\r\nThe same routine is needed in reverse, per partition, to identify the first valid [i.e. row-delimiting] `lineseparator`.\r\n\r\n</p>\r\n\r\n</details>\r\n\r\n---\r\n\r\n## Part 2 Overview of CSV partition validation\r\n\r\n<details><summary>Click for an overview of the CSV partition validation task</summary>\r\n\r\n<p>\r\n\r\nThe edge case of a shorter row is handled correctly by the CSV module:\r\n\r\n- The `quotechar` is left in, where it can be detected\r\n\r\n```py\r\n>>> r = csv.reader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n'))\r\n>>> for row in r: print(row)\r\n... \r\n['hello', 'world', 'etc']\r\n['foo\"']\r\n['etc', 'etc', 'etc']\r\n```\r\n\r\nOne possibility is a DictReader, which would presumably fail if one column (key) is not present:\r\n\r\n- This correctly supplies `None` when it finds missing values\r\n- This incorrectly defaults to reading the first line as a header row (when `fieldnames=None`)\r\n\r\n```py\r\n>>> r = csv.DictReader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n'))\r\n>>> for row in r: print(row)\r\n... \r\n{'hello': 'foo\"', 'world': None, 'etc': None}\r\n{'hello': 'etc', 'world': 'etc', 'etc': 'etc'}\r\n```\r\n\r\nThis could be avoided however by mandating the use of the `sample_df` earlier (the `head` formed from `b_sample` in dask)\r\n\r\n```py\r\n>>> bytestr = b'intA,intB,strC\\n1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"'\r\n>>> sample_df = pd.read_csv(io.BytesIO(bytestr))\r\n>>> r = csv.DictReader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), fieldnames=sample_df.columns)\r\n>>> for row in r: print(row)\r\n... \r\n{'intA': 'hello', 'intB': 'world', 'strC': 'etc'}\r\n{'intA': 'foo\"', 'intB': None, 'strC': None}\r\n{'intA': 'etc', 'intB': 'etc', 'strC': 'etc'}\r\n```\r\n\r\nWhich is correct!\r\n\r\nThis can also handle the case when the sample does not have a header:\r\n\r\n```py\r\n>>> rows_bytestr = b'1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"'\r\n>>> no_header_sample_df = pd.read_csv(io.BytesIO(rows_bytestr), header=None)\r\n>>> r = csv.DictReader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), fieldnames=no_header_sample_df.columns)\r\n>>> for row in r: print(row)\r\n... \r\n{0: 'hello', 1: 'world', 2: 'etc'}\r\n{0: 'foo\"', 1: None, 2: None}\r\n{0: 'etc', 1: 'etc', 2: 'etc'}\r\n```\r\n\r\nThe keys of the dict are integers rather than strings in that case.\r\n\r\nThe values are the fields, which can be checked for the unescaped quotechar\r\n\r\n```py\r\n>>> r = csv.DictReader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), fieldnames=no_header_sample_df.columns)\r\n>>> for row in r:\r\n...    if any(quotechar in v for v in row.values()):\r\n...        raise ValueError\r\n...    print(row)\r\n... \r\n{0: 'hello', 1: 'world', 2: 'etc'}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\nValueError\r\n```\r\n\r\nActually it has to be a little more precise than just checking if the value contains the quotechar:\r\n\r\n- Not preceded by the `escapechar` (so should use a regex created by an f-string\r\n- If `doublequote` is True, then two consecutive [again, unescaped] `quotechar` should be considered as a single `quotechar`\r\n  - This could be incorporated into the regex or could use `str.replace`\r\n\r\nIf the field has quotes around the entirety of it then these should (will) be stripped, so no need to handle that case.\r\n\r\nSo I think the regex is `rf'[^{escapechar}]{quotechar}\\{1,{1 + int(doublequote)}\\}'`\r\n\r\n```py\r\n>>> escapechar = r'\\\\'\r\n>>> quotechar = '\"'\r\n>>> n_matchable = 1 + int(doublequote)\r\n>>> re_patt_str = rf'([^{escapechar}]{quotechar})' + r'{1,' + f'{n_matchable}' + r'}'\r\n>>> re_patt_str\r\n'([^\\\\\\\\]\"){1,2}'\r\n>>> re_patt = re.compile(re_patt_str)\r\n```\r\n\r\n...but you also want:\r\n\r\n- to match it within a str (so within `.*` either side)\r\n- to identify the `quotechar` itself\r\n\r\nYou could therefore use a second match group inside the repetition matched group:\r\n\r\n```py\r\n>>> re_patt_str = rf'.*([^{escapechar}]({quotechar}))' + r'{1,' + f'{n_matchable}' + r'}.*'\r\n>>> re_patt = re.compile(re_patt_str)\r\n>>> re_patt.match('hello\"foo').groups()\r\n('o\"', '\"')\r\n```\r\n\r\nNote that the outer [repetition] match group includes whichever character precedes. You could also use a negative lookbehind\r\n\r\n```py\r\n>>> rf\"(?<!({escapechar}))\"\r\n'(?<!(\\\\\\\\))'\r\n>>> re_patt_str = rf'.*((?<!({escapechar}))({quotechar}))' + r'{1,' + f'{n_matchable}' + r'}.*'\r\n>>> re_patt = re.compile(re_patt_str)\r\n>>> re_patt\r\nre.compile('.*((?<!(\\\\\\\\))(\")){1,2}.*')\r\n```\r\n\r\nIn full, it works as expected, and does not provide an unwanted non-`escapechar` in the match group (if a negative lookbehind succeeds it provides `None` as the matched entity rather than what was there instead):\r\n\r\n```\r\n>>> re_patt_str = rf'.*((?<!({escapechar}))({quotechar}))' + r'{1,' + f'{n_matchable}' + r'}.*'\r\n>>> re_patt = re.compile(re_patt_str)\r\n>>> re_patt_str\r\n'.*((?<!(\\\\\\\\))(\")){1,2}.*'\r\n>>> re_patt.match(r'hello\\\"\\\"world').groups()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'NoneType' object has no attribute 'groups'\r\n>>> re_patt.match(r'hello\\\"\"world').groups()\r\n('\"', None, '\"')\r\n```\r\n\r\nWe can now specify this to the `csv.DictReader`, and since a match is a 3-tuple and a non-match is `None`, we can simply check against the truthiness of the match call:\r\n\r\n```py\r\n>>> r = csv.DictReader(io.StringIO('hello,world,etc\\nfoo\"\\netc,etc,etc\\n'), fieldnames=no_header_sample_df.columns)\r\n>>> for row in r:\r\n...    if any(re_patt.match(v) for v in row.values()):\r\n...        raise ValueError\r\n...    print(row)\r\n{0: 'hello', 1: 'world', 2: 'etc'}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\nValueError\r\n```\r\n\r\nFully validating would also mean checking that each row has the correct number of columns [i.e. consistent with the `head` sample]. In fact we can make it a self-contained demo now:\r\n\r\n```py\r\nimport csv\r\nimport io\r\nimport re\r\nimport pandas as pd\r\n\r\n__all__ = [\"reader\", \"validate_dictreader\", \"validate_str\"]\r\n\r\nsample_header_bytestr = b'intA,intB,strC\\n'\r\nsample_df = pd.read_csv(io.BytesIO(sample_header_bytestr))\r\n\r\ndef reader(input_str, df=sample_df):\r\n    \"\"\"\r\n    Args:\r\n      input_str : The string to load into a new :class:`io.StringIO` buffer to create\r\n                  the DictReader from\r\n      df        : The DataFrame whose columns give the fieldnames fro the DictReader\r\n    \"\"\"\r\n    buf = io.StringIO(input_str)\r\n    return csv.DictReader(buf, fieldnames=df.columns)\r\n\r\ndef validate_dictreader(r):\r\n    \"\"\"\r\n    Args:\r\n      r : (:class:`csv.DictReader`) The DictReader which can be iterated to give the rows to be validated\r\n          in the form of :class:`dict` objects (one per row).\r\n    \"\"\"\r\n    escapechar = r\"\\\\\"\r\n    quotechar = '\"'\r\n    doublequote = True\r\n    n_matchable = 1 + int(doublequote)\r\n    re_patt_str = rf'.*((?<!({escapechar}))({quotechar}))' + r'{1,' + f'{n_matchable}' + r'}.*'\r\n    re_patt = re.compile(re_patt_str)\r\n    validated_rows = []\r\n    for row in r:\r\n        if None in row.values():\r\n            raise ValueError(f\"Absent field (incomplete row) at {row=}\")\r\n        if any(re_patt.match(v) for v in row.values()):\r\n            raise ValueError(f\"{quotechar=} found in {row=}\")\r\n        validated_rows.append(row)\r\n    for row in validated_rows:\r\n        print(row) # Only print rows if entire DataFrame validated\r\n\r\ndef validate_str(input_str, sample_df=sample_df):\r\n    \"\"\"\r\n    Validate the string ``input_str`` whose column names (or index if no names) are\r\n    available through the ``sample_df`` DataFrame (sampled from the file's head).\r\n    \"\"\"\r\n    r = reader(input_str, df=sample_df)\r\n    validate_dictreader(r)\r\n```\r\n\r\n```py\r\n>>> validate_str(input_str='hello,world,etc\\nfoo\"\\netc,etc,etc\\n')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"validate_rows.py\", line 49, in validate_str\r\n    validate_dictreader(r)\r\n  File \"validate_rows.py\", line 36, in validate_dictreader\r\n    raise ValueError(f\"Absent field (incomplete row) at {row=}\")\r\nValueError: Absent field (incomplete row) at row={'intA': 'foo\"', 'intB': None, 'strC': None}\r\n```\r\n\r\nNote that fields being _present but empty_ is valid.\r\n\r\n```py\r\n>>> validate_str(input_str='1,2,etc\\n3,,\\n5,6,etc\\n')\r\n{'intA': '1', 'intB': '2', 'strC': 'etc'}\r\n{'intA': '3', 'intB': '', 'strC': ''}\r\n{'intA': '5', 'intB': '6', 'strC': 'etc'}\r\n```\r\n\r\nThe other check I wanted of an implementation was that an 'overfull' byte string would be acceptable **if sufficient number of rows were read before the extra/partial line was read in**. Instead of 10, I suggested to pretend this sufficient number of rows was 3, and then use the following example:\r\n\r\n```py\r\n>>> rows_str = '1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"\\n'\r\n>>> extra_line_str = '5,6,\"foo\\n'\r\n>>> overfull_rows_str = rows_str + extra_line_str\r\n>>> overfull_rows_str # 3 valid rows and an invalid (partial) row\r\n'1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"\\n5,6,\"foo\\n'\r\n>>> validate_str(overfull_rows_str)\r\n{'intA': '1', 'intB': '2', 'strC': 'hello'}\r\n{'intA': '3', 'intB': '4', 'strC': 'world'}\r\n{'intA': '5', 'intB': '6', 'strC': 'foo\\n7,8,bar\\n9,10,baz'}\r\n{'intA': '5', 'intB': '6', 'strC': 'foo\\n'}\r\n```\r\n\r\nUnfortunately this does not validate correctly\r\n\r\n</p>\r\n\r\n</details>\r\n\r\n---\r\n\r\n## Part 3: How to validate correctly\r\n\r\n<details><summary>Click to show the conclusions on partitioned CSV validation</summary>\r\n\r\n<p>\r\n\r\nAs shown in the previous section, the csv module's `DictReader` for some reason cleans up the lone `quotechar` in the example `overfull_rows_str`\r\n\r\n```\r\n'1,2,hello\\n3,4,world\\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"\\n5,6,\"foo\\n'\r\n```\r\n\r\nWhich should be split as:\r\n\r\n```\r\n1,2,hello\r\n3,4,world\r\n5,6,\"foo\\n7,8,bar\\n9,10,baz\"\r\n5,6,\"foo\\n\r\n```\r\n\r\nI suspect the only way that it can be ensured to do so is to read the rows one at a time, 'greedily', not lazily (e.g. the regex `?` quantifier). The error here arises from trying to read lazily, and arriving at an invalid row as a result.\r\n\r\n```py\r\n>>> r = csv.reader(io.StringIO(overfull_rows_str))\r\n>>> for row in r: print(row)\r\n...\r\n['1', '2', 'hello']\r\n['3', '4', 'world']\r\n['5', '6', 'foo\\n7,8,bar\\n9,10,baz']\r\n['5', '6', 'foo\\n']\r\n```\r\n\r\nThe real take-away here is that **you can't trust the last row** (it might be unfinished), and for this reason you might want to read in 11 rows and drop the last (to get 10 rows).\r\n\r\n**However**: note that the presence of the open `quotechar` is implicitly indicated by the presence of the newline within the row value (`'foo\\n'`)\r\n- If the row value ends in a newline, it should therefore be considered as (and validated as if) starting with a `quotechar`\r\n\r\nThis means we do not need to consider reading in an 11th line, only to check for the newline (and always making sure that the line we read in is `lineterminator`-terminated, or else we would not get that newline within the row value to check).\r\n\r\nIf we extend the previous example accordingly:\r\n\r\n```py\r\n>>> further_extension_str = 'one,two,three\\n'\r\n>>> further_extended_rows_str = overfull_rows_str + further_extension_str\r\n>>> r = csv.reader(io.StringIO(further_extended_rows_str))\r\n>>> for row in r: print(row)\r\n... \r\n['1', '2', 'hello']\r\n['3', '4', 'world']\r\n['5', '6', 'foo\\n7,8,bar\\n9,10,baz']\r\n['5', '6', 'foo\\none,two,three\\n']\r\n```\r\n\r\nNote that even though the lone `quotechar` on the 3rd row appeared to have been 'missed' (not shown in the row value), it was not missed from the row parsing, and so when further rows are given, the parser is not 'tricked' into believing the values to be on another row, but [correctly] conjoins them with an intra-row newline (`'foo\\n'` \u21d2 `'foo\\none,two,three\\n'`).\r\n\r\n```py\r\n>>> overfull_recovery_str = 'etc\"'\r\n>>> recovered_overfull_rows_str = further_extended_rows_str + overfull_extension_str\r\n>>> r = csv.reader(io.StringIO(recovered_overfull_rows_str))\r\n>>> for row in r: print(row)\r\n...\r\n['1', '2', 'hello']\r\n['3', '4', 'world']\r\n['5', '6', 'foo\\n7,8,bar\\n9,10,baz']\r\n['5', '6', 'foo\\none,two,three\\none,two,three\\netc']\r\n```\r\n\r\nNote that here, the final value has no newline (`lineseparator`) at the end of it, indicating that the multiline string value is closed.\r\n\r\n- Also note that naturally, this will still apply when reading in reverse (since from either direction a single `quotechar` will be picked up). The same problems would be encountered with multiline field values, just in reverse.\r\n\r\nThe last issue to address is the possibility of entering within a multiline string, e.g. consider if the parser picked up _after_ the first of these lines (becoming 'tricked' into treating the lines in all-caps as rows when in fact they are all a continuation of the line `'1,2,\"hello\\n'`):\r\n\r\n```\r\n1,2,\"hello\r\nA,B,C\r\nD,E,F\r\nG,H,I\r\nJ,K,L\"\r\n3,4,etc\r\n```\r\n\r\nOf course when all of these lines are present we can validate it:\r\n\r\n```py\r\n>>> full_str = '1,2,\"hello\\nA,B,C\\nD,E,F\\nG,H,I\\nJ,K,L\"\\n3,4,etc\\n'\r\n>>> validate_str(full_str)\r\n{'intA': '1', 'intB': '2', 'strC': 'hello\\nA,B,C\\nD,E,F\\nG,H,I\\nJ,K,L'}\r\n{'intA': '3', 'intB': '4', 'strC': 'etc'}\r\n```\r\n\r\nbut if we omit the first line:\r\n\r\n```py\r\n>>> trick_str = full_str[11:]\r\n>>> trick_str\r\n'A,B,C\\nD,E,F\\nG,H,I\\nJ,K,L\"\\n3,4,etc\\n'\r\n>>> validate_str(trick_str)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"validate_rows.py\", line 49, in validate_str\r\n    validate_dictreader(r)\r\n  File \"validate_rows.py\", line 38, in validate_dictreader\r\n    raise ValueError(f\"{quotechar=} found in {row=}\")\r\nValueError: quotechar='\"' found in row={'intA': 'J', 'intB': 'K', 'strC': 'L\"'}\r\n```\r\n\r\nThen we need to read through 4 lines until meeting the lone [closing] `quotechar` which raises the parser error.\r\nThis is as expected, and something to bear in mind when interpreting the meaning of this error. It also gives further reason _not_ to reduce the number of rows to sample from the partition tails, as too few risk not exitting the end of the multiline quoted string the parser started within.\r\n\r\nTo illustrate, here's the case when only 3 lines are iterated through (not reaching the `quotechar` on the 4th):\r\n\r\n```py\r\n>>> r = reader(trick_str)\r\n>>> escapechar = r\"\\\\\"\r\n>>> quotechar = '\"'\r\n>>> doublequote = True\r\n>>> n_matchable = 1 + int(doublequote)\r\n>>> re_patt_str = rf'.*((?<!({escapechar}))({quotechar}))' + r'{1,' + f'{n_matchable}' + r'}.*'\r\n>>> re_patt = re.compile(re_patt_str)\r\n>>> validated_rows = []\r\n>>> for row_idx, row in enumerate(r):\r\n>>>     if row_idx == 3: break\r\n>>>     if None in row.values():\r\n>>>         raise ValueError(f\"Absent field (incomplete row) at {row=}\")\r\n>>>     if any(re_patt.match(v) for v in row.values()):\r\n>>>         raise ValueError(f\"{quotechar=} found in {row=}\")\r\n>>>     validated_rows.append(row)\r\n>>>\r\n>>> for row in validated_rows:\r\n>>>     print(row) # Only print rows if entire DataFrame validated\r\n{'intA': 'A', 'intB': 'B', 'strC': 'C'}\r\n{'intA': 'D', 'intB': 'E', 'strC': 'F'}\r\n{'intA': 'G', 'intB': 'H', 'strC': 'I'}\r\n{'intA': '3', 'intB': '4', 'strC': 'etc'}\r\n```\r\n\r\ni.e. it validates the lines incorrectly into rows, as it couldn't distinguish. The only way to avoid this is a sufficiently large (and an assumption that this will be a rare occurrence in real world data).\r\n\r\n---\r\n\r\nThe csv module therefore provides solutions to all of the problems relevant to this task:\r\n\r\n1. Ability to indicate when more bytes are needed to complete a row (which can be read in from a buffer iteratively)\r\n  - Indicated by a newline at the end of the field value [always at the end of a row]\r\n2. Ability to indicate when the start position was within a row\r\n  - Indicated by a `quotechar` within a field value (anywhere)\r\n3. Ability to indicate when the row is not a valid row\r\n  - Indicated by any `None` values in the `DictReader` field values\r\n\r\nThe function given earlier (`validate_str`) is sufficient to do this, except it doesn't check for newlines (as in a real implementation, instead of throwing a ValueError you'd want to read in more bytes, which could be done without discarding the rows you already have(?)\r\n\r\n> If the row value ends in a newline, it should therefore be considered as (and validated as if) starting with a quotechar\r\n\r\n</p>\r\n\r\n</details>",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/902256015/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/902692239",
        "html_url": "https://github.com/dask/dask/issues/8045#issuecomment-902692239",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8045",
        "id": 902692239,
        "node_id": "IC_kwDOAbcwm841zf2P",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-20T13:26:33Z",
        "updated_at": "2021-08-20T13:26:33Z",
        "author_association": "MEMBER",
        "body": "This is quite a lot of information to take in, so before attempting that, a quick question, which of these does your procedure do:\r\n- given a random offset in a CSV file, find the location of valid newline character that is truly a row terminator\r\n- scan a CSV from the beginning to find the locations of all true row-terminating newlines",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/902692239/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/902724187",
        "html_url": "https://github.com/dask/dask/issues/8045#issuecomment-902724187",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8045",
        "id": 902724187,
        "node_id": "IC_kwDOAbcwm841znpb",
        "user": {
            "login": "lmmx",
            "id": 2979452,
            "node_id": "MDQ6VXNlcjI5Nzk0NTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2979452?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lmmx",
            "html_url": "https://github.com/lmmx",
            "followers_url": "https://api.github.com/users/lmmx/followers",
            "following_url": "https://api.github.com/users/lmmx/following{/other_user}",
            "gists_url": "https://api.github.com/users/lmmx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lmmx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lmmx/subscriptions",
            "organizations_url": "https://api.github.com/users/lmmx/orgs",
            "repos_url": "https://api.github.com/users/lmmx/repos",
            "events_url": "https://api.github.com/users/lmmx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lmmx/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-20T14:15:46Z",
        "updated_at": "2021-08-20T14:31:14Z",
        "author_association": "CONTRIBUTOR",
        "body": "Apologies for any information overload! I just wanted it in one place after having reached a conclusion I was confident in.\r\n\r\nI misread your question, it does the former. It does not read an entire file!\r\n\r\nIt would only end up reading an entire file if:\r\n\r\n- the file was a single row extending over multiple lines (if it was a single row on one line or otherwise in one partition then it would be assumed valid and nothing computed: `[0, EOF)` is assumed a valid partition range).\r\n- the file contained 10 or fewer rows per partition and more than 1 partition\r\n\r\nIf a `sample_partition_rows=10` argument were added, note that reducing the value to 0 would turn it off. My impression is dask is not made for such small file partitions however?\r\n\r\nIt does not need to proceed from the start (the algorithm is not sequential/recursive). It can take a random offset in a CSV file (the only other required info is the number of columns).\r\n\r\n- These offsets would be in `range(blocksize, size, blocksize)` (i.e. the ends of all the partitions except the last)\r\n  - e.g. `size=100, blocksize=50` --> 2 partitions, only check end of the first\r\n    - `range(blocksize, size, blocksize) = [50]`\r\n  - e.g. `size=100, blocksize=20` --> 5 partitions, only check ends of first 4\r\n    - `range(blocksize, size, blocksize) = [20, 40, 60, 80]`\r\n\r\n- Above I assumed the number of columns would require the `head` sample DataFrame, but in fact we could reduce that requirement to just the number of columns.\r\n\r\nIn practice however it will run once on an entire file due to the [unlikely] possibility of a partition to extend through one or more entire subsequent partitions (thus 'collapsing'/consuming them). To have confidence in the uniqueness of the row-terminator offsets you should compute them all (not just one)\r\n\r\n- Potentially in parallel but I don't know if I can assume multiprocessing is permitted so would just do it sequentially.\r\n\r\nI plan to reuse the _node visitor/translator callback_ pattern from the `fsspec_reference_maker.hdf` module to keep consistency across the library.\r\n\r\n(P.S. Forgot to mention: routine described above will be done by the `fsspec_reference_maker.csv` module so that dask \"need never know\", as you recommended. But I left the notes here as the purpose of that routine is dask-specific.)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/902724187/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/902942707",
        "html_url": "https://github.com/dask/dask/issues/8045#issuecomment-902942707",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8045",
        "id": 902942707,
        "node_id": "IC_kwDOAbcwm8410c_z",
        "user": {
            "login": "lmmx",
            "id": 2979452,
            "node_id": "MDQ6VXNlcjI5Nzk0NTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2979452?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lmmx",
            "html_url": "https://github.com/lmmx",
            "followers_url": "https://api.github.com/users/lmmx/followers",
            "following_url": "https://api.github.com/users/lmmx/following{/other_user}",
            "gists_url": "https://api.github.com/users/lmmx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lmmx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lmmx/subscriptions",
            "organizations_url": "https://api.github.com/users/lmmx/orgs",
            "repos_url": "https://api.github.com/users/lmmx/repos",
            "events_url": "https://api.github.com/users/lmmx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lmmx/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-20T20:34:30Z",
        "updated_at": "2021-08-20T20:57:40Z",
        "author_association": "CONTRIBUTOR",
        "body": "[Comment removed and moved to the fsspec-reference-maker thread]",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/902942707/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]