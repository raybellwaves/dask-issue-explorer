[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/224102674",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-224102674",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 224102674,
        "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDEwMjY3NA==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2016-06-06T22:03:11Z",
        "updated_at": "2016-06-06T22:03:11Z",
        "author_association": "MEMBER",
        "body": "We would need to find some way to read a sas file out in chunks.  Or were\nyou referring to trying to read multiple sas files?\n\nYou may appreciate\nhttp://dask.readthedocs.io/en/latest/delayed-collections.html\n\nOn Mon, Jun 6, 2016 at 1:36 PM, Albert DeFusco notifications@github.com\nwrote:\n\n> Would it be possible to get a read_sas method for Dask? Is it much more\n> work than read_csv?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/dask/dask/issues/1233, or mute the thread\n> https://github.com/notifications/unsubscribe/AASszNDmJUmy1sKSgzZ5WFnqhK0Bxhajks5qJITcgaJpZM4IvSjp\n> .\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/224102674/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/224112129",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-224112129",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 224112129,
        "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDExMjEyOQ==",
        "user": {
            "login": "AlbertDeFusco",
            "id": 43654,
            "node_id": "MDQ6VXNlcjQzNjU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/43654?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/AlbertDeFusco",
            "html_url": "https://github.com/AlbertDeFusco",
            "followers_url": "https://api.github.com/users/AlbertDeFusco/followers",
            "following_url": "https://api.github.com/users/AlbertDeFusco/following{/other_user}",
            "gists_url": "https://api.github.com/users/AlbertDeFusco/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/AlbertDeFusco/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/AlbertDeFusco/subscriptions",
            "organizations_url": "https://api.github.com/users/AlbertDeFusco/orgs",
            "repos_url": "https://api.github.com/users/AlbertDeFusco/repos",
            "events_url": "https://api.github.com/users/AlbertDeFusco/events{/privacy}",
            "received_events_url": "https://api.github.com/users/AlbertDeFusco/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2016-06-06T22:47:00Z",
        "updated_at": "2016-06-06T22:47:00Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks. Just reading one file with delayed may work. [pd.read_sas](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sas.html) supports chunsize.\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/224112129/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/224120339",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-224120339",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 224120339,
        "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDEyMDMzOQ==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2016-06-06T23:34:12Z",
        "updated_at": "2016-06-06T23:34:12Z",
        "author_association": "MEMBER",
        "body": "What we really need in order to parallelize a single file is a reader that\nprovides blocked random access like, \"please read rows 4000 to 5000\".\n\nOn Mon, Jun 6, 2016 at 3:47 PM, Albert DeFusco notifications@github.com\nwrote:\n\n> Thanks. Just reading one file with delayed may work. pd.read_sas\n> http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sas.html\n> supports chunsize.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/dask/dask/issues/1233#issuecomment-224112129, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AASszG1txukKwhhR8AHZAtlRfPgAhYC8ks5qJKNlgaJpZM4IvSjp\n> .\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/224120339/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/460224167",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-460224167",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 460224167,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDIyNDE2Nw==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-04T12:01:52Z",
        "updated_at": "2019-02-04T12:01:52Z",
        "author_association": "NONE",
        "body": "Hi everybody,\r\nLooks like I'm trying to do the same thing, and I'm running into some issues. I'm a dask.delayed object from pandas.read_sas, then using dask.dataframe.from_delayed. It's failing on the from_delayed step. It works fine on small files (in-memory) when no chunksize is specified, gives a memory error with large files (out-of-memory) also when no chunksize is specified. That makes sense.\r\nWhen I specify a chunksize (1000, 10000) I get the following error regardless of file size:\r\n\r\n\r\n  File \"C:\\tools\\anaconda3\\5.3.0\\lib\\site-packages\\dask\\dataframe\\utils.py\", line 301, in make_meta\r\n    raise TypeError(\"Don't know how to create metadata from {0}\".format(x))\r\n\r\nTypeError: Don't know how to create metadata from <pandas.io.sas.sas7bdat.SAS7BDATReader object at 0x000000000FAA9EB8>\r\n\r\nHas anybody encountered this? I see this issue is still open, but I'm hoping there's at least a work-around.\r\nCheers!\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/460224167/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/460247760",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-460247760",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 460247760,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDI0Nzc2MA==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-04T13:22:02Z",
        "updated_at": "2019-02-04T13:22:02Z",
        "author_association": "NONE",
        "body": "I guess not being able to get metadata from a reader object makes sense, given: \r\n\r\n`    meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\r\n        An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes and\r\n        column names of the output. This metadata is necessary for many\r\n        algorithms in dask dataframe to work.  For ease of use, some\r\n        alternative inputs are also available. Instead of a ``DataFrame``, a\r\n        ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)`` can be\r\n        provided. Instead of a series, a tuple of ``(name, dtype)`` can be\r\n        used. If not provided, dask will try to infer the metadata. This may\r\n        lead to unexpected results, so providing ``meta`` is recommended. For\r\n        more information, see ``dask.dataframe.utils.make_meta``.`\r\n\r\nIf I pass the first row of the sas7bdat file as a dataframe:\r\n`sreader = pd.read_sas(filepath, format='sas7bdat', encoding='iso-8859-1',\r\n                          chunksize=10000, iterator=True)\r\nparts = dask.delayed(pd.read_sas)(filepath, format='sas7bdat',\r\n                                      encoding='iso-8859-1',\r\n                                      chunksize=10000, \r\n                                      iterator=True)\r\nddf = dd.from_delayed(dfs=parts, meta=sreader.read(nrows=1))`\r\n\r\nThen I get the error:\r\n`ValueError: Metadata mismatch found in `from_delayed`.\r\nExpected partition of type `DataFrame` but got `SAS7BDATReader``\r\n\r\nWhich is because pd.read_sas returns a SAS7BDATReader in this case (chunksize not None, iterator=True).\r\nSince the output is to be a pandas.DataFrame, then should the wrapped function still be pd.read_sas? The examples I've seen so far say yes, but I haven't got it working yet.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/460247760/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/460258897",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-460258897",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 460258897,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDI1ODg5Nw==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-04T13:59:02Z",
        "updated_at": "2019-02-04T13:59:02Z",
        "author_association": "MEMBER",
        "body": "So presumably you would need `pd.DataFrame(pd.read_sas)` or something like that?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/460258897/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/460286784",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-460286784",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 460286784,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDI4Njc4NA==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-04T15:18:45Z",
        "updated_at": "2019-02-04T15:18:45Z",
        "author_association": "NONE",
        "body": "`pd.read_sas` returns a `SAS7BDATReader` object, which I expected `dd.from_delayed` could use to iterate through the chunks. `pd.read_csv` returns a `TextFileReader` object, but that gets handled by dask's `make_reader` function (which doesn't seem to play nice with pd.read_sas).\r\nWhat you're saying is that whatever goes into the `dask.delayed()` function has to already have the correct return type (i.e. no readers if we're trying to get DataFrames)? Like I mentioned above, using `pd.read_sas` without a `chunksize` results in a memory error, since it tries to load everything at once and return a `pd.DataFrame`.\r\nThe `dfs` in the `dd.from_delayed` function should be an iterable, and iterating through a `SAS7BDATReader` given by `pd.read_sas` definitely results in `pd.DataFrame`s of `chunksize` size. Must I further wrap my resulting `SAS7BDATReader` before passing it to the `dask.delayed` function?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/460286784/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/460351663",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-460351663",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 460351663,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDM1MTY2Mw==",
        "user": {
            "login": "kylebarron",
            "id": 15164633,
            "node_id": "MDQ6VXNlcjE1MTY0NjMz",
            "avatar_url": "https://avatars.githubusercontent.com/u/15164633?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kylebarron",
            "html_url": "https://github.com/kylebarron",
            "followers_url": "https://api.github.com/users/kylebarron/followers",
            "following_url": "https://api.github.com/users/kylebarron/following{/other_user}",
            "gists_url": "https://api.github.com/users/kylebarron/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kylebarron/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kylebarron/subscriptions",
            "organizations_url": "https://api.github.com/users/kylebarron/orgs",
            "repos_url": "https://api.github.com/users/kylebarron/repos",
            "events_url": "https://api.github.com/users/kylebarron/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kylebarron/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-04T18:09:49Z",
        "updated_at": "2019-02-04T18:09:49Z",
        "author_association": "CONTRIBUTOR",
        "body": "> What we really need in order to parallelize a single file is a reader that\r\n> provides blocked random access like, \"please read rows 4000 to 5000\".\r\n\r\nIf someone ends up trying to implement a PR for this, it may be useful to look at a package that reads SAS files for Spark: https://github.com/saurfang/spark-sas7bdat. I believe that package can provide blocked access.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/460351663/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/460547489",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-460547489",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 460547489,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDU0NzQ4OQ==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-05T08:09:58Z",
        "updated_at": "2019-02-05T08:09:58Z",
        "author_association": "NONE",
        "body": "@kylebarron thanks for the reference. It really feels like there's a straightforward solution with `delayed` and `pd.read_sas`, but I guess I'll have to go down the rabbit hole. I'd love to hear from @AlbertDeFusco regarding what the final implementation was there, if any. ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/460547489/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/461028669",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-461028669",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 461028669,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MTAyODY2OQ==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-06T13:48:13Z",
        "updated_at": "2019-02-06T13:48:13Z",
        "author_association": "NONE",
        "body": "Just returned to post my solution. It's not a proper dd.read_sas method. I realized I'll need to go through a SAS session for most data anyway, so I leveraged that. This solution isn't helpful if you don't actually have SAS, and still relies on running sessions. And it requires `saspy`.\r\n\r\nBasically:\r\n1. Generate a list of tuples that describe the first and last row number of each block you want from your SAS table, `block_tuple_list.`\r\n2. Define a function, `get_block_as_dataframe()`, that uses a tuple to connect to a SAS session and create a subtable containing only these rows, _close the session_, and then return that table as a `pandas.DataFrame`.\r\n3. Make the list of delayed function calls: `dfs = [delayed(get_block_as_dataframe)(tup) for tup in block_tuple_list]`\r\n4. Make the DataFrame: `ddf = dd.from_delayed(dfs)`\r\n\r\nThe problem with this is that you end up creating as many sessions as you have cores, so you could potentially end up choking whatever server is running this.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/461028669/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/461053203",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-461053203",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 461053203,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MTA1MzIwMw==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-06T14:58:12Z",
        "updated_at": "2019-02-06T14:58:12Z",
        "author_association": "MEMBER",
        "body": "@kingfischer16 , that is exactly the kind of pattern we use for SQL, for instance. Of course, the performance of the server for SQL versus SAS is very different, and you are here piping through an extra layer of library which may have its own considerations. Let us know how it goes.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/461053203/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/463562874",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-463562874",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 463562874,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MzU2Mjg3NA==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-14T09:49:06Z",
        "updated_at": "2019-02-14T09:49:06Z",
        "author_association": "NONE",
        "body": "Alright, this definitely works, at least for my specific needs. Data transmission seems to be the biggest bottleneck, so adding a few more filters to the submitted SAS code speeds things up. But the ability to use this dataframe to write a csv file in parallel is pretty awesome :)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/463562874/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/463631311",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-463631311",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 463631311,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MzYzMTMxMQ==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-14T13:43:49Z",
        "updated_at": "2019-02-14T13:43:49Z",
        "author_association": "MEMBER",
        "body": "If you would like to contribute your code as a read_sas_server method (or some suitable name), that would be appreciated. This could come in the form of a full PR with tests, which would need saspy and sas itself, or, simpler, as a gist that others can copy.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/463631311/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/465127478",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-465127478",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 465127478,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ2NTEyNzQ3OA==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-02-19T13:27:56Z",
        "updated_at": "2019-02-19T13:27:56Z",
        "author_association": "NONE",
        "body": "Good idea @martindurant . I haven't forgot about this, I'm still just finishing integrating this function into our library. I'll submit the `get_sas_as_dask_dataframe()` function as a gist.  Since the read speed across our network is so slow, I've ended up using this function to write a csv to my local machine, then handle that with dd.read_csv. Of course if you're low on disk space the dask dataframe still works with direct reference to the SAS server.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/465127478/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/471444822",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-471444822",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 471444822,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ3MTQ0NDgyMg==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-03-11T08:21:58Z",
        "updated_at": "2019-03-11T08:21:58Z",
        "author_association": "NONE",
        "body": "Tom of saspy replied to a question of mine related to this a few weeks ago: https://stackoverflow.com/questions/54653191/saspy-write-large-sas-table-to-local-csv\r\nI have yet to add my gist on reading sas tables into dask dataframes, but I expect his solution is much faster (if you have the local space for it). The latest version of saspy (2.4.3 as of this writing) now contains a 'download' method, letting you write any SAS table to a drive as a CSV. Building a dask df from this is a lot faster than making all those connections and calls to SAS sessions. I haven't implemented this myself, but it will remove a quite a bit of code when/if I do.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/471444822/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/471529618",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-471529618",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 471529618,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ3MTUyOTYxOA==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-03-11T13:03:23Z",
        "updated_at": "2019-03-11T13:03:23Z",
        "author_association": "MEMBER",
        "body": "That might indeed be a good solution to the problem, but I'm not sure it's a good candidate for trying to wrap into a method within Dask: it produces the temporary files, as you say, and also I expect would break in the distributed case. We could still document it, though, for those facing the same situation.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/471529618/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/488765926",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-488765926",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 488765926,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ4ODc2NTkyNg==",
        "user": {
            "login": "kingfischer16",
            "id": 17887920,
            "node_id": "MDQ6VXNlcjE3ODg3OTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17887920?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kingfischer16",
            "html_url": "https://github.com/kingfischer16",
            "followers_url": "https://api.github.com/users/kingfischer16/followers",
            "following_url": "https://api.github.com/users/kingfischer16/following{/other_user}",
            "gists_url": "https://api.github.com/users/kingfischer16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kingfischer16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kingfischer16/subscriptions",
            "organizations_url": "https://api.github.com/users/kingfischer16/orgs",
            "repos_url": "https://api.github.com/users/kingfischer16/repos",
            "events_url": "https://api.github.com/users/kingfischer16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kingfischer16/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-05-02T17:44:17Z",
        "updated_at": "2019-05-02T17:44:17Z",
        "author_association": "NONE",
        "body": "Ok, I finally put the gist online. You can find it here: https://gist.github.com/kingfischer16/0907764bed4fd9619f05929b7b964cdf\r\nLike it says, data transmission time is the limiting factor, but it does let you use SAS tables as a source for dask.DataFrames. The code uses saspy, which only works if you have SAS Base or SAS EG installed on the machine running the code. Cheers!",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/488765926/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/713645983",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-713645983",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 713645983,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxMzY0NTk4Mw==",
        "user": {
            "login": "kepiej",
            "id": 16258942,
            "node_id": "MDQ6VXNlcjE2MjU4OTQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/16258942?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kepiej",
            "html_url": "https://github.com/kepiej",
            "followers_url": "https://api.github.com/users/kepiej/followers",
            "following_url": "https://api.github.com/users/kepiej/following{/other_user}",
            "gists_url": "https://api.github.com/users/kepiej/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kepiej/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kepiej/subscriptions",
            "organizations_url": "https://api.github.com/users/kepiej/orgs",
            "repos_url": "https://api.github.com/users/kepiej/repos",
            "events_url": "https://api.github.com/users/kepiej/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kepiej/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-21T15:08:14Z",
        "updated_at": "2020-10-21T15:08:14Z",
        "author_association": "NONE",
        "body": "Hi,\r\n\r\nI came across [pyreadstat](https://github.com/Roche/pyreadstat#reading-rows-in-chunks) which can read random blocks of a SAS file. It also supports STATA/SPSS files.\r\nThe proposed solution of @kingfischer16 seems to work with pyreadstat:\r\n\r\n```python\r\nimport pyreadstat\r\nimport dask.dataframe as dd\r\nfrom dask.delayed import delayed\r\n\r\ndef dask_sas_reader(filepath, chunksize):\r\n    # Read metadata only of the SAS file in order to find out the number of rows\r\n    _, meta = pyreadstat.read_sas7bdat(filepath, disable_datetime_conversion=True, metadataonly=True)\r\n\r\n    # Helper function which reads a chunk of the SAS file\r\n    def read_sas_chunk(offset):\r\n        df, _ = pyreadstat.read_sas7bdat(filepath, disable_datetime_conversion=True, row_offset=offset, row_limit=chunksize)\r\n        return df\r\n\r\n    # Parallelize reading of chunks using delayed() and combine these in a dask dataframe\r\n    dfs = [delayed(read_sas_chunk)(x) for x in range(0, meta.number_rows, chunksize)]\r\n    return dd.from_delayed(dfs)\r\n\r\ndd_df = dask_sas_reader('my_sas_file.sas7bdat', chunksize=800000)\r\n# Show first 5 rows\r\ndd_df.head()\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/713645983/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/713829401",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-713829401",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 713829401,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxMzgyOTQwMQ==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-21T19:38:19Z",
        "updated_at": "2020-10-21T19:38:19Z",
        "author_association": "MEMBER",
        "body": "I wonder, can you tell: does selecting an offset immediately seek to that file position, or does it need to spool through the rest of the file to get there? Obviously, I don't know much about the file format or how to read it, apologies.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/713829401/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/714301700",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-714301700",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 714301700,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNDMwMTcwMA==",
        "user": {
            "login": "kepiej",
            "id": 16258942,
            "node_id": "MDQ6VXNlcjE2MjU4OTQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/16258942?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kepiej",
            "html_url": "https://github.com/kepiej",
            "followers_url": "https://api.github.com/users/kepiej/followers",
            "following_url": "https://api.github.com/users/kepiej/following{/other_user}",
            "gists_url": "https://api.github.com/users/kepiej/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kepiej/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kepiej/subscriptions",
            "organizations_url": "https://api.github.com/users/kepiej/orgs",
            "repos_url": "https://api.github.com/users/kepiej/repos",
            "events_url": "https://api.github.com/users/kepiej/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kepiej/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-22T07:47:06Z",
        "updated_at": "2020-10-22T07:47:06Z",
        "author_association": "NONE",
        "body": "> \r\n> \r\n> I wonder, can you tell: does selecting an offset immediately seek to that file position, or does it need to spool through the rest of the file to get there? Obviously, I don't know much about the file format or how to read it, apologies.\r\n\r\nI briefly looked through the wrapper code and part of the C code, but I can't immediately find an answer to your question. I also don't know much about the file format unfortunately. Sorry for not being able to be more helpful...",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/714301700/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/714511944",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-714511944",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 714511944,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNDUxMTk0NA==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-22T13:56:10Z",
        "updated_at": "2020-10-22T13:56:10Z",
        "author_association": "MEMBER",
        "body": "You could see whether the last chunk of a file loads as fast as the first, for a file of reasonably large size?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/714511944/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/715249904",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-715249904",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 715249904,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNTI0OTkwNA==",
        "user": {
            "login": "kepiej",
            "id": 16258942,
            "node_id": "MDQ6VXNlcjE2MjU4OTQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/16258942?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kepiej",
            "html_url": "https://github.com/kepiej",
            "followers_url": "https://api.github.com/users/kepiej/followers",
            "following_url": "https://api.github.com/users/kepiej/following{/other_user}",
            "gists_url": "https://api.github.com/users/kepiej/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kepiej/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kepiej/subscriptions",
            "organizations_url": "https://api.github.com/users/kepiej/orgs",
            "repos_url": "https://api.github.com/users/kepiej/repos",
            "events_url": "https://api.github.com/users/kepiej/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kepiej/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-23T10:19:31Z",
        "updated_at": "2020-10-23T10:19:31Z",
        "author_association": "NONE",
        "body": "> \r\n> \r\n> You could see whether the last chunk of a file loads as fast as the first, for a file of reasonably large size?\r\n\r\nOk, I was able to test this using a 25.8GB SAS file on a laptop with 16GB RAM memory in a Jupyter notebook. The file contains 150 000 000 rows. I chose the chunk size corresponding to approximately 1GB of data (i.e., 150 000 000/25 = 6 000 000). I first did:\r\n\r\n````python\r\n_, meta = pyreadstat.read_sas7bdat('my_25GB_sasfile.sas7bdat', disable_datetime_conversion=True, metadataonly=True)\r\n````\r\nIn a separate cell I read the first chunk and time it:\r\n\r\n````python\r\n%%timeit\r\n# Time reading of first chunk of file\r\ndf, _ = pyreadstat.read_sas7bdat('my_25GB_sasfile.sas7bdat', disable_datetime_conversion=True, row_limit=6000000)\r\ndf.head()\r\n````\r\n42.7 s \u00b1 6.32 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nThen, I time reading the final chunk of the file in a separate cell:\r\n\r\n````python\r\n%%timeit\r\n# Time reading of last chunk of file\r\ndf, _ = pyreadstat.read_sas7bdat('my_25GB_sasfile.sas7bdat', disable_datetime_conversion=True, row_offset=meta.number_rows - 6000000, row_limit=6000000)\r\ndf.head()\r\n````\r\n1min 42s \u00b1 766 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIt seems there's definitely an extra overhead between reading both chunks.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/715249904/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1399045042",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1399045042",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1399045042,
        "node_id": "IC_kwDOAbcwm85TY7uy",
        "user": {
            "login": "amcenroe",
            "id": 19820899,
            "node_id": "MDQ6VXNlcjE5ODIwODk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/19820899?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/amcenroe",
            "html_url": "https://github.com/amcenroe",
            "followers_url": "https://api.github.com/users/amcenroe/followers",
            "following_url": "https://api.github.com/users/amcenroe/following{/other_user}",
            "gists_url": "https://api.github.com/users/amcenroe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/amcenroe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/amcenroe/subscriptions",
            "organizations_url": "https://api.github.com/users/amcenroe/orgs",
            "repos_url": "https://api.github.com/users/amcenroe/repos",
            "events_url": "https://api.github.com/users/amcenroe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/amcenroe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-20T22:54:17Z",
        "updated_at": "2023-01-20T22:54:17Z",
        "author_association": "NONE",
        "body": "> > You could see whether the last chunk of a file loads as fast as the first, for a file of reasonably large size?\r\n> \r\n> Ok, I was able to test this using a 25.8GB SAS file on a laptop with 16GB RAM memory in a Jupyter notebook. The file contains 150 000 000 rows. I chose the chunk size corresponding to approximately 1GB of data (i.e., 150 000 000/25 = 6 000 000). I first did:\r\n> \r\n> ```python\r\n> _, meta = pyreadstat.read_sas7bdat('my_25GB_sasfile.sas7bdat', disable_datetime_conversion=True, metadataonly=True)\r\n> ```\r\n> \r\n> In a separate cell I read the first chunk and time it:\r\n> \r\n> ```python\r\n> %%timeit\r\n> # Time reading of first chunk of file\r\n> df, _ = pyreadstat.read_sas7bdat('my_25GB_sasfile.sas7bdat', disable_datetime_conversion=True, row_limit=6000000)\r\n> df.head()\r\n> ```\r\n> \r\n> 42.7 s \u00b1 6.32 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n> \r\n> Then, I time reading the final chunk of the file in a separate cell:\r\n> \r\n> ```python\r\n> %%timeit\r\n> # Time reading of last chunk of file\r\n> df, _ = pyreadstat.read_sas7bdat('my_25GB_sasfile.sas7bdat', disable_datetime_conversion=True, row_offset=meta.number_rows - 6000000, row_limit=6000000)\r\n> df.head()\r\n> ```\r\n> \r\n> 1min 42s \u00b1 766 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n> \r\n> It seems there's definitely an extra overhead between reading both chunks.\r\n\r\nQuestions, when you specify row limit, is it means open sas file partially, open until got row you want then close file or load all then perform some kind like sas_df.head(100)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1399045042/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1477043631",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1477043631",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1477043631,
        "node_id": "IC_kwDOAbcwm85YCeWv",
        "user": {
            "login": "yessine-zghal",
            "id": 74101918,
            "node_id": "MDQ6VXNlcjc0MTAxOTE4",
            "avatar_url": "https://avatars.githubusercontent.com/u/74101918?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yessine-zghal",
            "html_url": "https://github.com/yessine-zghal",
            "followers_url": "https://api.github.com/users/yessine-zghal/followers",
            "following_url": "https://api.github.com/users/yessine-zghal/following{/other_user}",
            "gists_url": "https://api.github.com/users/yessine-zghal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yessine-zghal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yessine-zghal/subscriptions",
            "organizations_url": "https://api.github.com/users/yessine-zghal/orgs",
            "repos_url": "https://api.github.com/users/yessine-zghal/repos",
            "events_url": "https://api.github.com/users/yessine-zghal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yessine-zghal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-20T22:46:03Z",
        "updated_at": "2023-03-20T22:46:03Z",
        "author_association": "NONE",
        "body": "hello I have a 58 GB of xpt file I want to read it and do statistics hypotheses is any solution (I have a limited RAM 12 GB).",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1477043631/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1477953474",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1477953474",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1477953474,
        "node_id": "IC_kwDOAbcwm85YF8fC",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-21T14:35:51Z",
        "updated_at": "2023-03-21T14:35:51Z",
        "author_association": "MEMBER",
        "body": "@yessine-zghal , dask still has not implemented a SAS reader, I suppose due to lack of motivation. I think there is enough in the thread above for you to have a go: either using dask, as described, or with the offset parameter directly, to read just one smaller chunk for processing at a time. Given your memory constraints, the simple non-dask approach is probably what you want.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1477953474/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1483747584",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1483747584",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1483747584,
        "node_id": "IC_kwDOAbcwm85YcDEA",
        "user": {
            "login": "yessine-zghal",
            "id": 74101918,
            "node_id": "MDQ6VXNlcjc0MTAxOTE4",
            "avatar_url": "https://avatars.githubusercontent.com/u/74101918?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yessine-zghal",
            "html_url": "https://github.com/yessine-zghal",
            "followers_url": "https://api.github.com/users/yessine-zghal/followers",
            "following_url": "https://api.github.com/users/yessine-zghal/following{/other_user}",
            "gists_url": "https://api.github.com/users/yessine-zghal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yessine-zghal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yessine-zghal/subscriptions",
            "organizations_url": "https://api.github.com/users/yessine-zghal/orgs",
            "repos_url": "https://api.github.com/users/yessine-zghal/repos",
            "events_url": "https://api.github.com/users/yessine-zghal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yessine-zghal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-25T07:05:44Z",
        "updated_at": "2023-03-25T07:05:44Z",
        "author_association": "NONE",
        "body": "Can you recommend a way to help me read this data? I would make the same statistics on it I need to read all it?\r\nand I think converting to CSV file to use Dask but the conversion takes a lot of time @martindurant Thank you for your response",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1483747584/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1493226678",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1493226678",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1493226678,
        "node_id": "IC_kwDOAbcwm85ZANS2",
        "user": {
            "login": "yessine-zghal",
            "id": 74101918,
            "node_id": "MDQ6VXNlcjc0MTAxOTE4",
            "avatar_url": "https://avatars.githubusercontent.com/u/74101918?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yessine-zghal",
            "html_url": "https://github.com/yessine-zghal",
            "followers_url": "https://api.github.com/users/yessine-zghal/followers",
            "following_url": "https://api.github.com/users/yessine-zghal/following{/other_user}",
            "gists_url": "https://api.github.com/users/yessine-zghal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yessine-zghal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yessine-zghal/subscriptions",
            "organizations_url": "https://api.github.com/users/yessine-zghal/orgs",
            "repos_url": "https://api.github.com/users/yessine-zghal/repos",
            "events_url": "https://api.github.com/users/yessine-zghal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yessine-zghal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-02T05:18:26Z",
        "updated_at": "2023-04-02T05:18:26Z",
        "author_association": "NONE",
        "body": "@martindurant \r\nwhen i used a server with 160 GB of ram and i use this code to read an xpt file with chunk it takes time to read all the chunks and he didn't stop  . I don't know what is the solution to convert .xpt to csv to use dask after that  . Then can you recommend me a solution to convert this big file 58 GB to csv to use dask i need all the data to make descriptive and advanced statistics .\r\nthis is the code :\r\n\r\nimport pyreadstat\r\nimport pandas as pd\r\nimport gc\r\n# Specify the file path\r\nfile_path = 'VS.xpt'\r\n\r\n# Set the chunk size (number of rows to read per iteration)\r\nchunksize = 6000000\r\n\r\n# Initialize an empty dataframe to store the data\r\ndf = pd.DataFrame()\r\n\r\n# Initialize the row counter\r\ni = 0\r\n\r\n# Loop through the file in chunks\r\nwhile True:\r\n    # Read the chunk\r\n    data_iterator_i = pyreadstat.read_xport(file_path, row_offset=i*chunksize, row_limit=chunksize)\r\n    chunk_df = data_iterator_i[0]\r\n    \r\n    # If the chunk is empty, we've reached the end of the file\r\n    if chunk_df.empty:\r\n        break\r\n    \r\n    # Append the chunk to the main dataframe\r\n    df = pd.concat([df, chunk_df], ignore_index=True)\r\n    \r\n    # Release memory\r\n    del chunk_df\r\n    gc.collect()\r\n    \r\n    # Increment the row counter\r\n    i += 1\r\n    print(i)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1493226678/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1493328903",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1493328903",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1493328903,
        "node_id": "IC_kwDOAbcwm85ZAmQH",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-02T13:13:28Z",
        "updated_at": "2023-04-02T13:13:28Z",
        "author_association": "MEMBER",
        "body": "Please, do not load all the data into a single dataframe in memory, you know it doesn't fit! For each loop, immediately write out the chunk to another format like parquet or csv, in a unique file, and then forget that particular chunk and move on.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1493328903/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1493331070",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1493331070",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1493331070,
        "node_id": "IC_kwDOAbcwm85ZAmx-",
        "user": {
            "login": "amcenroe",
            "id": 19820899,
            "node_id": "MDQ6VXNlcjE5ODIwODk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/19820899?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/amcenroe",
            "html_url": "https://github.com/amcenroe",
            "followers_url": "https://api.github.com/users/amcenroe/followers",
            "following_url": "https://api.github.com/users/amcenroe/following{/other_user}",
            "gists_url": "https://api.github.com/users/amcenroe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/amcenroe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/amcenroe/subscriptions",
            "organizations_url": "https://api.github.com/users/amcenroe/orgs",
            "repos_url": "https://api.github.com/users/amcenroe/repos",
            "events_url": "https://api.github.com/users/amcenroe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/amcenroe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-02T13:22:24Z",
        "updated_at": "2023-04-02T13:22:24Z",
        "author_association": "NONE",
        "body": "Or maybe you can to try chunk with saspy instead :), with a help of future concurent to make it faster\n\nGet Outlook for Android<https://aka.ms/AAb9ysg>\n________________________________\nFrom: Martin Durant ***@***.***>\nSent: Sunday, April 2, 2023 8:13:39 PM\nTo: dask/dask ***@***.***>\nCc: Andika Mcenroe ***@***.***>; Comment ***@***.***>\nSubject: Re: [dask/dask] dd.read_sas() (#1233)\n\n\nPlease, do not load all the data into a single dataframe in memory, you know it doesn't fit! For each loop, immediately write out the chunk to another format like parquet or csv, in a unique file, and then forget that particular chunk and move on.\n\n\u2014\nReply to this email directly, view it on GitHub<https://github.com/dask/dask/issues/1233#issuecomment-1493328903>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEXHCY66HM6BNG5KAFCBBDDW7F3QHANCNFSM4CF5FDUQ>.\nYou are receiving this because you commented.Message ID: ***@***.***>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1493331070/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1493385918",
        "html_url": "https://github.com/dask/dask/issues/1233#issuecomment-1493385918",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/1233",
        "id": 1493385918,
        "node_id": "IC_kwDOAbcwm85ZA0K-",
        "user": {
            "login": "yessine-zghal",
            "id": 74101918,
            "node_id": "MDQ6VXNlcjc0MTAxOTE4",
            "avatar_url": "https://avatars.githubusercontent.com/u/74101918?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yessine-zghal",
            "html_url": "https://github.com/yessine-zghal",
            "followers_url": "https://api.github.com/users/yessine-zghal/followers",
            "following_url": "https://api.github.com/users/yessine-zghal/following{/other_user}",
            "gists_url": "https://api.github.com/users/yessine-zghal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yessine-zghal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yessine-zghal/subscriptions",
            "organizations_url": "https://api.github.com/users/yessine-zghal/orgs",
            "repos_url": "https://api.github.com/users/yessine-zghal/repos",
            "events_url": "https://api.github.com/users/yessine-zghal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yessine-zghal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-02T16:32:38Z",
        "updated_at": "2023-04-02T16:32:38Z",
        "author_association": "NONE",
        "body": "@martindurant  after creating csv files(each chunks is a csv data) form the original data(.xpt) can I use Dask to acces for all this chunk to read it and to make a single data like xpt but in csv format for stat ?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1493385918/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]