{
    "url": "https://api.github.com/repos/dask/dask/issues/8480",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8480/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8480/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8480/events",
    "html_url": "https://github.com/dask/dask/issues/8480",
    "id": 1079018198,
    "node_id": "I_kwDOAbcwm85AUILW",
    "number": 8480,
    "title": "`test_scheduler_highlevel_graph_unpack_import` flaky",
    "user": {
        "login": "jrbourbeau",
        "id": 11656932,
        "node_id": "MDQ6VXNlcjExNjU2OTMy",
        "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jrbourbeau",
        "html_url": "https://github.com/jrbourbeau",
        "followers_url": "https://api.github.com/users/jrbourbeau/followers",
        "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}",
        "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions",
        "organizations_url": "https://api.github.com/users/jrbourbeau/orgs",
        "repos_url": "https://api.github.com/users/jrbourbeau/repos",
        "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jrbourbeau/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1887344368,
            "node_id": "MDU6TGFiZWwxODg3MzQ0MzY4",
            "url": "https://api.github.com/repos/dask/dask/labels/tests",
            "name": "tests",
            "color": "a0f9b4",
            "default": false,
            "description": "Unit tests and/or continuous integration"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2021-12-13T20:55:50Z",
    "updated_at": "2022-02-16T20:11:41Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "From time to time `dask/tests/test_layers.py::test_scheduler_highlevel_graph_unpack_import` will fail in CI with the following error (see the traceback further below):\r\n\r\n```python\r\n                # Check whether we imported `lib` on the scheduler\r\n>               assert not any(module.startswith(lib) for module in new_modules)\r\nE               assert not True\r\nE                +  where True = any(<generator object test_scheduler_highlevel_graph_unpack_import.<locals>.<genexpr> at 0x0000017314F18C80>)\r\n```\r\n\r\n<details>\r\n<summary>Full traceback:</summary>\r\n\r\n```python\r\n================================== FAILURES ===================================\r\n___ test_scheduler_highlevel_graph_unpack_import[False-_pq_pyarrow-pandas.] ___\r\n[gw1] win32 -- Python 3.8.12 C:\\Miniconda3\\envs\\test-environment\\python.exe\r\n\r\nop = <function _pq_pyarrow at 0x000001730E265670>, lib = 'pandas.'\r\noptimize_graph = False\r\nloop = <tornado.platform.asyncio.AsyncIOLoop object at 0x00000173150F8730>\r\ntmpdir = local('C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-runneradmin\\\\pytest-0\\\\popen-gw1\\\\test_scheduler_highlevel_graph6')\r\n\r\n    @pytest.mark.parametrize(\r\n        \"op,lib\",\r\n        [\r\n            (_dataframe_shuffle, \"pandas.\"),\r\n            (_dataframe_broadcast_join, \"pandas.\"),\r\n            (_pq_pyarrow, \"pandas.\"),\r\n            (_pq_fastparquet, \"pandas.\"),\r\n            (_read_csv, \"pandas.\"),\r\n            (_array_creation, \"numpy.\"),\r\n            (_array_map_overlap, \"numpy.\"),\r\n        ],\r\n    )\r\n    @pytest.mark.parametrize(\"optimize_graph\", [True, False])\r\n    def test_scheduler_highlevel_graph_unpack_import(op, lib, optimize_graph, loop, tmpdir):\r\n        # Test that array/dataframe-specific modules are not imported\r\n        # on the scheduler when an HLG layers are unpacked/materialized.\r\n    \r\n        with cluster(scheduler_kwargs={\"plugins\": [SchedulerImportCheck(lib)]}) as (\r\n            scheduler,\r\n            workers,\r\n        ):\r\n            with Client(scheduler[\"address\"], loop=loop) as c:\r\n                # Perform a computation using a HighLevelGraph Layer\r\n                c.compute(op(tmpdir), optimize_graph=optimize_graph)\r\n    \r\n                # Get the new modules which were imported on the scheduler during the computation\r\n                end_modules = c.run_on_scheduler(lambda: set(sys.modules))\r\n                start_modules = c.run_on_scheduler(\r\n                    lambda dask_scheduler: dask_scheduler.plugins[\r\n                        SchedulerImportCheck.name\r\n                    ].start_modules\r\n                )\r\n                new_modules = end_modules - start_modules\r\n    \r\n                # Check that the scheduler didn't start with `lib`\r\n                # (otherwise we arent testing anything)\r\n                assert not any(module.startswith(lib) for module in start_modules)\r\n    \r\n                # Check whether we imported `lib` on the scheduler\r\n>               assert not any(module.startswith(lib) for module in new_modules)\r\nE               assert not True\r\nE                +  where True = any(<generator object test_scheduler_highlevel_graph_unpack_import.<locals>.<genexpr> at 0x0000017314F18C80>)\r\n\r\ndask\\tests\\test_layers.py:188: AssertionError\r\n---------------------------- Captured stderr call -----------------------------\r\ndistributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\r\n\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\distributed\\node.py:160: UserWarning: Port 8787 is already in use.\r\n\r\nPerhaps you already have a cluster running?\r\n\r\nHosting the HTTP server on port 53842 instead\r\n\r\n  warnings.warn(\r\n\r\ndistributed.scheduler - INFO - Clear task state\r\n\r\ndistributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:53843\r\n\r\ndistributed.scheduler - INFO -   dashboard at:           127.0.0.1:53842\r\n\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:53844\r\n\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:53844\r\n\r\ndistributed.worker - INFO -          dashboard at:            127.0.0.1:53845\r\n\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:53843\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.worker - INFO -               Threads:                          1\r\n\r\ndistributed.worker - INFO -                Memory:                   7.00 GiB\r\n\r\ndistributed.worker - INFO -       Local Directory: D:\\a\\dask\\dask\\_test_worker-2d864e31-db54-4f34-8d0e-0a362ad8a84b\\dask-worker-space\\worker-vluf95em\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:53844', name: tcp://127.0.0.1:53844, status: undefined, memory: 0, processing: 0>\r\n\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:53843\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:53844\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:53851\r\n\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:53851\r\n\r\ndistributed.worker - INFO -          dashboard at:            127.0.0.1:53852\r\n\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:53843\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.worker - INFO -               Threads:                          1\r\n\r\ndistributed.worker - INFO -                Memory:                   7.00 GiB\r\n\r\ndistributed.worker - INFO -       Local Directory: D:\\a\\dask\\dask\\_test_worker-52400da1-4b9c-4e1b-92e2-6b03a81db034\\dask-worker-space\\worker-wdah6tcy\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:53851', name: tcp://127.0.0.1:53851, status: undefined, memory: 0, processing: 0>\r\n\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:53851\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:53843\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.scheduler - INFO - Receive client connection: Client-e6d1e9ae-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO - Run out-of-band function 'lambda'\r\n\r\ndistributed.worker - INFO - Run out-of-band function 'lambda'\r\n\r\ndistributed.scheduler - INFO - Remove client Client-e6d1e9ae-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.scheduler - INFO - Remove client Client-e6d1e9ae-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.scheduler - INFO - Close client connection: Client-e6d1e9ae-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.worker - INFO - Stopping worker at tcp://127.0.0.1:53851\r\n\r\ndistributed.worker - INFO - Stopping worker at tcp://127.0.0.1:53844\r\n\r\ndistributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:53844', name: tcp://127.0.0.1:53844, status: running, memory: 0, processing: 0>\r\n\r\ndistributed.core - INFO - Removing comms to tcp://127.0.0.1:53844\r\n\r\n____ test_scheduler_highlevel_graph_unpack_import[False-_read_csv-pandas.] ____\r\n[gw1] win32 -- Python 3.8.12 C:\\Miniconda3\\envs\\test-environment\\python.exe\r\n\r\nop = <function _read_csv at 0x000001730E265790>, lib = 'pandas.'\r\noptimize_graph = False\r\nloop = <tornado.platform.asyncio.AsyncIOLoop object at 0x0000017314F577F0>\r\ntmpdir = local('C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-runneradmin\\\\pytest-0\\\\popen-gw1\\\\test_scheduler_highlevel_graph8')\r\n\r\n    @pytest.mark.parametrize(\r\n        \"op,lib\",\r\n        [\r\n            (_dataframe_shuffle, \"pandas.\"),\r\n            (_dataframe_broadcast_join, \"pandas.\"),\r\n            (_pq_pyarrow, \"pandas.\"),\r\n            (_pq_fastparquet, \"pandas.\"),\r\n            (_read_csv, \"pandas.\"),\r\n            (_array_creation, \"numpy.\"),\r\n            (_array_map_overlap, \"numpy.\"),\r\n        ],\r\n    )\r\n    @pytest.mark.parametrize(\"optimize_graph\", [True, False])\r\n    def test_scheduler_highlevel_graph_unpack_import(op, lib, optimize_graph, loop, tmpdir):\r\n        # Test that array/dataframe-specific modules are not imported\r\n        # on the scheduler when an HLG layers are unpacked/materialized.\r\n    \r\n        with cluster(scheduler_kwargs={\"plugins\": [SchedulerImportCheck(lib)]}) as (\r\n            scheduler,\r\n            workers,\r\n        ):\r\n            with Client(scheduler[\"address\"], loop=loop) as c:\r\n                # Perform a computation using a HighLevelGraph Layer\r\n                c.compute(op(tmpdir), optimize_graph=optimize_graph)\r\n    \r\n                # Get the new modules which were imported on the scheduler during the computation\r\n                end_modules = c.run_on_scheduler(lambda: set(sys.modules))\r\n                start_modules = c.run_on_scheduler(\r\n                    lambda dask_scheduler: dask_scheduler.plugins[\r\n                        SchedulerImportCheck.name\r\n                    ].start_modules\r\n                )\r\n                new_modules = end_modules - start_modules\r\n    \r\n                # Check that the scheduler didn't start with `lib`\r\n                # (otherwise we arent testing anything)\r\n                assert not any(module.startswith(lib) for module in start_modules)\r\n    \r\n                # Check whether we imported `lib` on the scheduler\r\n>               assert not any(module.startswith(lib) for module in new_modules)\r\nE               assert not True\r\nE                +  where True = any(<generator object test_scheduler_highlevel_graph_unpack_import.<locals>.<genexpr> at 0x000001731435D4A0>)\r\n\r\ndask\\tests\\test_layers.py:188: AssertionError\r\n---------------------------- Captured stderr call -----------------------------\r\ndistributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\r\n\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\distributed\\node.py:160: UserWarning: Port 8787 is already in use.\r\n\r\nPerhaps you already have a cluster running?\r\n\r\nHosting the HTTP server on port 53912 instead\r\n\r\n  warnings.warn(\r\n\r\ndistributed.scheduler - INFO - Clear task state\r\n\r\ndistributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:53913\r\n\r\ndistributed.scheduler - INFO -   dashboard at:           127.0.0.1:53912\r\n\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:53914\r\n\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:53914\r\n\r\ndistributed.worker - INFO -          dashboard at:            127.0.0.1:53915\r\n\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:53913\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.worker - INFO -               Threads:                          1\r\n\r\ndistributed.worker - INFO -                Memory:                   7.00 GiB\r\n\r\ndistributed.worker - INFO -       Local Directory: D:\\a\\dask\\dask\\_test_worker-8101a517-fdfe-49d9-a1b7-96142cd7142f\\dask-worker-space\\worker-w913isno\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:53914', name: tcp://127.0.0.1:53914, status: undefined, memory: 0, processing: 0>\r\n\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:53914\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:53913\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:53921\r\n\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:53921\r\n\r\ndistributed.worker - INFO -          dashboard at:            127.0.0.1:53922\r\n\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:53913\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.worker - INFO -               Threads:                          1\r\n\r\ndistributed.worker - INFO -                Memory:                   7.00 GiB\r\n\r\ndistributed.worker - INFO -       Local Directory: D:\\a\\dask\\dask\\_test_worker-05a9730f-b3ce-4f8a-88b8-35fa94c0a60d\\dask-worker-space\\worker-abxa6lx5\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:53921', name: tcp://127.0.0.1:53921, status: undefined, memory: 0, processing: 0>\r\n\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:53921\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:53913\r\n\r\ndistributed.worker - INFO - -------------------------------------------------\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.scheduler - INFO - Receive client connection: Client-ee1c03db-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.core - INFO - Starting established connection\r\n\r\ndistributed.worker - INFO - Run out-of-band function 'lambda'\r\n\r\ndistributed.worker - INFO - Run out-of-band function 'lambda'\r\n\r\ndistributed.scheduler - INFO - Remove client Client-ee1c03db-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.scheduler - INFO - Remove client Client-ee1c03db-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.scheduler - INFO - Close client connection: Client-ee1c03db-5c41-11ec-9070-6045bd7a8291\r\n\r\ndistributed.worker - INFO - Stopping worker at tcp://127.0.0.1:53914\r\n\r\ndistributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:53914', name: tcp://127.0.0.1:53914, status: running, memory: 0, processing: 0>\r\n\r\ndistributed.core - INFO - Removing comms to tcp://127.0.0.1:53914\r\n\r\ndistributed.worker - INFO - Stopping worker at tcp://127.0.0.1:53921\r\n\r\ndistributed.scheduler - INFO - Scheduler closing...\r\n```\r\n\r\n</details>",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8480/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8480/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}