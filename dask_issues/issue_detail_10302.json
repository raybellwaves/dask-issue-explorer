{
    "url": "https://api.github.com/repos/dask/dask/issues/10302",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10302/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10302/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10302/events",
    "html_url": "https://github.com/dask/dask/issues/10302",
    "id": 1717469240,
    "node_id": "I_kwDOAbcwm85mXoA4",
    "number": 10302,
    "title": "`read_csv` with `include_path_column` and `dtype_backend='pyarrow'` generates a mix of String and Categorical which hurts Parquet usage",
    "user": {
        "login": "ianozsvald",
        "id": 273210,
        "node_id": "MDQ6VXNlcjI3MzIxMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/273210?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ianozsvald",
        "html_url": "https://github.com/ianozsvald",
        "followers_url": "https://api.github.com/users/ianozsvald/followers",
        "following_url": "https://api.github.com/users/ianozsvald/following{/other_user}",
        "gists_url": "https://api.github.com/users/ianozsvald/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ianozsvald/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ianozsvald/subscriptions",
        "organizations_url": "https://api.github.com/users/ianozsvald/orgs",
        "repos_url": "https://api.github.com/users/ianozsvald/repos",
        "events_url": "https://api.github.com/users/ianozsvald/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ianozsvald/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        },
        {
            "id": 3798602129,
            "node_id": "LA_kwDOAbcwm87iahGR",
            "url": "https://api.github.com/repos/dask/dask/labels/enhancement",
            "name": "enhancement",
            "color": "C2E0C6",
            "default": true,
            "description": "Improve existing functionality or make things work better"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2023-05-19T15:55:15Z",
    "updated_at": "2023-06-19T01:59:42Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "**Describe the issue**:\r\n\r\nUsing `read_csv` to read CSVs as `pyarrow` dtypes with  `dtype_backend='pyarrow'`, then outputting a Parquet file set gives us a mix of PyArrow and `categorical` outputs. The categorical is the added `path` if `include_path_column=True`. \r\n\r\nPerhaps if `dtype_backend='pyarrow'` the behaviour of `include_path_column=True` should use a PyArrow string, not a `categorical`?\r\n\r\nThis came up when trying to load my Parquet output from Dask into Polars: https://github.com/pola-rs/polars/issues/8925\r\n\r\nBy subsequently removing `include_path_column=True` from the `dd.read_csv`, the pipeline generates an all-PyArrow output with no categorical (i.e. no `path` column but the 14 expected columns are all present).\r\n\r\nHaving removed `include_path_column` from this Parquet output, the resulting Parquet file-set can be read with Polars with no need to sub-select away the \"path\" column.\r\n\r\n**Partial example**:\r\n\r\n```python\r\n\r\n# define a function to read each csv file\r\ndef read_results(file, sep):\r\n    df = dd.read_csv(file,\r\n                     delimiter=sep,\r\n                     doublequote=False,\r\n                     on_bad_lines='warn',\r\n                     include_path_column=True, # FORCES a categorical column\r\n                     parse_dates=['test_date'],\r\n                     dtype_backend='pyarrow', # REQUEST all pyarrow dtypes and no categoricals\r\n                    )\r\n    return df\r\n\r\n# some lines ommitted for date parsing and binding read_results to previously sniffed varying csv formats...\r\n\r\n# inferred dtypes are all pyarrow as expected\r\nddf.dtypes\r\ntest_id               int64[pyarrow]\r\ntest_date             datetime64[ns]\r\ntest_class_id         int64[pyarrow]\r\nfuel_type            string[pyarrow]\r\n...14 in total, clipped for brevity, all pyarrow types\r\ndtype: object\r\n\r\n# generate the parquet output, let it process for 10 minutes\r\nf = ddf.to_parquet('test_result.parquet', write_index=False, overwrite=True, compute=False).persist()\r\nprogress(f)\r\n\r\n\r\n# now roundtrip the result back in and check what we've got\r\nddf_result = dd.read_parquet('test_result.parquet')\r\nddf_result.info(verbose=True)\r\n<class 'dask.dataframe.core.DataFrame'>\r\nIndex: 639506962 entries, 0 to 791798\r\nData columns (total 15 columns):\r\n #   Column             Non-Null Count  Dtype\r\n---  ------             --------------  -----\r\n 0   test_id            639506962 non-null      int64[pyarrow]\r\n 2   test_date          639506962 non-null      datetime64[ns]\r\n 3   test_class_id      639506962 non-null      int64[pyarrow]\r\n11   fuel_type          639506962 non-null      string\r\n...14 in total as before and all pyarrow until...\r\n14   path               639506962 non-null      category  # UNEXPECTED non-arrow category\r\ndtypes: category(1), datetime64[ns](2), int64[pyarrow](5), string(7)\r\nmemory usage: 65.2 GB\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nI'm guessing this is old-and-sensible behaviour from when NumPy was the default in Pandas but now that Arrow is a first-class citizen, maybe this behaviour needs updating?\r\n\r\n**Environment**:\r\n\r\n- Dask version: '2023.5.0'\r\n- Python version: 3.11\r\n- Operating System: Linux Mint, Ubuntu\r\n- Install method (conda, pip, source): pip\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10302/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10302/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}