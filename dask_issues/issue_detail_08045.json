{
    "url": "https://api.github.com/repos/dask/dask/issues/8045",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8045/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8045/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8045/events",
    "html_url": "https://github.com/dask/dask/issues/8045",
    "id": 971635987,
    "node_id": "MDU6SXNzdWU5NzE2MzU5ODc=",
    "number": 8045,
    "title": "Calculate valid partition divisions to avoid parser EOF error when CSV rows contain unescaped newlines",
    "user": {
        "login": "lmmx",
        "id": 2979452,
        "node_id": "MDQ6VXNlcjI5Nzk0NTI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2979452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/lmmx",
        "html_url": "https://github.com/lmmx",
        "followers_url": "https://api.github.com/users/lmmx/followers",
        "following_url": "https://api.github.com/users/lmmx/following{/other_user}",
        "gists_url": "https://api.github.com/users/lmmx/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/lmmx/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/lmmx/subscriptions",
        "organizations_url": "https://api.github.com/users/lmmx/orgs",
        "repos_url": "https://api.github.com/users/lmmx/repos",
        "events_url": "https://api.github.com/users/lmmx/events{/privacy}",
        "received_events_url": "https://api.github.com/users/lmmx/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 5,
    "created_at": "2021-08-16T11:22:40Z",
    "updated_at": "2021-10-25T01:48:31Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "This follows discussion in #4415 on the parser error encountered (\u2018EOF within string\u2019) when partitioning a file with unescaped newlines on a row. The partition naively splits on newlines (from what I understand?), which is only a valid assumption for files without unescaped within-row EOL characters.\r\n\r\nhttps://github.com/dask/dask/blob/e6b9e8983b26389c74036aeec615ac0e5ef34c65/dask/dataframe/io/csv.py#L704-L707\r\n\r\n**Would it be possible to \u2018assist\u2019 dask in identifying where to partition a CSV somehow?**\r\n\r\nFrom the above issue I surmise that this is not currently a feature (but if it is and nobody thought to suggest it there please let me know).\r\n\r\nI can imagine a function that:\r\n- opens the file,\r\n- seeks to the would-be partition offset minus some chunk size (expected to cover one or more lines),\r\n- reads in a chunk\r\n- identifies the latest row ending position within that chunk\r\n- passes that chunk to dask as the partition point\r\n- [repeat until file is completely partitioned]\r\n\r\n(I recently wrote a function that \u2018greps backwards\u2019 within a buffer, [here](https://github.com/lmmx/wikitransp/blob/1e6a49983960b69736ee95073357b19bccd2b91e/src/wikitransp/scraper/buf_grep.py#L10), which seems in the same spirit)\r\n\r\nThis could be done to identify the partition positions and then pass these into dask, or dask could provide a helper method to generate these positions.\r\n\r\nAlternatively, if dask\u2019s partition positions are accessible, these could be modified after calculation, but I presume manually doing so would be undesirable (primarily as if all the partition positions moved backwards then the final one would risk being greater than the max. permitted partition size)\r\n\r\nI tried to load the [WIT](https://github.com/google-research-datasets/wit) dataset which has newlines within rows (i.e. unescaped free text, from Wikipedia image captions) and [cannot get the advantage of cuDF speedup over pandas due to the problem of distributing the computation](https://github.com/rapidsai/cudf/issues/9042) (in the way that dask achieves with partitions).\r\n\r\nI think I\u2019d be able to speed this up if dask could successfully partition with such a routine, but I don\u2019t know where to put it (i.e. I don\u2019t see the \u201cselect where to partition\u201d aspect of dask exposed in the API, I presume it is not)\r\n\r\nAnother alternative I can envision is to `try`/`except` reading one row at the start of each partition (skipping the first as this is not post-partition), and if it fails with the EOF error then move that partition to the previous newline and repeat\r\n\r\n---\r\n\r\nMinimal example:\r\n\r\n- First get the sample file (here I also decompressed it)\r\n  - `wget https://storage.googleapis.com/gresearch/wit/wit_v1.train.all-1percent_sample.tsv.gz`\r\n\r\n```py\r\nimport dask.dataframe as dd\r\nfrom pathlib import Path\r\nimport time\r\n\r\np = Path(\"wit_v1.train.all-1percent_sample.tsv\")\r\nprint(f\"Data file: {p.name}\")\r\n\r\nt0 = time.time()\r\ndtype_dict = {\"mime_type\": \"category\"}\r\nfields = [*dtype_dict]\r\ndf = dd.read_csv(p, sep=\"\\t\", dtype=dtype_dict, usecols=fields)\r\nfiletypes = list(df.mime_type)\r\nt1 = time.time()\r\nprint(f\"dask took: {t1-t0}s\")\r\n```\r\n\u21e3\r\n```STDERR\r\nData file: wit_v1.train.all-1percent_sample.tsv\r\nTraceback (most recent call last):\r\n  File \"dask_test.py\", line 12, in <module>\r\n    filetypes = list(df.mime_type)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/dataframe/core.py\", line 579, in __len__\r\n    return self.reduction(\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/base.py\", line 286, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/base.py\", line 568, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/threaded.py\", line 79, in get\r\n    results = get_async(\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/local.py\", line 514, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/local.py\", line 325, in reraise\r\n    raise exc\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/local.py\", line 223, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/optimization.py\", line 969, in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/core.py\", line 151, in get\r\n    result = _execute_task(task, cache)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/core.py\", line 121, in <genexpr>\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/dataframe/io/csv.py\", line 125, in __call__\r\n    df = pandas_read_text(\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/dask/dataframe/io/csv.py\", line 178, in pandas_read_text\r\n    df = reader(bio, **kwargs)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 488, in _read\r\n    return parser.read(nrows)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1047, in read\r\n    index, columns, col_dict = self._engine.read(nrows)\r\n  File \"/home/louis/miniconda3/envs/dask_test/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 223, in read\r\n    chunks = self._reader.read_low_memory(nrows)\r\n  File \"pandas/_libs/parsers.pyx\", line 801, in pandas._libs.parsers.TextReader.read_low_memory\r\n  File \"pandas/_libs/parsers.pyx\", line 857, in pandas._libs.parsers.TextReader._read_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 843, in pandas._libs.parsers.TextReader._tokenize_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 1925, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 34111\r\n```\r\n\r\n(The exact row number given varies)\r\n\r\n- This refers to an error thrown when `reader` (i.e. `pd.read_csv`) reads the bytes buffer `bio` https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L173-L178\r\n- via https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L125-L135\r\n\r\n- In turn this `CsvFunctionWrapper` call comes from [`text_blocks_to_pandas`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L285-L398) getting called in [the return statement of `read_pandas`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L622-L633) (which gets put [in the `dask.dataframe` namespace ](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/__init__.py#L22), from [`dd.io`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/__init__.py#L2), from [`dd.io.csv`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L745) where it's a wrapper on `make_reader` using `pd.read_csv` as its [`reader`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L711) arg)\r\n  - i.e. it simplifies to just [`pd.read_csv`](https://github.com/pandas-dev/pandas/blob/0d46c238e7ec92363152270f444bc0cb251919b7/pandas/io/parsers/readers.py#L502), passing a bytes buffer corresponding to the block's bytes within the file rather than the input file\r\n\r\nI can't see any way of inspecting the [divisions](https://docs.dask.org/en/latest/dataframe-design.html#partitions) that are computed here (as they're not coming from a pre-existing dataframe but created from the TSV, hence no index to display in the divisions)\r\n\r\n```py\r\n>>> df.divisions\r\n(None, None, None, None, None, None, None, None, None, None, None, None)\r\n>>> df.known_divisions\r\nFalse\r\n```\r\n\r\nbut since it's bytes buffers under the hood you could check the divisions by seeking to the end of these buffers and performing the routine described above.\r\n\r\nThe blocks in question are the [`values`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L624) passed as the `block_lists` arg to `text_blocks_to_pandas` after being constructed in `read_pandas`: https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L620\r\n\r\nso `read_pandas` is where the change I am suggesting would go.\r\n\r\nThe `values` in turn were unpacked from `b_out`, which comes from `read_bytes` (from [`dask.bytes.core`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/bytes/core.py#L12)):\r\n\r\nhttps://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L547-L562\r\n\r\nThe offsets are derived from these block sizes without any line delimiter detection:\r\n\r\nhttps://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/bytes/core.py#L115-L116\r\n\r\nand then very importantly **the blocks are split according to the delimiter** here (the delimiter was [passed as the byte-encoded line terminator](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/dataframe/io/csv.py#L549)):\r\n\r\nhttps://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/bytes/core.py#L126-L138\r\n\r\nAfter which they are ready to be [passed on](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/bytes/core.py#L130-L137) to the `delayed_read` which then becomes `values` a.k.a. `block_list`.\r\n\r\nIt's that step of splitting the blocks according to the delimiter that I think could be modified to ensure rows are parsed correctly.\r\n\r\n2 further aspects to this:\r\n\r\n- Within the `delayed_read` function call is `OpenFile` (i.e. executed and then passed into `delayed_read`)\r\n- `delayed_read` is itself an alias: https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/bytes/core.py#L123\r\n  - [`delayed`](https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/delayed.py#L233) is the lazy computation proxy (not relevant to what's done here)\r\n  - `read_blocks_from_file` means that the computation being delayed is to read the block passed with 4 arguments: `lazy_file`, `off`, `bs`, `delimiter` https://github.com/dask/dask/blob/5cb8961b6035c90654e3999a7d99e6f5305b7ffc/dask/bytes/core.py#L168-L172 \r\n  - The `lazy_file` = an [`OpenFile`](https://github.com/intake/filesystem_spec/blob/master/fsspec/core.py#L30) object via `fsspec.core`\r\n  - The `off` offset is the unmodified offset (a multiple of the block size from `list(range(0, size, block_size)`)\r\n  - The `bs` is the `l`, an item in `lengths`, which will always be `block_size` except for the last block which may be shorter if the file size is not an exact multiple of the `block_size`\r\n  - `read_block` is called when `l` in `lengths` is set [by virtue of `block_size` being set, as it is here] to anything other than `None`, and this function [comes from `fsspec.utils`](https://github.com/intake/filesystem_spec/blob/3cdfd6cf8386857815535a32f747cd2033b30da7/fsspec/utils.py#L203-L265). \r\n- The [signature and docstring](https://github.com/intake/filesystem_spec/blob/3cdfd6cf8386857815535a32f747cd2033b30da7/fsspec/utils.py#L203-L217) show that its default behaviour is `split_before=False`, hence it starts/stops reading **after** the delimiter [newline] at `offset` and `offset+length`. This is how the newlines are detected.\r\n\r\n---\r\n\r\nSo long story short:\r\n- Roughly speaking [assuming file is evenly split into blocks of equal size], `dd.read_csv` reduces to\r\n\r\n```py\r\nfor o in range(0, size, dd.io.csv.AUTO_BLOCK_SIZE):\r\n  fsspec.read_block(f=input_csv, offset=o, length=dd.io.csv.AUTO_BLOCKSIZE,, delimiter=\"\\n\", split_before=False)\r\n```\r\n\r\n- This respects line separators but not necessarily row-delimiting line separators\r\n- Extending this procedure to respect row-delimiting line separators would make dask usable with a larger subset of the CSV files pandas can read\r\n\r\n---\r\n\r\nThe problem remains somewhat tricky to specify (how to verify that the newline at a given offset into a CSV is/isn\u2019t the start of a valid row and partition at the next one)\r\n\r\n---\r\n\r\nI thought about this some more and the three approaches I can think of are:\r\n\r\n1. Let the user provide a custom checking function to be used to verify the buffer bytes at a position are valid\r\n  - In the example dataset I am using, this would be very simple: the first column must equal a lower case two letter language code followed by the separator, and so on. This would make it very simple to check that the bytes after the line separator at a given offset was indeed the start of a row. (Or rather, to rule out invalid rows)\r\n2. Let the user provide a list of rows which may contain free text (including newlines, i.e. the \"separable columns\", those which may be separated across lines), so as to validate based on an attempted separation (reading as many bytes as necessary, including line separator/s, until fulfilling the expected number of columns). Alternatively, these may be discerned from the sample.\r\n3. Simply have pandas 'stream' a row, and catch any parsing error as a failure\r\n\r\nThe 1st option would potentially involve reading fewer bytes (thus faster).\r\n\r\nThe 2nd option seems the simplest to achieve, to identify the correct \"delimiter offset\" for the given partition offset (that is, the number of newline delimiters away from the offset at which the next row-delimiting newline is found). If the first newline delimiter after the partition offset was a valid start of row then the 'delimiter offset' would be 0, if it was the one after then it'd be 1, and so on.\r\n\r\nThe 3rd option would be most 'foolproof' (and perhaps simplest).\r\n\r\nI need to think about this some more (I suspect it may need a mix of the 3 approaches). Any suggestions/comments welcome",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8045/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8045/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}