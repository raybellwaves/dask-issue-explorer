{
    "url": "https://api.github.com/repos/dask/dask/issues/6562",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/6562/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/6562/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/6562/events",
    "html_url": "https://github.com/dask/dask/issues/6562",
    "id": 686515724,
    "node_id": "MDU6SXNzdWU2ODY1MTU3MjQ=",
    "number": 6562,
    "title": "pivot_table on a lazy Dask dataframe has significant performance cliffs",
    "user": {
        "login": "sm-Fifteen",
        "id": 516999,
        "node_id": "MDQ6VXNlcjUxNjk5OQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/516999?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sm-Fifteen",
        "html_url": "https://github.com/sm-Fifteen",
        "followers_url": "https://api.github.com/users/sm-Fifteen/followers",
        "following_url": "https://api.github.com/users/sm-Fifteen/following{/other_user}",
        "gists_url": "https://api.github.com/users/sm-Fifteen/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/sm-Fifteen/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/sm-Fifteen/subscriptions",
        "organizations_url": "https://api.github.com/users/sm-Fifteen/orgs",
        "repos_url": "https://api.github.com/users/sm-Fifteen/repos",
        "events_url": "https://api.github.com/users/sm-Fifteen/events{/privacy}",
        "received_events_url": "https://api.github.com/users/sm-Fifteen/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 9,
    "created_at": "2020-08-26T18:07:18Z",
    "updated_at": "2021-01-11T21:29:13Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "\r\n**What happened**:\r\nUsing pivot_table to try and convert a series of large vertically stored CSV files into a \"wide\" table, I've been running into memory limitation issues with pandas, so I turned to Dask. Dask has shown to have surprisingly poor performance when running pivot_table after read_csv (from 15 seconds to 10 minutes with the MCV below), with a fairly low CPU, Memory and Disk I/O usage, even though one would expect one of those three to be at maximum capacity.\r\n\r\nLooking at the client's timing and profiling data, I found most of the runtime to be taken up by `pivot_table_count` (in purple) and `pivot_table_sum` (in green): \r\n\r\n![Capture-dask-overview](https://user-images.githubusercontent.com/516999/91332204-d4894580-e799-11ea-9e6c-d9282d552007.PNG)\r\n\r\nFurthermore, most of both tasks' runtime was taken up by something to do with boxing dates, which are considered to be extension dtypes:\r\n\r\n![Capture-dask-purple-flame](https://user-images.githubusercontent.com/516999/91332641-8294ef80-e79a-11ea-932f-3c14743e4127.PNG)\r\n![Capture-dask-green-flame](https://user-images.githubusercontent.com/516999/91332648-84f74980-e79a-11ea-8009-f27f5ead1aec.PNG)\r\n\r\nI'm not entirely sure what's causing these tasks to take so long (GIL contention, maybe?), but it's making pivot_table perform far worse than it should. The long yellow line on both flamegraphs is from a slow `is_unique` operation on the categorical column, so there's another similar performance problem there even if we remove the dates.\r\n\r\n**What you expected to happen**:\r\nThe count and sum sections of the pivot aggregation should not be taking up as much of the computation's runtime as they currently are. Those unexpected performance cliffs are making Dask perform significantly worse than Pandas loading and manipulating each file one after the other, mostly because it appears to be unable to properly use the system resources for that operation and acting as a bottleneck for the entire task graph. Given how `pivot_table` appears to work, this could be impacting other `groupby` operations as well.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nCreating data and testing performance in-memory:\r\n\r\n```py\r\nimport pandas as pd\r\nimport numpy as np\r\nimport dask.dataframe as dd\r\nbig_matrix = np.zeros((500,50000))\r\npd_matrix = pd.DataFrame(data=big_matrix)\r\npd_matrix_v = pd_matrix.unstack().reset_index()\r\npd_matrix_v['level_0'] = pd.to_datetime(pd_matrix_v['level_0'] + 1_000_000_000, unit='s', origin='unix')\r\ndask_matrix_v = dd.from_pandas(pd_matrix_v, npartitions=10)\r\ndask_matrix_v = dask_matrix_v.categorize(columns='level_1')\r\n\r\n# Fast (10-15s)\r\npivot_task = dd.reshape.pivot_table(dask_matrix_v, index='level_0', columns='level_1', values=0)\r\npivot_task.compute()\r\n\r\n# Fast (10-15s)\r\npd_matrix_v.pivot_table(index='level_0', columns='level_1', values=0)\r\n\r\ndd.to_csv(dask_matrix_v, \"./test.csv\")\r\n```\r\n\r\nLoading data from disk and testing performance for lazy dataframes:\r\n\r\n```py\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\nfrom dask.distributed import Client\r\nclient = Client() # For monitoring\r\n\r\ndask_matrix_v = dd.read_csv(\"./test.csv/*.part\", usecols=['level_0', 'level_1', '0'], dtype={'level_1': 'int32', '0': 'float32'}, parse_dates=['level_0'])\r\ndask_matrix_v = dask_matrix_v.categorize(columns='level_1')\r\n\r\n# Very slow (10 minutes)\r\ndask_matrix_v = dd.reshape.pivot_table(dask_matrix_v, index='level_0', columns='level_1', values='0')\r\ndask_matrix_v.compute()\r\n\r\n# Just slow without parse_dates (3 minutes)\r\ndask_matrix_v = dd.read_csv(\"./test.csv/*.part\", usecols=['level_0', 'level_1', '0'], dtype={'level_0': 'object', 'level_1': 'int32', '0': 'float32'})\r\n\r\ndask_matrix_v = dask_matrix_v.categorize(columns='level_1')\r\ndask_matrix_v = dd.reshape.pivot_table(dask_matrix_v, index='level_0', columns='level_1', values='0')\r\ndask_matrix_v.compute()\r\n```\r\n\r\n**Anything else we need to know?**:\r\nUsing the same example as above, this is the timing graph once the datetime values are are replaced by strings. There's still a bottleneck on the categorical axis causing `pivot_table_count` and `pivot_table_sum` to stand out (though not as much), but unlike the dates, this one can't be avoided since Dask's `pivot_table` implementation requires the `columns` axis to be categorized.\r\n![Capture-dask-overview-dates-are-strings](https://user-images.githubusercontent.com/516999/91339653-c987e280-e7a4-11ea-86bf-9bcb4aa24046.PNG)\r\n\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.24.0\r\n- Python version: 3.8.0\r\n- Operating System: Windows 10\r\n- Install method (conda, pip, source): pip\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/6562/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/6562/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}