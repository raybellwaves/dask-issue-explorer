[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1513778185",
        "html_url": "https://github.com/dask/dask/issues/10198#issuecomment-1513778185",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10198",
        "id": 1513778185,
        "node_id": "IC_kwDOAbcwm85aOmwJ",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-18T20:50:01Z",
        "updated_at": "2023-04-18T20:51:16Z",
        "author_association": "MEMBER",
        "body": "Thanks for raising @Puumanamana \r\n\r\nThe `csv` reader doesn't really handle the case that there is an inconsistent schema across different files in the dataset. The problem is not really related to `rename`, but to the fact that any default `map_partitions` call will raise an error if the the columns produced by a task are different from the columns in the global `_meta` attribute (and in your case the `_meta` attribute will only have the columns found in the first file).\r\n\r\nI'm not sure if there is a simple way to deal with this limitation besides allowing the user to pass in a custom `meta` argument.\r\n\r\nIf you are okay with having a 1:1 mapping between files and partitions, `from_map` may provide a work-around:\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\n\r\n# Define the schema you want to enforce\r\nmeta = pd.DataFrame({k: pd.Series([], dtype=\"Int64\") for k in [\"c1\", \"c2\", \"c3\"]})\r\n\r\ndef read_csv_file(path):\r\n    # Read in a file and enforce the global schema\r\n    return pd.concat([meta, pd.read_csv(path)])\r\n\r\ndata = dd.from_map(read_csv_file, [\"file0.csv\", \"file1.csv\"], meta=meta)\r\ndata.rename(columns=dict(c1=\"c_new\")).compute()\r\n```\r\n```\r\n   c_new    c2    c3\r\n0      1     1  <NA>\r\n0      1  <NA>     1\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1513778185/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1513789726",
        "html_url": "https://github.com/dask/dask/issues/10198#issuecomment-1513789726",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10198",
        "id": 1513789726,
        "node_id": "IC_kwDOAbcwm85aOpke",
        "user": {
            "login": "Puumanamana",
            "id": 39426440,
            "node_id": "MDQ6VXNlcjM5NDI2NDQw",
            "avatar_url": "https://avatars.githubusercontent.com/u/39426440?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Puumanamana",
            "html_url": "https://github.com/Puumanamana",
            "followers_url": "https://api.github.com/users/Puumanamana/followers",
            "following_url": "https://api.github.com/users/Puumanamana/following{/other_user}",
            "gists_url": "https://api.github.com/users/Puumanamana/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Puumanamana/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Puumanamana/subscriptions",
            "organizations_url": "https://api.github.com/users/Puumanamana/orgs",
            "repos_url": "https://api.github.com/users/Puumanamana/repos",
            "events_url": "https://api.github.com/users/Puumanamana/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Puumanamana/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-18T21:01:08Z",
        "updated_at": "2023-04-18T21:01:08Z",
        "author_association": "NONE",
        "body": "I see, that makes sense. For my use case, I'm just doing some preprocessing to make all dataframes uniform (with the same schema) before combining them. \r\nI think your solution would work for me. Would there be any performance difference between that and dd.read_csv? Also, maybe another alternative would be to start from a bag, and then cast to a dataframe?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1513789726/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1513803021",
        "html_url": "https://github.com/dask/dask/issues/10198#issuecomment-1513803021",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10198",
        "id": 1513803021,
        "node_id": "IC_kwDOAbcwm85aOs0N",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-18T21:14:12Z",
        "updated_at": "2023-04-18T21:14:12Z",
        "author_association": "MEMBER",
        "body": ">For my use case, I'm just doing some preprocessing to make all dataframes uniform (with the same schema) before combining them.\r\n\r\nRight. This is probably the ideal approach. I suppose you can also do this \"pre-processing\" after the `read_csv` call using `map_partitions`:\r\n\r\n```python\r\nmeta = pd.DataFrame({k: pd.Series([], dtype=\"Int64\") for k in [\"c1\", \"c2\", \"c3\"]})\r\ndata = dd.read_csv(\"file*csv\").map_partitions(lambda x: pd.concat([meta, x]))\r\n```\r\n(or replace `pd.concat` with some other logic to enforce the schema you want)\r\n\r\n> I think your solution would work for me. Would there be any performance difference between that and dd.read_csv?\r\n\r\n`from_map` is quite performant, but using `map_partitions` after `dd.read_csv` is probably better (sorry for missing that earlier).\r\n\r\n>Also, maybe another alternative would be to start from a bag, and then cast to a dataframe?\r\n\r\nI'd probably avoid using bag here if it isn't necessary.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1513803021/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]