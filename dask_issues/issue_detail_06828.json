{
    "url": "https://api.github.com/repos/dask/dask/issues/6828",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/6828/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/6828/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/6828/events",
    "html_url": "https://github.com/dask/dask/issues/6828",
    "id": 740506724,
    "node_id": "MDU6SXNzdWU3NDA1MDY3MjQ=",
    "number": 6828,
    "title": "Memory surge with da.unique",
    "user": {
        "login": "chrisroat",
        "id": 1053153,
        "node_id": "MDQ6VXNlcjEwNTMxNTM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1053153?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/chrisroat",
        "html_url": "https://github.com/chrisroat",
        "followers_url": "https://api.github.com/users/chrisroat/followers",
        "following_url": "https://api.github.com/users/chrisroat/following{/other_user}",
        "gists_url": "https://api.github.com/users/chrisroat/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/chrisroat/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/chrisroat/subscriptions",
        "organizations_url": "https://api.github.com/users/chrisroat/orgs",
        "repos_url": "https://api.github.com/users/chrisroat/repos",
        "events_url": "https://api.github.com/users/chrisroat/events{/privacy}",
        "received_events_url": "https://api.github.com/users/chrisroat/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862305,
            "node_id": "MDU6TGFiZWwyNDI4NjIzMDU=",
            "url": "https://api.github.com/repos/dask/dask/labels/array",
            "name": "array",
            "color": "006b75",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 11,
    "created_at": "2020-11-11T06:35:09Z",
    "updated_at": "2021-03-27T04:08:30Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**What happened**:\r\n\r\nI am porting some skimage functionality to dask for histogram matching.  The code seems to work correctly for small examples and unit tests, so I believe it is coded correctly (famous last words, I know).  However, when performing a moderate-sized task, the dask version is goes slow and uses a lot of memory.  The numpy version operates with no such issues on the same data.\r\n\r\n**What you expected to happen**:\r\n\r\nOn a single-chunk moderate example, numpy and dask should give the same performance.  Likely, there is some subtlety to dask I do not appreciate here.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n\r\n# Numpy code\r\n# Copied for skimage.exposure.histogram_matching.py for comparison\r\ndef np_match(source, template, dtype=np.uint16):\r\n    _, src_unique_indices, src_counts = np.unique(source.ravel(), return_inverse=True, return_counts=True)\r\n\r\n    tmpl_values, tmpl_counts = np.unique(template.ravel(), return_counts=True)\r\n\r\n    src_quantiles = np.cumsum(src_counts) / source.size\r\n    tmpl_quantiles = np.cumsum(tmpl_counts) / template.size\r\n\r\n    interp_a_values = np.interp(src_quantiles, tmpl_quantiles, tmpl_values).astype(dtype)\r\n\r\n    return interp_a_values[src_unique_indices].reshape(source.shape)\r\n\r\n```\r\n\r\n```python\r\n# Dask port\r\n\r\ndef _interpolate(src_quantiles, tmpl_quantiles, tmpl_values, dtype):\r\n    return np.interp(src_quantiles, tmpl_quantiles, tmpl_values).astype(dtype)\r\n\r\ndef da_match(source, template, dtype=np.uint16):\r\n    _, src_unique_indices, src_counts = da.unique(source.ravel(), return_inverse=True, return_counts=True)\r\n\r\n    tmpl_values, tmpl_counts = da.unique(template.ravel(), return_counts=True)\r\n\r\n    src_quantiles = da.cumsum(src_counts) / source.size\r\n    tmpl_quantiles = da.cumsum(tmpl_counts) / template.size\r\n\r\n    # Debugging here and computing any of the above variables is fast, as expected.  It's this next step,\r\n    # passing the delayed into the map_blocks, which seems to trigger an issue.\r\n    # (Yes, I do understand this is a bottleneck in the graph, and must only use int16 or smaller dtypes.)\r\n    interp_a_values = dask.delayed(_interpolate)(src_quantiles, tmpl_quantiles, tmpl_values, dtype)\r\n    interp_a_values = da.from_delayed(interp_a_values, dtype=dtype, shape=(np.nan, ))  # shape is src_quantiles.shape\r\n    \r\n    def lookup(chunk, values):\r\n        return values[chunk]\r\n    \r\n    result = src_unique_indices.map_blocks(lookup, values=interp_a_values, dtype=interp_a_values.dtype)\r\n    \r\n    return result.reshape(source.shape)\r\n```\r\n\r\nTest case:\r\n```python\r\nchn0 = np.random.randint(0, 500, (63, 300, 300), dtype=np.uint16)\r\nchn1 = np.random.randint(0, 500, (63, 300, 300), dtype=np.uint16)\r\n\r\nresult_np = np_match(chn0, chn1)  # sub-second, no noticable memory usage\r\n\r\nresult_da = da_match(da.from_array(chn0), da.from_array(chn1)).compute()  # 11 seconds, 50GB\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nI have experimented with passing a delayed shape to `from_delayed` and using the meta keyword in `map_blocks`.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.30.0\r\n- Python version: 3.8.5\r\n- Operating System: Ubuntu 20.4\r\n- Install method (conda, pip, source): pip\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/6828/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/6828/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}