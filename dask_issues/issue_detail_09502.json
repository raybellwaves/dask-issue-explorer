{
    "url": "https://api.github.com/repos/dask/dask/issues/9502",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9502/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9502/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9502/events",
    "html_url": "https://github.com/dask/dask/issues/9502",
    "id": 1376803191,
    "node_id": "I_kwDOAbcwm85SEFl3",
    "number": 9502,
    "title": "The workers keep \"Waiting to connect to Scheduler\" while computing large matrix or tensor",
    "user": {
        "login": "LUOXIAO92",
        "id": 89069722,
        "node_id": "MDQ6VXNlcjg5MDY5NzIy",
        "avatar_url": "https://avatars.githubusercontent.com/u/89069722?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/LUOXIAO92",
        "html_url": "https://github.com/LUOXIAO92",
        "followers_url": "https://api.github.com/users/LUOXIAO92/followers",
        "following_url": "https://api.github.com/users/LUOXIAO92/following{/other_user}",
        "gists_url": "https://api.github.com/users/LUOXIAO92/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/LUOXIAO92/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/LUOXIAO92/subscriptions",
        "organizations_url": "https://api.github.com/users/LUOXIAO92/orgs",
        "repos_url": "https://api.github.com/users/LUOXIAO92/repos",
        "events_url": "https://api.github.com/users/LUOXIAO92/events{/privacy}",
        "received_events_url": "https://api.github.com/users/LUOXIAO92/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1342320181,
            "node_id": "MDU6TGFiZWwxMzQyMzIwMTgx",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20info",
            "name": "needs info",
            "color": "d0cfcc",
            "default": false,
            "description": "Needs further information from the user"
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2022-09-17T15:34:41Z",
    "updated_at": "2022-10-24T02:14:53Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\nThe workers keep \"Waiting to connect to Scheduler\" while computing a large matrix or tensor. I'm not sure, but it seems that the scheduler can not afford to deal a huge number of task graphs. \r\n\r\n**What you expected to happen**:\r\nAny scale of matrix or tensor can be computed with enough memory.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n    rs = da.random.RandomState(seed=1234,RandomState=np.random.RandomState)\r\n    Dcut = 16\r\n    b = 8\r\n    T1 = rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\\\r\n        + 1j*rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\r\n    T2 = rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\\\r\n        + 1j*rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\r\n    T = da.einsum(\"abcdijkl,efghjilk->aebfcgdh\",T1,T2)\r\n    trace = da.einsum(\"aabbccdd\",T)\r\n    trace = trace.compute()\r\n```\r\nThis can be computed when Dcut less equal than 12.\r\n\r\n<details>\r\n<summary>Full code:</summary>\r\n\r\n```python\r\nimport sys\r\nimport datetime\r\n\r\nfrom dask_mpi import initialize\r\nfrom dask.utils import parse_bytes\r\nfrom dask.distributed import Client\r\nimport dask.array as da\r\nimport numpy as np\r\nimport dask\r\n\r\nif __name__ == \"__main__\":\r\n    initialize(interface=\"tofu0\",\r\n                local_directory=\"./tmp\",\r\n                protocol=\"tcp\",\r\n                nthreads=1,\r\n                dashboard=False,\r\n                memory_limit=parse_bytes('621 MB')\r\n                )\r\n    client = Client()\r\n    print(\"client:\",client)\r\n    \r\n    now=datetime.datetime.now()\r\n    with open(\"result.out\",\"a\") as f:\r\n        f.write(\"{:}\\n\".format(now))\r\n        \r\n    rs = da.random.RandomState(seed=1234,RandomState=np.random.RandomState)\r\n    Dcut = 16\r\n    b = 8\r\n    T1 = rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\\\r\n        + 1j*rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\r\n    T2 = rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\\\r\n        + 1j*rs.random(size=(Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut,Dcut), chunks=(b,b,b,b,b,b,b,b))\r\n\r\n    T1mem=T1.nbytes/(1024**3)\r\n\r\n    from time import time\r\n    with open(\"result.out\",\"a\") as f:\r\n        f.write(\"{:}\\n\".format(client))\r\n        t0=time()\r\n        T = da.einsum(\"abcdijkl,efghjilk->aebfcgdh\",T1,T2)\r\n        trace  = da.einsum(\"aabbccdd\",T)\r\n        print(T)\r\n        f.write(\"{:}\\n\".format(T))\r\n        trace = trace.compute()\r\n        t1=time()\r\n        client.close()\r\n        print(\"time:\",t1-t0,\"s\")\r\n        print(\"trace:\",trace)\r\n        f.write(\"T1 mem: {:.2e} Gbytes, Dcut: {:}, trace: {:.6e} , time: {:.2e} s\\n\\n\".format(T1mem, Dcut, trace, t1-t0))\r\n```\r\n</details>\r\n\r\n**Anything else we need to know?**:\r\nI'm using dask_mpi.\r\nThe architecture of cpu is ARMv8-A\r\n\r\n**Environment**:\r\n- Dask version: 2022.9.0\r\n- Python version: 3.9\r\n- Operating System: Red Hat Enterprise Linux 8\r\n- Install method (conda, pip, source): conda\r\n\r\n<!-- If you are reporting an issue such as scale stability, cluster deadlock.\r\nPlease provide a cluster dump state with this issue, by running client.dump_cluster_state()\r\n\r\nhttps://distributed.dask.org/en/stable/api.html?highlight=dump_cluster_state#distributed.Client.dump_cluster_state\r\n\r\n-->\r\n\r\n**Out put log:**\r\nSince the log is too large, i have picked some important message.\r\n<details>\r\n<summary>Log:</summary>\r\n\r\n```\r\n2022-09-17 23:04:42,758 - distributed.scheduler - INFO - State start\r\n2022-09-17 23:04:42,806 - distributed.scheduler - INFO -   Scheduler at: tcp://10.10.112.183:36955\r\n2022-09-17 23:04:42,807 - distributed.scheduler - INFO -   dashboard at:        10.10.112.183:8787\r\n2022-09-17 23:04:43,752 - distributed.diskutils - INFO - Found stale lock file and directory '/work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-3k4_s7ea', purging\r\n2022-09-17 23:04:43,768 - distributed.diskutils - INFO - Found stale lock file and directory '/work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-kfu_yv74', purging\r\n2022-09-17 23:04:43,790 - distributed.diskutils - INFO - Found stale lock file and directory '/work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-tr24dqnr', purging\r\n2022-09-17 23:04:43,799 - distributed.diskutils - INFO - Found stale lock file and directory '/work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-hb95os2q', purging\r\n2022-09-17 23:04:43,816 - distributed.diskutils - INFO - Found stale lock file and directory '/work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-0b_iz67d', purging\r\n2022-09-17 23:04:43,839 - distributed.worker - INFO -       Start worker at:  tcp://10.10.120.191:45861\r\n2022-09-17 23:04:43,839 - distributed.worker - INFO -       Start worker at:  tcp://10.10.120.191:38835\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -          Listening to:  tcp://10.10.120.191:45861\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -          Listening to:  tcp://10.10.120.191:38835\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -           Worker name:                        435\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -           Worker name:                        459\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -          dashboard at:        10.10.120.191:42959\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -          dashboard at:        10.10.120.191:32953\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -               Threads:                          1\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -               Threads:                          1\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -                Memory:                 592.23 MiB\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -                Memory:                 592.23 MiB\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -       Local Directory: /work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-j01iemwx\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -       Local Directory: /work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-ty809bue\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO - -------------------------------------------------\r\n/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.00s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\r\n  next(self.gen)\r\n2022-09-17 23:04:43,839 - distributed.worker - INFO -       Start worker at:  tcp://10.10.120.143:33273\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -          Listening to:  tcp://10.10.120.143:33273\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -           Worker name:                        357\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO -          dashboard at:        10.10.120.143:36149\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:04:43,840 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -               Threads:                          1\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -                Memory:                 592.23 MiB\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO -       Local Directory: /work/03/wo22i012/w17001/test/dask/tmp/dask-worker-space/worker-xsmvfoac\r\n2022-09-17 23:04:43,841 - distributed.worker - INFO - -------------------------------------------------\r\n/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.01s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\r\n  next(self.gen)\r\n/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.01s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\r\n  next(self.gen)\r\n/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.01s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\r\n  next(self.gen)\r\n...\r\n2022-09-17 23:04:53,078 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.10.112.183:39139', name: 36, status: init, memory: 0, processing: 0>\r\n2022-09-17 23:04:53,079 - distributed.core - INFO - Starting established connection\r\n2022-09-17 23:04:53,080 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.10.112.183:39139\r\n2022-09-17 23:04:53,080 - distributed.core - INFO - Starting established connection\r\n2022-09-17 23:04:53,081 - distributed.worker - INFO -         Registered to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:04:53,081 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:53,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.10.112.183:33745', name: 20, status: init, memory: 0, processing: 0>\r\n2022-09-17 23:04:53,083 - distributed.core - INFO - Starting established connection\r\n2022-09-17 23:04:53,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.10.112.183:33745\r\n2022-09-17 23:04:53,084 - distributed.core - INFO - Starting established connection\r\n2022-09-17 23:04:53,085 - distributed.worker - INFO -         Registered to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:04:53,085 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:53,086 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.10.112.183:34031', name: 14, status: init, memory: 0, processing: 0>\r\n2022-09-17 23:04:53,087 - distributed.core - INFO - Starting established connection\r\n2022-09-17 23:04:53,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.10.112.183:34031\r\n2022-09-17 23:04:53,088 - distributed.core - INFO - Starting established connection\r\n2022-09-17 23:04:53,089 - distributed.worker - INFO -         Registered to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:04:53,089 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-17 23:04:53,091 - distributed.core - INFO - Starting established connection\r\n/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/dask/array/routines.py:328: PerformanceWarning: Increasing number of chunks by factor of 16\r\n  intermediate = blockwise(\r\n2022-09-17 23:04:56,126 - distributed.worker_memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 499.69 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:04:56,145 - distributed.worker_memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 491.75 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:04:56,218 - distributed.worker_memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 515.62 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:04:56,624 - distributed.worker_memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 501.81 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:05:13,865 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:13,865 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:13,866 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:13,866 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n...\r\n2022-09-17 23:05:15,285 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:15,287 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:15,291 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:19,431 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:29,734 - distributed.worker_memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 436.50 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:05:29,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\r\n2022-09-17 23:05:43,991 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:43,991 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:43,991 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:43,991 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n...\r\n2022-09-17 23:05:44,148 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:44,152 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:29,759 - distributed.worker_memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 444.69 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:05:44,024 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:05:44,026 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n...\r\n2022-09-17 23:07:15,781 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:07:19,920 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:07:30,690 - distributed.worker_memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 504.31 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:07:30,691 - distributed.worker_memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 504.31 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:07:30,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 154.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\r\n2022-09-17 23:07:44,465 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:07:44,465 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n...\r\n2022-09-17 23:07:44,610 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:07:30,716 - distributed.worker_memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 504.31 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:07:44,479 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n...\r\n2022-09-17 23:08:16,005 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:08:16,008 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:08:31,198 - distributed.worker_memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 371.56 MiB -- Worker memory limit: 592.23 MiB\r\n2022-09-17 23:08:31,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 215.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\r\n2022-09-17 23:08:44,680 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n...\r\n2022-09-17 23:14:17,292 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:14:17,368 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:14:17,368 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n2022-09-17 23:14:17,368 - distributed.worker - INFO - Waiting to connect to:  tcp://10.10.112.183:36955\r\n[WARN] PLE 0610 plexec The process terminated with the signal.(rank=8)(nid=0x02b20008)(sig=24)\r\n```\r\n</details>",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9502/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9502/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}