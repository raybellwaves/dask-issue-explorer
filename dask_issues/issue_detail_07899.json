{
    "url": "https://api.github.com/repos/dask/dask/issues/7899",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/7899/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/7899/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/7899/events",
    "html_url": "https://github.com/dask/dask/issues/7899",
    "id": 944745056,
    "node_id": "MDU6SXNzdWU5NDQ3NDUwNTY=",
    "number": 7899,
    "title": "[FEA] API to write dask dataframes to local storage of each node in multi-node cluster",
    "user": {
        "login": "VibhuJawa",
        "id": 4837571,
        "node_id": "MDQ6VXNlcjQ4Mzc1NzE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4837571?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/VibhuJawa",
        "html_url": "https://github.com/VibhuJawa",
        "followers_url": "https://api.github.com/users/VibhuJawa/followers",
        "following_url": "https://api.github.com/users/VibhuJawa/following{/other_user}",
        "gists_url": "https://api.github.com/users/VibhuJawa/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/VibhuJawa/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/VibhuJawa/subscriptions",
        "organizations_url": "https://api.github.com/users/VibhuJawa/orgs",
        "repos_url": "https://api.github.com/users/VibhuJawa/repos",
        "events_url": "https://api.github.com/users/VibhuJawa/events{/privacy}",
        "received_events_url": "https://api.github.com/users/VibhuJawa/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 10,
    "created_at": "2021-07-14T19:51:07Z",
    "updated_at": "2021-07-16T20:04:04Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "#### [FEA] API to write dask dataframes to local storage of each node in multi-node cluster\r\n\r\n\r\n### Example requested API:\r\n\r\n```python\r\ndf.to_parquet(xxx, write_locally_per_node=True) \r\n```\r\n**Please feel free to suggest a better API to do this.**  \r\n\r\n\r\n### Usecase Details: \r\n\r\nWhile working on multi-node dask cluster users often don't have the shared storage available in the system.  This problem becomes more worse for multi-node cloud systems when intra-node communication is often a bottleneck.  \r\n\r\n\r\nWe need to write these frames locally to run a bunch of downstream tasks (think running multiple machine learning models on each node). \r\n\r\nHaving an API that allows this will simplify doing this. \r\n\r\n### Workaround:\r\n\r\nWe currently use the below workaround to achieve this but having it locally in dask will be super helpful. \r\n```python\r\n\r\n## This function writes data-frames\r\ndef writing_func(df,node_ip, out_dir, part_num):\r\n    import os\r\n    worker_ip = get_worker().name.split('//')[1].split(':')[0]\r\n    \r\n    ### ensure we are writing using a worker that belongs to node_ip\r\n    assert worker_ip==node_ip\r\n    \r\n    out_fn = str(part_num) + '.parquet'\r\n    out_path=os.path.join(out_dir,out_fn)\r\n    df.to_parquet(out_path)\r\n    return len(df)\r\n\r\ndef get_node_ip_dict():\r\n    workers = client.scheduler_info()['workers'].keys()\r\n    ips = set([x.split('//')[1].split(':')[0] for x in workers])\r\n    ip_d = dict.fromkeys(ips)\r\n    for worker in workers:\r\n        ip = worker.split('//')[1].split(':')[0]\r\n        if ip_d[ip] is None:\r\n            ip_d[ip]=[]\r\n        ip_d[ip].append(worker)\r\n    return ip_d\r\n    \r\n    \r\n ip_dict = get_node_ip_dict()\r\n \r\noutput_task_ls=[]\r\nfor node_ip,node_workers in ip_dict.items():\r\n    \r\n    ## create a task ls\r\n    task_ls = [dask.delayed(writing_func)(df,node_ip,out_dir,part_num) for part_num,df in enumerate(dask_df.to_delayed())]\r\n    \r\n    ## submit the task to the workers on the node\r\n    o_ls = client.compute(task_ls,workers=node_workers,allow_other_workers=False,sync=False)\r\n    output_task_ls.append(o_ls)\r\n\r\nlen_list = [sum([o.result() for o in o_ls]) for o_ls in output_task_ls]\r\n```\r\n CC: @quasiben , @randerzander",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/7899/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/7899/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}