{
    "url": "https://api.github.com/repos/dask/dask/issues/8294",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8294/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8294/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8294/events",
    "html_url": "https://github.com/dask/dask/issues/8294",
    "id": 1034913744,
    "node_id": "I_kwDOAbcwm849r4fQ",
    "number": 8294,
    "title": "Shuffle prototype: Feedback (disk usage + workers dying)",
    "user": {
        "login": "DahnJ",
        "id": 18722560,
        "node_id": "MDQ6VXNlcjE4NzIyNTYw",
        "avatar_url": "https://avatars.githubusercontent.com/u/18722560?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/DahnJ",
        "html_url": "https://github.com/DahnJ",
        "followers_url": "https://api.github.com/users/DahnJ/followers",
        "following_url": "https://api.github.com/users/DahnJ/following{/other_user}",
        "gists_url": "https://api.github.com/users/DahnJ/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/DahnJ/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/DahnJ/subscriptions",
        "organizations_url": "https://api.github.com/users/DahnJ/orgs",
        "repos_url": "https://api.github.com/users/DahnJ/repos",
        "events_url": "https://api.github.com/users/DahnJ/events{/privacy}",
        "received_events_url": "https://api.github.com/users/DahnJ/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 6,
    "created_at": "2021-10-25T09:46:15Z",
    "updated_at": "2021-11-06T14:03:01Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "I have tried #8223 on a ~3.4TB gzipped Parquet dataset.\r\n\r\nI tried four runs so far, with two different behaviours\r\n- First I tried the whole dataset. I got to the last step (`to_parquet`), but then ran out of disk space with over 4TB being used by the shuffle\r\n- The subsequent three runs I have tried to use a subset (by passing a smaller number of files to `read_parquet`). I did not get over the `set_index` step. Workers seem to die and the computation hangs, always towards the end. Errors below.\r\n\r\nBased on the first run, this does look like it could've actually been successful if I had more disk space. That's quite exciting, as external sorting has been a big issue for me. \r\n\r\n\r\n\r\n\r\n### Code used\r\nSadly I cannot share the data. I am at least sharing the code I'm using.\r\n\r\n```python\r\nfrom dask.distributed import Client, LocalCluster\r\nfrom pathlib import Path\r\n\r\ncluster = LocalCluster(n_workers=10, memory_limit='25GB', threads_per_worker=2)\r\nclient = Client(cluster)\r\npaths_files = list(Path('.').glob('*/*.parquet'))\r\ncolumns = ['col_a', 'col_b']\r\n\r\ndata = dd.read_parquet(paths_files, columns=columns)\r\n\r\n# Here is where workers fail and computation hangs (3 out of 4 tries with a subset)\r\ndata = data.set_index('col_a', shuffle=\"service\")\r\n# Here is where it ran out of disk space (1 out of 4 tries)\r\ndata.to_parquet('cache-imported.gz.parq', compression='gzip')\r\n```\r\n\r\n### Errors\r\n\r\n<details>\r\n  <summary>Orginal error</summary>\r\n  \r\n```python\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38001\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 205, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 492, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 319, in connect\r\n    handshake = await asyncio.wait_for(comm.read(), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 494, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:38001 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38215\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:38215 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39419\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:39419 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42233\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:42233 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33333\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:33333 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38215\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:38215 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33333\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:33333 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33333\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:33333 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36893\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 205, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 492, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 319, in connect\r\n    handshake = await asyncio.wait_for(comm.read(), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 494, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:36893 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41795\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 205, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 492, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 319, in connect\r\n    handshake = await asyncio.wait_for(comm.read(), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 494, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:41795 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41795\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:41795 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41795\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:41795 after 30 s\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38707\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 205, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 492, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 319, in connect\r\n    handshake = await asyncio.wait_for(comm.read(), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 494, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2654, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3982, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 3959, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:38707 after 30 s\r\ndistributed.utils - ERROR - (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", [(\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'register-replica', 'released', 'compute-task-1635104458.1960478', 1635104468.872155), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'fetch', {}, 'compute-task-1635104458.1960478', 1635104468.8722844), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104468.872853), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104468.8727045', 1635104468.8728676), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104470.384826), ('receive-dep-failed', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104503.914738), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing-dep'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'fetch', {}, 'ensure-communicating-1635104468.8727045', 1635104503.9147604), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'missing', {}, 'ensure-communicating-1635104503.91477', 1635104503.9147818), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing', 'fetch', {}, 'find-missing-1635104504.5391095', 1635104504.5398495), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104504.5398893), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104504.5398538', 1635104504.5398993), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104504.5399654), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {}, 'processing-released-1635104504.5400004', 1635104504.5406826), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.5406876), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.540689), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'release-key', 'processing-released-1635104504.5400004'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'released', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'forgotten'}, 'processing-released-1635104504.5400004', 1635104504.5407002), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'forgotten', {}, 'processing-released-1635104504.5400004', 1635104504.5407038), ('lost-during-gather', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538'), ('receive-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104548.287821)])\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils.py\", line 648, in log_errors\r\n    yield\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2745, in gather_dep\r\n    assert ts, (d, self.story(d))\r\nAssertionError: (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", [(\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'register-replica', 'released', 'compute-task-1635104458.1960478', 1635104468.872155), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'fetch', {}, 'compute-task-1635104458.1960478', 1635104468.8722844), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104468.872853), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104468.8727045', 1635104468.8728676), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104470.384826), ('receive-dep-failed', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104503.914738), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing-dep'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'fetch', {}, 'ensure-communicating-1635104468.8727045', 1635104503.9147604), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'missing', {}, 'ensure-communicating-1635104503.91477', 1635104503.9147818), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing', 'fetch', {}, 'find-missing-1635104504.5391095', 1635104504.5398495), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104504.5398893), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104504.5398538', 1635104504.5398993), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104504.5399654), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {}, 'processing-released-1635104504.5400004', 1635104504.5406826), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.5406876), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.540689), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'release-key', 'processing-released-1635104504.5400004'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'released', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'forgotten'}, 'processing-released-1635104504.5400004', 1635104504.5407002), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'forgotten', {}, 'processing-released-1635104504.5400004', 1635104504.5407038), ('lost-during-gather', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538'), ('receive-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104548.287821)])\r\ntornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f5f45925ac0>>, <Task finished name='Task-8548' coro=<Worker.gather_dep() done, defined at /root/miniconda3/lib/python3.9/site-packages/distributed/worker.py:2588> exception=AssertionError((\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", [(\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'register-replica', 'released', 'compute-task-1635104458.1960478', 1635104468.872155), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'fetch', {}, 'compute-task-1635104458.1960478', 1635104468.8722844), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104468.872853), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104468.8727045', 1635104468.8728676), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104470.384826), ('receive-dep-failed', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104503.914738), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing-dep'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'fetch', {}, 'ensure-communicating-1635104468.8727045', 1635104503.9147604), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'missing', {}, 'ensure-communicating-1635104503.91477', 1635104503.9147818), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing', 'fetch', {}, 'find-missing-1635104504.5391095', 1635104504.5398495), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104504.5398893), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104504.5398538', 1635104504.5398993), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104504.5399654), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {}, 'processing-released-1635104504.5400004', 1635104504.5406826), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.5406876), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.540689), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'release-key', 'processing-released-1635104504.5400004'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'released', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'forgotten'}, 'processing-released-1635104504.5400004', 1635104504.5407002), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'forgotten', {}, 'processing-released-1635104504.5400004', 1635104504.5407038), ('lost-during-gather', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538'), ('receive-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104548.287821)]))>)\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\n    future.result()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2745, in gather_dep\r\n    assert ts, (d, self.story(d))\r\nAssertionError: (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", [(\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'register-replica', 'released', 'compute-task-1635104458.1960478', 1635104468.872155), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'fetch', {}, 'compute-task-1635104458.1960478', 1635104468.8722844), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104468.872853), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104468.8727045', 1635104468.8728676), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104470.384826), ('receive-dep-failed', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104468.8727045', 1635104503.914738), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing-dep'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'fetch', {}, 'ensure-communicating-1635104468.8727045', 1635104503.9147604), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'missing', {}, 'ensure-communicating-1635104503.91477', 1635104503.9147818), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'missing', 'fetch', {}, 'find-missing-1635104504.5391095', 1635104504.5398495), ('gather-dependencies', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'stimulus', 1635104504.5398893), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'fetch', 'flight', {}, 'ensure-communicating-1635104504.5398538', 1635104504.5398993), ('request-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104504.5399654), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {}, 'processing-released-1635104504.5400004', 1635104504.5406826), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.5406876), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'flight', 'cancelled', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'released'}, 'processing-released-1635104504.5400004', 1635104504.540689), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'release-key', 'processing-released-1635104504.5400004'), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'cancelled', 'released', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\": 'forgotten'}, 'processing-released-1635104504.5400004', 1635104504.5407002), (\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\", 'released', 'forgotten', {}, 'processing-released-1635104504.5400004', 1635104504.5407038), ('lost-during-gather', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538'), ('receive-dep', 'tcp://127.0.0.1:41795', {\"('re-quantiles-1-59cbe1d46b9bfe7bf5a9da9c07036ff5', 75)\"}, 'ensure-communicating-1635104504.5398538', 1635104548.287821)])\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary> Another error during `to_parquet`</summary>\r\n\r\n\r\n```python\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37667\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37530 remote=tcp://127.0.0.1:37667>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:37667 -> tcp://127.0.0.1:34511\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:37667 remote=tcp://127.0.0.1:37530>: TimeoutError: [Errno 110] Connection timed out\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36281\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45000 remote=tcp://127.0.0.1:36281>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:36281 -> tcp://127.0.0.1:34511\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:36281 remote=tcp://127.0.0.1:45000>: TimeoutError: [Errno 110] Connection timed out\r\ntornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f348fe7bac0>>, <Task finished name='Task-9116' coro=<Worker.heartbeat() done, defined at /root/miniconda3/lib/python3.9/site-packages/distributed/worker.py:1182> exception=OSError('Timed out during handshake while connecting to tcp://127.0.0.1:43431 after 30 s')>)\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 245, in write\r\n    async def write(self, msg, serializers=None, on_error=\"message\"):\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\r\n    fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 320, in connect\r\n    await asyncio.wait_for(comm.write(local_info), time_left())\r\n  File \"/root/miniconda3/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\n    future.result()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1231, in heartbeat\r\n    raise e\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1190, in heartbeat\r\n    response = await retry_operation(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 860, in send_recv_from_rpc\r\n    comm = await self.pool.connect(self.addr)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1048, in connect\r\n    raise exc\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 1032, in connect\r\n    comm = await fut\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py\", line 324, in connect\r\n    raise OSError(\r\nOSError: Timed out during handshake while connecting to tcp://127.0.0.1:43431 after 30 s\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:36281 -> tcp://127.0.0.1:37667\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:36281 remote=tcp://127.0.0.1:44898>: TimeoutError: [Errno 110] Connection timed out\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36281\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44898 remote=tcp://127.0.0.1:36281>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:43515 -> tcp://127.0.0.1:37275\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43515 remote=tcp://127.0.0.1:45898>: TimeoutError: [Errno 110] Connection timed out\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43515\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45898 remote=tcp://127.0.0.1:43515>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37275\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:40650 remote=tcp://127.0.0.1:37275>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43515\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:46346 remote=tcp://127.0.0.1:43515>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:43515 -> tcp://127.0.0.1:37667\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43515 remote=tcp://127.0.0.1:46346>: TimeoutError: [Errno 110] Connection timed out\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:37275 -> tcp://127.0.0.1:34511\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:37275 remote=tcp://127.0.0.1:40650>: TimeoutError: [Errno 110] Connection timed out\r\ndistributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 29.25 GiB -- Worker memory limit: 46.57 GiB\r\ndistributed.worker - ERROR - failed during get data with tcp://127.0.0.1:37667 -> tcp://127.0.0.1:36281\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 1649, in get_data\r\n    response = await comm.read(deserializers=serializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:37667 remote=tcp://127.0.0.1:37070>: TimeoutError: [Errno 110] Connection timed out\r\ndistributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37667\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 867, in _read_to_buffer\r\n    bytes_read = self.read_from_fd(buf)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/tornado/iostream.py\", line 1140, in read_from_fd\r\n    return self.socket.recv_into(buf, len(buf))\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 2741, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4031, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/worker.py\", line 4011, in _get_data\r\n    response = await send_recv(\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/core.py\", line 640, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 221, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37070 remote=tcp://127.0.0.1:37667>: ConnectionResetError: [Errno 104] Connection reset by peer\r\ndistributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 33.46 GiB -- Worker memory limit: 46.57 GiB\r\ndistributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 33.46 GiB -- Worker memory limit: 46.57 GiB\r\ndistributed.worker - WARNING - Compute Failed\r\nFunction:  unpack\r\nargs:      (<dask.dataframe.shuffle_service.ShuffleService object at 0x7f4627812d00>, 240, None)\r\nkwargs:    {}\r\nException: 'AttributeError(\"\\'ShuffleService\\' object has no attribute \\'retrieve_futures\\'\")'\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/tmp/ipykernel_815540/1044375382.py in <module>\r\n----> 1 data.to_parquet('cache-imported.gz.parq', compression='gzip')\r\n\r\n~/miniconda3/lib/python3.9/site-packages/dask/dataframe/core.py in to_parquet(self, path, *args, **kwargs)\r\n   4558         from .io import to_parquet\r\n   4559 \r\n-> 4560         return to_parquet(self, path, *args, **kwargs)\r\n   4561 \r\n   4562     def to_orc(self, path, *args, **kwargs):\r\n\r\n~/miniconda3/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py in to_parquet(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, **kwargs)\r\n    723     if compute:\r\n    724         if write_metadata_file:\r\n--> 725             return compute_as_if_collection(\r\n    726                 DataFrame, graph, (final_name, 0), **compute_kwargs\r\n    727             )\r\n\r\n~/miniconda3/lib/python3.9/site-packages/dask/base.py in compute_as_if_collection(cls, dsk, keys, scheduler, get, **kwargs)\r\n    313     schedule = get_scheduler(scheduler=scheduler, cls=cls, get=get)\r\n    314     dsk2 = optimization_function(cls)(dsk, keys, **kwargs)\r\n--> 315     return schedule(dsk2, keys, **kwargs)\r\n    316 \r\n    317 \r\n\r\n~/miniconda3/lib/python3.9/site-packages/distributed/client.py in get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\r\n   2691                     should_rejoin = False\r\n   2692             try:\r\n-> 2693                 results = self.gather(packed, asynchronous=asynchronous, direct=direct)\r\n   2694             finally:\r\n   2695                 for f in futures.values():\r\n\r\n~/miniconda3/lib/python3.9/site-packages/distributed/client.py in gather(self, futures, errors, direct, asynchronous)\r\n   1967             else:\r\n   1968                 local_worker = None\r\n-> 1969             return self.sync(\r\n   1970                 self._gather,\r\n   1971                 futures,\r\n\r\n~/miniconda3/lib/python3.9/site-packages/distributed/client.py in sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\r\n    863             return future\r\n    864         else:\r\n--> 865             return sync(\r\n    866                 self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\r\n    867             )\r\n\r\n~/miniconda3/lib/python3.9/site-packages/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    325     if error[0]:\r\n    326         typ, exc, tb = error[0]\r\n--> 327         raise exc.with_traceback(tb)\r\n    328     else:\r\n    329         return result[0]\r\n\r\n~/miniconda3/lib/python3.9/site-packages/distributed/utils.py in f()\r\n    308             if callback_timeout is not None:\r\n    309                 future = asyncio.wait_for(future, callback_timeout)\r\n--> 310             result[0] = yield future\r\n    311         except Exception:\r\n    312             error[0] = sys.exc_info()\r\n\r\n~/miniconda3/lib/python3.9/site-packages/tornado/gen.py in run(self)\r\n    760 \r\n    761                     try:\r\n--> 762                         value = future.result()\r\n    763                     except Exception:\r\n    764                         exc_info = sys.exc_info()\r\n\r\n~/miniconda3/lib/python3.9/site-packages/distributed/client.py in _gather(self, futures, errors, direct, local_worker)\r\n   1832                             exc = CancelledError(key)\r\n   1833                         else:\r\n-> 1834                             raise exception.with_traceback(traceback)\r\n   1835                         raise exc\r\n   1836                     if errors == \"skip\":\r\n\r\n~/miniconda3/lib/python3.9/site-packages/dask/dataframe/shuffle_service.py in unpack()\r\n     49     This is a task in the task graph\r\n     50     \"\"\"\r\n---> 51     return service.get(i)\r\n     52 \r\n     53 \r\n\r\n~/miniconda3/lib/python3.9/site-packages/dask/dataframe/shuffle_service.py in get()\r\n    726                 # ^ this causes the `process_received_shards` threads to flush any data out\r\n    727                 # of `worker.shuffler`, then stop.\r\n--> 728                 concurrent.futures.wait(self.retrieve_futures)\r\n    729                 self.retrieve_futures.clear()\r\n    730 \r\n\r\nAttributeError: 'ShuffleService' object has no attribute 'retrieve_futures'\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Environment\r\n- Dask version: `'2021.09.1+26.gfd1b02b6'` (#8223)\r\n- Pandas: `1.3.1`\r\n- Python version: `3.9.5`\r\n- Operating System: `Ubuntu 18.04.5 LTS`\r\n- Install method (conda, pip, source): `conda`",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8294/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8294/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}