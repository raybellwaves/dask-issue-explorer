{
    "url": "https://api.github.com/repos/dask/dask/issues/10567",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10567/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10567/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10567/events",
    "html_url": "https://github.com/dask/dask/issues/10567",
    "id": 1949556555,
    "node_id": "I_kwDOAbcwm850M99L",
    "number": 10567,
    "title": "Bug in the graph optimization causing exception to be thrown",
    "user": {
        "login": "AnsgarSchuffenhauer",
        "id": 65599632,
        "node_id": "MDQ6VXNlcjY1NTk5NjMy",
        "avatar_url": "https://avatars.githubusercontent.com/u/65599632?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/AnsgarSchuffenhauer",
        "html_url": "https://github.com/AnsgarSchuffenhauer",
        "followers_url": "https://api.github.com/users/AnsgarSchuffenhauer/followers",
        "following_url": "https://api.github.com/users/AnsgarSchuffenhauer/following{/other_user}",
        "gists_url": "https://api.github.com/users/AnsgarSchuffenhauer/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/AnsgarSchuffenhauer/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/AnsgarSchuffenhauer/subscriptions",
        "organizations_url": "https://api.github.com/users/AnsgarSchuffenhauer/orgs",
        "repos_url": "https://api.github.com/users/AnsgarSchuffenhauer/repos",
        "events_url": "https://api.github.com/users/AnsgarSchuffenhauer/events{/privacy}",
        "received_events_url": "https://api.github.com/users/AnsgarSchuffenhauer/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        },
        {
            "id": 3880424463,
            "node_id": "LA_kwDOAbcwm87nSpQP",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20triage",
            "name": "needs triage",
            "color": "eeeeee",
            "default": false,
            "description": "Needs a response from a contributor"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-10-18T11:50:54Z",
    "updated_at": "2023-11-20T01:46:56Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "**Describe the issue**:\r\nThe compute on  DASK collection raises an exception (see below) when calling compute() without any arguments, but succeeds when using compute(optimize_graph=False)\r\n\r\n```python-traceback\r\n\r\nValueError                                Traceback (most recent call last)\r\nInput In [27], in <cell line: 2>()\r\n      1 import dask.dataframe\r\n----> 2 dask.dataframe.from_dask_array(prox, columns = \"test\").value_counts().compute()\r\n\r\nFile ~/.conda/envs/ml4del_concise_plus/lib/python3.9/site-packages/dask/dataframe/optimize.py:25, in optimize(dsk, keys, **kwargs)\r\n     22 else:\r\n     23     # Perform Blockwise optimizations for HLG input\r\n     24     dsk = optimize_dataframe_getitem(dsk, keys=keys)\r\n---> 25     dsk = optimize_blockwise(dsk, keys=keys)\r\n     26     dsk = fuse_roots(dsk, keys=keys)\r\n     27 dsk = dsk.cull(set(keys))\r\n\r\nFile ~/.conda/envs/ml4del_concise_plus/lib/python3.9/site-packages/dask/blockwise.py:1077, in optimize_blockwise(graph, keys)\r\n   1052 def optimize_blockwise(graph, keys=()):\r\n   1053     \"\"\"High level optimization of stacked Blockwise layers\r\n   1054 \r\n   1055     For operations that have multiple Blockwise operations one after the other, like\r\n   (...)\r\n   1075     rewrite_blockwise\r\n   1076     \"\"\"\r\n-> 1077     out = _optimize_blockwise(graph, keys=keys)\r\n   1078     while out.dependencies != graph.dependencies:\r\n   1079         graph = out\r\n\r\nFile ~/.conda/envs/ml4del_concise_plus/lib/python3.9/site-packages/dask/blockwise.py:1154, in _optimize_blockwise(full_graph, keys)\r\n   1151             stack.append(d)\r\n   1153 # Merge these Blockwise layers into one\r\n-> 1154 new_layer = rewrite_blockwise([layers[l] for l in blockwise_layers])\r\n   1155 out[layer] = new_layer\r\n   1157 # Get the new (external) dependencies for this layer.\r\n   1158 # This corresponds to the dependencies defined in\r\n   1159 # full_graph.dependencies and are not in blockwise_layers\r\n\r\nFile ~/.conda/envs/ml4del_concise_plus/lib/python3.9/site-packages/dask/blockwise.py:1281, in rewrite_blockwise(inputs)\r\n   1273 dependents = reverse_dict(dependencies)\r\n   1275 new_index_iter = (\r\n   1276     c + (str(d) if d else \"\")  # A, B, ... A1, B1, ...\r\n   1277     for d in itertools.count()\r\n   1278     for c in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\r\n   1279 )\r\n-> 1281 [root] = [k for k, v in dependents.items() if not v]\r\n   1283 # Our final results.  These will change during fusion below\r\n   1284 indices = list(inputs[root].indices)\r\n\r\nValueError: too many values to unpack (expected 1)\r\n```\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nSadly enough I didn't succeed to produce a minimally complete verifiable example, without throwing the whole code and data out at this, as apparently the DASK dataframe underlying the example below needs to have some level of complexity for this to show up.  The immediate line of code producing this looks like the following:\r\n\r\n````python\r\ndask_df[dask_df[\"my_bool_col\"]].compute()\r\n````\r\nThe following succeeds without error\r\n\r\n````python\r\ndask_df[dask_df[\"my_bool_col\"]].compute(optimize_graph=False)\r\n````\r\n\r\n**Anything else we need to know?**:\r\n\r\nThe best way to look into this would probably be to export the task graph that fails in optimization some useful way allowing you to reconstitute it and feed it into the optimizer. Please advise how that best can be done in a way most useful to you . For the moment I did execute the \r\n\r\n```python\r\nstr(dask_df[dask_df[\"my_bool_col\"]].__dask_graph__().to_dict())\r\n````\r\nthe result of this is the file below\r\n[task_graph.dict.txt](https://github.com/dask/dask/files/13021531/task_graph.dict.txt)\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2023.8.0 \r\n- Python version: 3.9\r\n- Operating System: linux (CentOS7)\r\n- Install method (conda, pip, source): conda\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10567/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10567/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}