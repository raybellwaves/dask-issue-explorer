{
    "url": "https://api.github.com/repos/dask/dask/issues/9619",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9619/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9619/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9619/events",
    "html_url": "https://github.com/dask/dask/issues/9619",
    "id": 1434649108,
    "node_id": "I_kwDOAbcwm85VgwIU",
    "number": 9619,
    "title": "Read_parquet is slower than expected with S3",
    "user": {
        "login": "mrocklin",
        "id": 306380,
        "node_id": "MDQ6VXNlcjMwNjM4MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mrocklin",
        "html_url": "https://github.com/mrocklin",
        "followers_url": "https://api.github.com/users/mrocklin/followers",
        "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
        "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
        "organizations_url": "https://api.github.com/users/mrocklin/orgs",
        "repos_url": "https://api.github.com/users/mrocklin/repos",
        "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mrocklin/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 51,
    "created_at": "2022-11-03T13:17:46Z",
    "updated_at": "2024-04-29T01:46:23Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "I was looking at a read_parquet profile with @th3ed @ncclementi and @gjoseph92 \r\n\r\nLooking at this performance report: https://raw.githubusercontent.com/coiled/h2o-benchmarks/main/performance-reports-pyarr_str-50GB/q1_50GB_pyarr.html\r\nI see the following analysis (two minute video): https://www.loom.com/share/4c8ad1c5251a4e658c1c47ee2113f34a\r\n\r\nWe're spending only about 20-25% of our time reading from S3, and about 5% of our time converting data to Pandas.  We're spending a lot of our time doing *something else*.\r\n\r\n@gjoseph92 took a look at this with pyspy and generated reports like the following: [tls-10_0_0_177-42425.json](https://speedscope.app/#profileURL=https%3a%2f%2fgistcdn.githack.com%2fgjoseph92%2ff71d825abb5a5d7e44f1cdeb041d8bff%2fraw%2ffbcd35977906999c5df4d88ca9d847a6b7ccf0aa%2ftls-10_0_0_177-42425.json)\r\n\r\nI'm copying a note from him below:\r\n\r\n> What you'll see from this is that pyarrow isn't doing the actual reads. Because dask uses s3fs, the C++ arrow code has to call back into Python for each read. Ultimately, the reads are actually happening on the fsspec event loop (see the `fsspecIO` thread in profiles). If we look there, about 40% of CPU time is spent waiting for something (aka data from S3, good), but 60% is spent doing stuff in Python (which I'd consider overhead, to some degree).\r\n\r\n> We can also see that 30% of the total time is spent blocking on Python's GIL (all the `pthread_cond_timedwait`s) (look at the functions calling into this and the corresponding lines in the Python source if you don't believe me; they're all `Py_END_ALLOW_THREADS`). This is an issue known as the convoy effect: https://bugs.python.org/issue7946, https://github.com/dask/distributed/issues/6325.\r\n\r\n> My takeaway is that using fsspec means dask is using Python for reads, which might be adding significant overhead / reducing parallelism due to the GIL.\r\n\r\n> I'd be interested in doing a comparison by hacking together a version that bypasses fsspec, and uses pyarrow's native [S3FileSystem](https://arrow.apache.org/docs/python/generated/pyarrow.fs.S3FileSystem.html#pyarrow.fs.S3FileSystem) directly. Before that though, it might be good to get some baseline numbers on how fast we can pull the raw data from S3 (just as bytes), to understand what performance we can expect.\r\n\r\n> FYI I also tried https://developer.nvidia.com/blog/optimizing-access-to-parquet-data-with-fsspec/, but it was ~2x slower. Haven't tried repeating that though, so not sure if it's a real result.\r\n\r\n> One other thing I find surprising is that polars appears to be using fsspec for reads as well, rather than the native S3FileSystem or GCSFileSystem:\r\n> https://github.com/pola-rs/polars/blob/445c550e8f965d9e8f2da1cb2d01b6c15874f6c8/py-polars/polars/io.py#L949-L956\r\n> https://github.com/pola-rs/polars/blob/445c550e8f965d9e8f2da1cb2d01b6c15874f6c8/py-polars/polars/internals/io.py#L114-L121\r\n\r\n> I would have expected polars and dask read performance to be closer in this case. We should probably confirm for ourselves that they're not.\r\n\r\nIt looks like we could make things a lot faster.  I'm curious about the right steps to isolate the problem further.\r\n\r\ncc'ing @martindurant @rjzamora @ritchie46 @fjetter ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9619/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9619/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}