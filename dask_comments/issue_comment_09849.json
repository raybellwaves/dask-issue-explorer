[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1398799162",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1398799162",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1398799162,
        "node_id": "IC_kwDOAbcwm85TX_s6",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-20T18:49:17Z",
        "updated_at": "2023-01-20T18:49:17Z",
        "author_association": "MEMBER",
        "body": "Note that https://github.com/dask/dask/pull/9637 is proposing the addition of a blocksize argument, but is using it in a slightly different way (at least for now). In that PR we simply parse the parquet metadata for the first file in the dataset, and use the `total_byte_size` metadata to estimate the expected size of a row-group in the dataset. From there, the `split_row_group` argument is automatically set to satisfy the default or user-specified `blocksize` argument.\r\n\r\nDoes this approach seem like a reasonable first step?\r\n\r\n>Add the blocksize parameter to read_parquet. This will simply measure the size on disk of each partition, load the partition with a single task and, if it exceeds blocksize, split it into equal parts.\r\n\r\nNot sure I completely understand this plan. The result of `read_parquet` is expected to be a `Blockwise` mapping between some abstract `read` function (designed to output a single DataFrame partition) and a list of sources (path and row-group-list combinations). Therefore, a single task within the `read_parquet` graph is required to produce a single partition, otherwise a follow-on operation will be constructing a graph with the wrong number of input keys.\r\n\r\nThere is a good chance that I am misunderstanding the proposal here, so feel free to clarify my confusion.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1398799162/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402107389",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402107389",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402107389,
        "node_id": "IC_kwDOAbcwm85TknX9",
        "user": {
            "login": "crusaderky",
            "id": 6213168,
            "node_id": "MDQ6VXNlcjYyMTMxNjg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/crusaderky",
            "html_url": "https://github.com/crusaderky",
            "followers_url": "https://api.github.com/users/crusaderky/followers",
            "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
            "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
            "organizations_url": "https://api.github.com/users/crusaderky/orgs",
            "repos_url": "https://api.github.com/users/crusaderky/repos",
            "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
            "received_events_url": "https://api.github.com/users/crusaderky/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T15:12:52Z",
        "updated_at": "2023-01-24T15:12:52Z",
        "author_association": "MEMBER",
        "body": "> Not sure I completely understand this plan. \r\n\r\nIt's about converting the load, which is currently happening in a single layer, into two layers: the first layer loads whole row groups; the second layer is a 1-to-n slicing that breaks down the chunks.\r\n\r\ne.g. we call ``read_parquet(\"s3://foo/bar.parquet\", block_size=\"100 MiB\")``.\r\n\r\n- s3://foo/bar.parquet/0.parquet: 150 MiB, 20k rows\r\n- s3://foo/bar.parquet/1.parquet: 80 MiB, 12k rows\r\n- s3://foo/bar.parquet/2.parquet: 75 MiB, 11k rows\r\n\r\ndd.read_parquet would output:\r\n\r\n- (read, 0): (read_parquet, s3://foo/bar.parquet/0.parquet), \r\n- (read, 1): (read_parquet, s3://foo/bar.parquet/1.parquet),\r\n- (read, 2): (read_parquet, s3://foo/bar.parquet/2.parquet),\r\n- (repartition, 0): (getitem, (read, 0), slice(0, 10_000)),\r\n- (repartition, 1): (getitem, (read, 0), slice(10_000, 20_000)),\r\n- (repartition, 2): (read, 1)\r\n- (repartition, 3): (read, 2)\r\n\r\nwhich is the same thing that `dd.read_parquet().repartition(partition_size=...)` does today, minus the need to read everything from disk twice.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402107389/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402159991",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402159991",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402159991,
        "node_id": "IC_kwDOAbcwm85Tk0N3",
        "user": {
            "login": "crusaderky",
            "id": 6213168,
            "node_id": "MDQ6VXNlcjYyMTMxNjg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/crusaderky",
            "html_url": "https://github.com/crusaderky",
            "followers_url": "https://api.github.com/users/crusaderky/followers",
            "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
            "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
            "organizations_url": "https://api.github.com/users/crusaderky/orgs",
            "repos_url": "https://api.github.com/users/crusaderky/repos",
            "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
            "received_events_url": "https://api.github.com/users/crusaderky/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T15:41:33Z",
        "updated_at": "2023-01-24T15:41:33Z",
        "author_association": "MEMBER",
        "body": "@rjzamora tangential topic to https://github.com/dask/dask/pull/9637: I'm looking now at a dataset, `s3://coiled-datasets/prefect-dask/nyc-uber-lyft/processed_data.parquet`, where partition size varies wildly, between 22MiB and 836 MiB. \r\n\r\n`dd.read_parquet(..., split_row_groups=1)` doesn't do anything; if I call `[len(f.row_groups) for f in ds.get_fragments()]` it tells me that each partition - including the 800+ MiB ones - contain exactly one row group.\r\n\r\nThe dataset was created with `dd.DataFrame.to_parquet()`. I couldn't find any option there to set row group size, but maybe I overlooked it?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402159991/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402200936",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402200936",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402200936,
        "node_id": "IC_kwDOAbcwm85Tk-No",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T16:08:13Z",
        "updated_at": "2023-01-24T16:08:41Z",
        "author_association": "MEMBER",
        "body": ">The dataset was created with dd.DataFrame.to_parquet(). I couldn't find any option there to set row group size, but maybe I overlooked it?\r\n\r\nThe `pyarrow.parquet.write_table` function takes a `row_group_size` argument to specify the number of rows you want in each row-group, so you can do something like `ddf.to_parquet(path, engine=\"pyarrow\", row_group_size=1000)` to specify that each row-group should contain no more than 1000 rows.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402200936/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402229832",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402229832",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402229832,
        "node_id": "IC_kwDOAbcwm85TlFRI",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T16:27:12Z",
        "updated_at": "2023-01-24T16:27:12Z",
        "author_association": "MEMBER",
        "body": ">It's about converting the load, which is currently happening in a single layer, into two layers: the first layer loads whole row groups; the second layer is a 1-to-n slicing that breaks down the chunks....\r\nwhich is the same thing that dd.read_parquet().repartition(partition_size=...) does today, minus the need to read everything from disk twice.\r\n\r\n`read_parquet` is a lazy operation and does not currently read in any data. Therefore it's not clear to me why `dd.read_parquet().repartition(partition_size=...)` would be less performant that the two-layer approach you are describing.  As far as I can tell, you are proposing that `read_parquet` should no longer be a lazy operation, but should instead read in the data and repartition eagerly. Do I have that right?\r\n\r\n**Side Note 1**: While I am likely to be very interested and supportive of better-optimized IO algorithms, I will most likely push back on anything besides \"simplification\" in `read_parquet` itself. That is, anything that is not traditional lazy behavior probably belongs in a separate API.\r\n\r\n**Side Note 2**: Instead of reading in real data, and repartitioning after the fact, I **have** been experimenting with the idea of collecting a full metadata summary of the dataset upfront (using Delayed for large/remote datasets).  Using this approach,  the uncompressed size of each row-group (in parquet format) can be collected fairly quickly, and a custom partitioning plan can be computed **before** any real data is ingested.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402229832/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402234035",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402234035",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402234035,
        "node_id": "IC_kwDOAbcwm85TlGSz",
        "user": {
            "login": "crusaderky",
            "id": 6213168,
            "node_id": "MDQ6VXNlcjYyMTMxNjg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/crusaderky",
            "html_url": "https://github.com/crusaderky",
            "followers_url": "https://api.github.com/users/crusaderky/followers",
            "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
            "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
            "organizations_url": "https://api.github.com/users/crusaderky/orgs",
            "repos_url": "https://api.github.com/users/crusaderky/repos",
            "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
            "received_events_url": "https://api.github.com/users/crusaderky/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T16:30:11Z",
        "updated_at": "2023-01-24T16:30:11Z",
        "author_association": "MEMBER",
        "body": "> As far as I can tell, you are proposing that read_parquet should no longer be a lazy operation, but should instead read in the data and repartition eagerly. Do I have that right?\r\n\r\nNo, I am suggesting that read_parquet should eagerly read **file sizes**. It won't open the actual files.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402234035/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402237249",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402237249",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402237249,
        "node_id": "IC_kwDOAbcwm85TlHFB",
        "user": {
            "login": "crusaderky",
            "id": 6213168,
            "node_id": "MDQ6VXNlcjYyMTMxNjg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/crusaderky",
            "html_url": "https://github.com/crusaderky",
            "followers_url": "https://api.github.com/users/crusaderky/followers",
            "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
            "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
            "organizations_url": "https://api.github.com/users/crusaderky/orgs",
            "repos_url": "https://api.github.com/users/crusaderky/repos",
            "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
            "received_events_url": "https://api.github.com/users/crusaderky/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T16:32:20Z",
        "updated_at": "2023-01-24T16:32:20Z",
        "author_association": "MEMBER",
        "body": "> The `pyarrow.parquet.write_table` function takes a `row_group_size` argument to specify the number of rows you want in each row-group, so you can do something like `ddf.to_parquet(path, engine=\"pyarrow\", row_group_size=1000)` to specify that each row-group should contain no more than 1000 rows.\r\n\r\nThis should definitely be in the documentation of `to_parquet` and IMHO it should default to something sensibly small.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402237249/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1402242038",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1402242038",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1402242038,
        "node_id": "IC_kwDOAbcwm85TlIP2",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-24T16:35:29Z",
        "updated_at": "2023-01-24T16:35:29Z",
        "author_association": "MEMBER",
        "body": ">No, I am suggesting that read_parquet should eagerly read file sizes. It won't open the actual files.\r\n\r\nOkay got it - Thanks for clearing that up!\r\n\r\n>This should definitely be in the documentation of to_parquet and IMHO it should default to something sensibly small.\r\n\r\nAgree. We don't really document engine-specific options, but this one does seem important.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1402242038/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1538872707",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1538872707",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1538872707,
        "node_id": "IC_kwDOAbcwm85buVWD",
        "user": {
            "login": "novdanody",
            "id": 98500306,
            "node_id": "U_kgDOBd7-0g",
            "avatar_url": "https://avatars.githubusercontent.com/u/98500306?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/novdanody",
            "html_url": "https://github.com/novdanody",
            "followers_url": "https://api.github.com/users/novdanody/followers",
            "following_url": "https://api.github.com/users/novdanody/following{/other_user}",
            "gists_url": "https://api.github.com/users/novdanody/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/novdanody/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/novdanody/subscriptions",
            "organizations_url": "https://api.github.com/users/novdanody/orgs",
            "repos_url": "https://api.github.com/users/novdanody/repos",
            "events_url": "https://api.github.com/users/novdanody/events{/privacy}",
            "received_events_url": "https://api.github.com/users/novdanody/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-08T18:51:34Z",
        "updated_at": "2023-05-08T18:51:34Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hi! I am new to contributing to dask and would like to pick up this issue. I am looking into implementing blocksize in read_parquet as described above, but have two questions on the implemtnation detail:\r\n1. Would the file size collection be done during the gather statistics part or when calling from_map on each parts of the parquet dataset?\r\n2. Would this imply refactoring the ParquetFunctionWrapper to call repartition after calling read_parquet_part?\r\n\r\nThanks for the clarification!",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1538872707/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1539168226",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1539168226",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1539168226,
        "node_id": "IC_kwDOAbcwm85bvdfi",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-08T22:59:42Z",
        "updated_at": "2023-05-08T23:00:46Z",
        "author_association": "MEMBER",
        "body": "Welcome @novdanody !\r\n\r\nPlease note that a `blocksize` argument has already been added to `read_parquet` since this issue was first filed. For the implemented feature, however, `blocksize` is only used to dynamically choose the number of row-groups to assign to each output partition.  While this has made oversized partitions less prevalent, it has not address the case that each file is a single/giant row-group.\r\n\r\nCan you provide some context on your motivation to add this feature?\r\n\r\nIt does seem possible that we could still split large partitions if/when the uncompressed storage size of a row-group is larger than `blocksize`. For example, we could probably use the statistics (processed in [process_statistics](https://github.com/dask/dask/blob/e9845aadc7c32078e55354b61f5626ac759e8b28/dask/dataframe/io/parquet/core.py#L1417)) to figure out which output partitions need to be split. (Note that I would **not** use the file size, since compression is common in parquet)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1539168226/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1540159936",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1540159936",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1540159936,
        "node_id": "IC_kwDOAbcwm85bzPnA",
        "user": {
            "login": "novdanody",
            "id": 98500306,
            "node_id": "U_kgDOBd7-0g",
            "avatar_url": "https://avatars.githubusercontent.com/u/98500306?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/novdanody",
            "html_url": "https://github.com/novdanody",
            "followers_url": "https://api.github.com/users/novdanody/followers",
            "following_url": "https://api.github.com/users/novdanody/following{/other_user}",
            "gists_url": "https://api.github.com/users/novdanody/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/novdanody/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/novdanody/subscriptions",
            "organizations_url": "https://api.github.com/users/novdanody/orgs",
            "repos_url": "https://api.github.com/users/novdanody/repos",
            "events_url": "https://api.github.com/users/novdanody/events{/privacy}",
            "received_events_url": "https://api.github.com/users/novdanody/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-09T13:47:10Z",
        "updated_at": "2023-05-09T13:47:10Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks for your reply @rjzamora! I was just looking around for a good second issue and this issue pops up. It seems interesting and I was interested in learning more about the implementation details of read_parquet since I use it quite often. \r\n\r\nTwo follow ups to your reply:\r\n1. Does this mean in terms of usage we want toavoid implementing blocksize in read_parquet as described in this issue, and rather increase the number of row-groups in each file when writing the datasets?\r\n2. Are we still looking to implement blocksize to non-line json files and other read_* functions in as described in this issue #9850 ?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1540159936/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1542275767",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1542275767",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1542275767,
        "node_id": "IC_kwDOAbcwm85b7UK3",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-10T14:04:14Z",
        "updated_at": "2023-05-10T14:04:14Z",
        "author_association": "MEMBER",
        "body": ">Does this mean in terms of usage we want toavoid implementing blocksize in read_parquet as described in this issue, and rather increase the number of row-groups in each file when writing the datasets?\r\n\r\nGiven that `read_parquet` is already pretty \"complex\" (in terms of the large number of options and multiple engines) and used so extensively, we will certainly need to be picky about any new options or significant changes in behavior. Also, since the `Engine` API is being consumed by other libraries like dask-cudf and dask-geopandas, you might find this to be a relatively frustrating \"second issue\" :)\r\n\r\nFor those who have control over the creation of the parquet dataset, it is indeed best for the files to be created with reasonable row-group sizes (~128MiB or less).  With that said, there are many large-row-group datasets out in the wild, and it can often be inconvenient and/or impractical to rewrite the data.  Therefore, I do still think this feature could be valuable (if it is approached carefully).\r\n\r\n>Are we still looking to implement blocksize to non-line json files and other read_* functions in as described in this issue https://github.com/dask/dask/issues/9850 ?\r\n\r\nI do think it makes sense to have a `blocksize` argument for all IO functions, and other formats should be more straightforward.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1542275767/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1544200925",
        "html_url": "https://github.com/dask/dask/issues/9849#issuecomment-1544200925",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9849",
        "id": 1544200925,
        "node_id": "IC_kwDOAbcwm85cCqLd",
        "user": {
            "login": "novdanody",
            "id": 98500306,
            "node_id": "U_kgDOBd7-0g",
            "avatar_url": "https://avatars.githubusercontent.com/u/98500306?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/novdanody",
            "html_url": "https://github.com/novdanody",
            "followers_url": "https://api.github.com/users/novdanody/followers",
            "following_url": "https://api.github.com/users/novdanody/following{/other_user}",
            "gists_url": "https://api.github.com/users/novdanody/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/novdanody/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/novdanody/subscriptions",
            "organizations_url": "https://api.github.com/users/novdanody/orgs",
            "repos_url": "https://api.github.com/users/novdanody/repos",
            "events_url": "https://api.github.com/users/novdanody/events{/privacy}",
            "received_events_url": "https://api.github.com/users/novdanody/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-11T15:28:04Z",
        "updated_at": "2023-05-11T15:28:04Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks! I will familiarize myself with the codebase with other changes to start and avoid read_parquet for now.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1544200925/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]