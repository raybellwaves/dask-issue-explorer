{
    "url": "https://api.github.com/repos/dask/dask/issues/7702",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/7702/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/7702/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/7702/events",
    "html_url": "https://github.com/dask/dask/issues/7702",
    "id": 900664459,
    "node_id": "MDU6SXNzdWU5MDA2NjQ0NTk=",
    "number": 7702,
    "title": "Allow encoding execution priorities / order in delayed objects / graph",
    "user": {
        "login": "mlondschien",
        "id": 61679398,
        "node_id": "MDQ6VXNlcjYxNjc5Mzk4",
        "avatar_url": "https://avatars.githubusercontent.com/u/61679398?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mlondschien",
        "html_url": "https://github.com/mlondschien",
        "followers_url": "https://api.github.com/users/mlondschien/followers",
        "following_url": "https://api.github.com/users/mlondschien/following{/other_user}",
        "gists_url": "https://api.github.com/users/mlondschien/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mlondschien/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mlondschien/subscriptions",
        "organizations_url": "https://api.github.com/users/mlondschien/orgs",
        "repos_url": "https://api.github.com/users/mlondschien/repos",
        "events_url": "https://api.github.com/users/mlondschien/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mlondschien/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1968982461,
            "node_id": "MDU6TGFiZWwxOTY4OTgyNDYx",
            "url": "https://api.github.com/repos/dask/dask/labels/delayed",
            "name": "delayed",
            "color": "4f6edd",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2021-05-25T11:17:46Z",
    "updated_at": "2021-10-14T07:53:15Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "## Setup\r\n\r\nI have a function as follows:\r\n```python\r\ndef my_function(ddf: dd.DataFrame, list_of_args: List[Dict]) -> List[Delayed]:\r\n    ddf_tasks = ddf.to_delayed()\r\n\r\n    reduced_results = []\r\n    for args in list_of_args:\r\n        reduced_results.append([delayed(reduce)(task, args) for task in ddf_tasks])\r\n\r\n    combined_results = [delayed(combine)(result, args) for result in reduced_result]\r\n    return combined_results \r\n```\r\nHere `reduce` creates summary statistics about a `pandas.DataFrame` and `combine` combines these summaries. A motivating example would be to compute the percentage of missings in an individual column (I know dask can do this, these get arbitrarily more complex):\r\n\r\n```python\r\ndef reduce(df, args):\r\n    return {\"n\": len(df), \"n_missing\": df[args[\"column\"]].isna().sum()}\r\n\r\n\r\ndef combine(results, args):\r\n    n_missing = sum(result[\"n_missing\"] for result in results)\r\n    n = sum(result[\"n\"] for result in results)\r\n    return n_missing / n\r\n```\r\n\r\nA typical call of `my_function` would then be as follows:\r\n```python\r\nfrom kartothek.io.dask.dataframe import read_dataset_as_ddf\r\nimport minimalkv\r\n\r\nstore = minimalkv.get_store_from_url(\"hfs:///path/to/file\")\r\n\r\nddf = read_dataset_as_ddf(uuid=\"uuid\", store=store)\r\n\r\nlist_of_args = [{\"column\": \"a\"}, {\"column\": \"b\"}, {\"column\": \"c\"}]\r\n\r\nresult = my_function(ddf, list_of_args)\r\n```\r\n\r\nHere each task in `ddf.to_delayed()` is basically a delayed call of `pd.read_parquet`. Importantly the result of these delayed calls is big and `ddf.compute()` will not fit into memory at once. However the return of the `reduce` functions is small. We can assume that all results of all `reduce` calls fit into memory at once, if not we would apply a tree-reduce algorithm.\r\n\r\nOn a small sample, this is the result of `visualize(result, filename=\"file.jpeg\", color=\"order\", cmap=\"autumn\", optimize_graph=True)`:\r\n\r\n![file](https://user-images.githubusercontent.com/61679398/119481291-d1d07680-bd52-11eb-934b-61cb98666323.jpeg)\r\n\r\nThe square nodes 0, ..., 4 contain large amounts of data. Optimally, dask would first compute one of them (say 0), compute the results of all dependencies without further dependencies (here 7, 29 and 22) and then discard the data (0) to free up memory. Sadly, this is not what happens. In the graph the nodes are by execution order. IIUC, according to [this](https://docs.dask.org/en/latest/order.html), dask selects one of the final nodes (20) and prioritizes its dependencies (3, 15, 11 and 19). These have dependencies 4, 2, 3, 1, 0, so dask (tries to) load all data into memory. It does not free the memory once children are computed, since the data is also a parent of other nodes further down in the queue (e.g. 26 / 21, 25, 24, 23, 22). As all data does not fit into memory, dask begins writing to / loading from disk.\r\n\r\n## Request / Question\r\n\r\nIt would be nice to reprioritize the above graph and encode the priorisation in the graph / delayed objects. If I understand correctly, currenlty, as described [here](https://distributed.dask.org/en/latest/priority.html), priorities can only be passed to a scheduler, but not encoded in the graph. In case I could encode priorities, I could add checkpoints that make sure the dask gets executed in the order that I would like. Here I would probably add something like\r\n\r\n```python\r\nfrom dask.graph_manipulation import checkpoint\r\n\r\ncheckpoints = [\r\n    checkpoint(\r\n        *[reduced_result[i] for reduced_result in reduced_results],\r\n        priority=i\r\n     ) for i in range(len(ddf_tasks))\r\n]\r\n```\r\nto my function and integrate the checkpoints into the dask graph. \r\n\r\nDo you have any suggestions on how to implement this with existing tools? Boundary conditions are:\r\n - All data does not fit into memory.\r\n - The input data frame might contain some computation. We should thus not copy its graph multiple times over.\r\n - The dask execution should make use of parallelization. Making loading of a partition (e.g. 2) dependent on all children of another (e.g. 3, 27, 21, children of 4) would break this.\r\n\r\nOptimally, each worker would load a separate partition of the dask data frame, compute all dependencies up to the combine step, keep these results in memory but discard the data and continue with the next partition.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/7702/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/7702/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}