{
    "url": "https://api.github.com/repos/dask/dask/issues/9888",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9888/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9888/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9888/events",
    "html_url": "https://github.com/dask/dask/issues/9888",
    "id": 1560000679,
    "node_id": "I_kwDOAbcwm85c-7in",
    "number": 9888,
    "title": "Reuse of keys in blockwise fusion can cause spurious KeyErrors on distributed cluster",
    "user": {
        "login": "fjetter",
        "id": 8629629,
        "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fjetter",
        "html_url": "https://github.com/fjetter",
        "followers_url": "https://api.github.com/users/fjetter/followers",
        "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
        "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
        "organizations_url": "https://api.github.com/users/fjetter/orgs",
        "repos_url": "https://api.github.com/users/fjetter/repos",
        "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
        "received_events_url": "https://api.github.com/users/fjetter/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2156573524,
            "node_id": "MDU6TGFiZWwyMTU2NTczNTI0",
            "url": "https://api.github.com/repos/dask/dask/labels/highlevelgraph",
            "name": "highlevelgraph",
            "color": "8c24d6",
            "default": false,
            "description": "Issues relating to HighLevelGraphs."
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 13,
    "created_at": "2023-01-27T15:57:58Z",
    "updated_at": "2024-03-18T01:45:08Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "Subsequent Blockwise layers are currently fused into a single layer. This reduces the number of tasks, the overhead and is very generally a good thing to do. Currently, the fused output does not generate unique key names which is a problem from a UX perspective but can also cause severe failure cases when being executed on the distributed scheduler since distributed assumes that a task key is a unique identifier for the **entire task**. While it is true that the data output of the fused key and the non-fused key is identical, the runspec and the local topology is intentionally very different. Specifically, a fused task, for example, may not have any dependencies while the non-fused task does have dependencies.\r\n\r\nAn example where this matters is the following (async code is not necessary but the condition is actually a bit difficult to trigger and this helps. Paste this code in a Jupyter notebook and run it a couple of times).\r\n\r\n\r\n```python\r\nimport asyncio\r\nfrom distributed import Client, Scheduler, Worker\r\n\r\nimport dask\r\nimport time\r\nimport dask.dataframe as dd\r\nasync with (\r\n    Scheduler() as s,\r\n    Worker(s.address) as a,\r\n    Worker(s.address) as b,\r\n    Client(s.address, asynchronous=True) as c\r\n):\r\n    df = dask.datasets.timeseries(\r\n        start=\"2000-01-01\",\r\n        end=\"2000-01-10\",\r\n        dtypes={\"x\": float, \"y\": float},\r\n        freq=\"100 s\",\r\n    )\r\n    out = dd.shuffle.shuffle(df, \"x\")\r\n    out = out.persist()\r\n\r\n    while not a.tasks:\r\n        await asyncio.sleep(0.05)\r\n\r\n    del out\r\n\r\n    out = dd.shuffle.shuffle(df, \"x\")\r\n    x, y = c.compute([df.x.size, out.x.size])\r\n    x = await c.gather(x, y)\r\n```\r\n\r\nNote how initial shuffle is persisted and a slightly different version of this graph is computed *again* below but the graph below is slightly different. From what I can gather, the initial persist is fusing the keys while the latter one does not (maybe it's the other way round, I'm not sure. Either way a different issue).\r\n\r\nThis specific reproducer actually triggers (not every time) a `KeyError` in a workers data buffer while trying to read data. \r\n\r\n```\r\n023-01-27 16:28:34,648 - distributed.worker - ERROR - Exception during execution of task ('size-chunk-0482e0de93343089cd64837d139d9a80-49c0404470df4695b3f5aa383f11c345', 3).\r\nTraceback (most recent call last):\r\n  File \"/Users/fjetter/workspace/distributed/distributed/worker.py\", line 2366, in _prepare_args_for_execution\r\n    data[k] = self.data[k]\r\n  File \"/Users/fjetter/workspace/distributed/distributed/spill.py\", line 257, in __getitem__\r\n    return super().__getitem__(key)\r\n  File \"/Users/fjetter/mambaforge/envs/dask-distributed-310/lib/python3.10/site-packages/zict/buffer.py\", line 108, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: \"('simple-shuffle-539a650502e21de2dabd2021c9c9e684', 3)\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/fjetter/workspace/distributed/distributed/worker.py\", line 2247, in execute\r\n    args2, kwargs2 = self._prepare_args_for_execution(ts, args, kwargs)\r\n  File \"/Users/fjetter/workspace/distributed/distributed/worker.py\", line 2370, in _prepare_args_for_execution\r\n    data[k] = Actor(type(self.state.actors[k]), self.address, k, self)\r\nKeyError: \"('simple-shuffle-539a650502e21de2dabd2021c9c9e684', 3)\"\r\n```\r\n\r\nThis is caused because the dependency relations between tasks are no longer accurate on the scheduler and it considers a task \"ready\", i.e. all dependencies in memory, too soon, causing a failure on the worker.\r\nWhen `validate` is activated, the scheduler catches these cases earlier and raises appropriate AssertionErrors. This is not checked at runtime for performance reasons and is typically not necessary since we rely on the assumption that keys identify a task uniquely.\r\n\r\nApart from this artificial example, we do have internal reports about such a spurious KeyError in combination with an xgboost workload",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9888/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9888/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}