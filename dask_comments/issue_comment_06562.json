[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/684796786",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-684796786",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 684796786,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY4NDc5Njc4Ng==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-09-01T11:53:43Z",
        "updated_at": "2020-09-01T11:53:43Z",
        "author_association": "MEMBER",
        "body": "Thanks for the report. It'd be great if you could dig into things a little further to simplify.\r\n\r\nI'd start with figuring out why `pivot_count`, which is just calling pandas' `pivot_table` is taking longer in the case of reading from csv. Something strange is going on there.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/684796786/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/717112258",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-717112258",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 717112258,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNzExMjI1OA==",
        "user": {
            "login": "klnrdknt",
            "id": 35272820,
            "node_id": "MDQ6VXNlcjM1MjcyODIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/35272820?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/klnrdknt",
            "html_url": "https://github.com/klnrdknt",
            "followers_url": "https://api.github.com/users/klnrdknt/followers",
            "following_url": "https://api.github.com/users/klnrdknt/following{/other_user}",
            "gists_url": "https://api.github.com/users/klnrdknt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/klnrdknt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/klnrdknt/subscriptions",
            "organizations_url": "https://api.github.com/users/klnrdknt/orgs",
            "repos_url": "https://api.github.com/users/klnrdknt/repos",
            "events_url": "https://api.github.com/users/klnrdknt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/klnrdknt/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-27T09:35:54Z",
        "updated_at": "2020-10-27T09:35:54Z",
        "author_association": "NONE",
        "body": "I have a similar problem with pivot_table, except that for me it crashes the worker. The laptop I work on has 16 GB RAM. I'm trying to import around 8 files \u00e0 5-6GB (long format) and pivot them. I would have believed that the way Dask is designed, that this is specifically a problem it was built to tackle.\r\n\r\nThe pivot_table is a little frustrating, since pandas seems to be able to handle it, as I can use chunked import, pivot each chunk and then concat all chunks at the end. I was just hoping that dask would speed this process up, since pandas can of course only use a single process.\r\n\r\nIs there any recommended way to get around this?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/717112258/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/717238352",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-717238352",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 717238352,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNzIzODM1Mg==",
        "user": {
            "login": "TomAugspurger",
            "id": 1312546,
            "node_id": "MDQ6VXNlcjEzMTI1NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1312546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TomAugspurger",
            "html_url": "https://github.com/TomAugspurger",
            "followers_url": "https://api.github.com/users/TomAugspurger/followers",
            "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
            "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
            "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
            "repos_url": "https://api.github.com/users/TomAugspurger/repos",
            "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-27T13:20:49Z",
        "updated_at": "2020-10-27T13:20:49Z",
        "author_association": "MEMBER",
        "body": "I think we're still looking for someone to investigate the issue @klnrdknt. Are you interested?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/717238352/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/717424552",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-717424552",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 717424552,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNzQyNDU1Mg==",
        "user": {
            "login": "klnrdknt",
            "id": 35272820,
            "node_id": "MDQ6VXNlcjM1MjcyODIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/35272820?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/klnrdknt",
            "html_url": "https://github.com/klnrdknt",
            "followers_url": "https://api.github.com/users/klnrdknt/followers",
            "following_url": "https://api.github.com/users/klnrdknt/following{/other_user}",
            "gists_url": "https://api.github.com/users/klnrdknt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/klnrdknt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/klnrdknt/subscriptions",
            "organizations_url": "https://api.github.com/users/klnrdknt/orgs",
            "repos_url": "https://api.github.com/users/klnrdknt/repos",
            "events_url": "https://api.github.com/users/klnrdknt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/klnrdknt/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-27T18:04:50Z",
        "updated_at": "2020-10-27T18:04:50Z",
        "author_association": "NONE",
        "body": "Well, apart from the fact that this is probably out of my expertise, I can try to generate some more info. An in depth investigation is probably above me, unless you give me some hints on where to look and which tools to employ for this.\r\n\r\nFirst I'll try to provide a more in-depth description of my issue.\r\n\r\nCHAPTER 1:\r\n\r\nFor a machine of 16 GB RAM I created a test dataset using:\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom datetime import datetime\r\nimport dask\r\n\r\n# create random datetime\r\ndef pp(start, end, n):\r\n    start_u = start.value//10**9\r\n    end_u = end.value//10**9\r\n\r\n    return pd.DatetimeIndex((10**9*np.random.randint(start_u, end_u, n, dtype=np.int64)).view('M8[ns]'))\r\n\r\nfor i in range(9):\r\n    ids = np.random.randint(0, 50, size=int(120e6))\r\n    data = np.random.randn(int(120e6))\r\n    index = pp(pd.to_datetime(\"2020-01-01\"), pd.to_datetime(\"2020-03-30\"), int(120e6))\r\n    df = pd.DataFrame()\r\n    df[\"Id\"] = ids\r\n    df[\"Values\"] = data\r\n    df[\"Time\"] = index\r\n    df.to_csv(f\"test_{i}.csv\", index=False)\r\n```\r\n\r\nThis creates a couple of files which are around 5.1 GB, which is roughly what I use in my real example. The rows and contents are also the same, apart from the fact that there should be some rough order to the datetimeindex with real world data. However, it also did not make sense to create a linear index, as we want some indices to overlap.\r\n\r\nThese are the operations I perform on the data:\r\n\r\n```python\r\nfrom dask.distributed import Client\r\nclient = Client()\r\nclient\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/35272820/97337018-02653600-1880-11eb-8695-2037083670de.png)\r\n\r\n```python\r\nimport dask.dataframe as dd\r\ndf = dd.read_csv(\"test_*.csv\", assume_missing=True)\r\ndf[\"Time\"] = dd.to_datetime(\r\n    df[\"Time\"],\r\n    format=\"%Y-%m-%d %H:%M:%S\"\r\n)\r\ndf = df.dropna(subset=[\"Time\"])\r\ndf[\"Time\"] = df[\"Time\"].dt.round(\"S\")\r\ndf = df.categorize(columns=[\"Id\"])\r\ndf = df.pivot_table(\r\n    index=\"Time\",\r\n    columns=\"Id\",\r\n    values=\"Values\"\r\n)\r\ndf = df.groupby(\"Time\").mean()\r\ndf.columns = list(df.columns)\r\ndf = df.reset_index().set_index(\"Time\")\r\ndf = df.resample(\"10S\").mean()\r\ndf = df.dropna(how=\"all\")\r\nprint(df)\r\n```\r\n\r\nUsing the standard worker configuration as provided by the call to `Client()` this crashes even before pivoting and without any call to `compute()` or `persist()`.\r\n\r\n![image](https://user-images.githubusercontent.com/35272820/97337209-47896800-1880-11eb-9eac-1c5e2e28ddf8.png)\r\n\r\nI've tried avoiding this error by using less threads and increasing memory size. That makes it a whole lot slower and doesnt change when it crashes.\r\n\r\nI'm however using my own PC, not the one at work so the setup might be a little different.\r\n\r\nThinking about this, I guess the main difference between this dataset here and the one at work is that the indices aren't ordered at all, while the dates in the real dataset tend to be almost in correct order with some overlaps here and there.\r\n\r\nCHAPTER 2\r\n\r\nSo I'm going to try a different approach, ordering the indices before pivoting.\r\n\r\n```python\r\ndf = dd.read_csv(\"test_*.csv\", assume_missing=True)\r\ndf[\"Time\"] = dd.to_datetime(\r\n    df[\"Time\"],\r\n    format=\"%Y-%m-%d %H:%M:%S\"\r\n)\r\ndf = df.dropna(subset=[\"Time\"])\r\ndf[\"Time\"] = df[\"Time\"].dt.round(\"S\")\r\n# added this line ########\r\ndf.set_index(\"Time\").reset_index()\r\n##########################\r\ndf = df.categorize(columns=[\"Id\"])\r\ndf = df.pivot_table(\r\n    index=\"Time\",\r\n    columns=\"Id\",\r\n    values=\"Values\"\r\n)\r\ndf = df.groupby(\"Time\").mean()\r\ndf.columns = list(df.columns)\r\ndf = df.reset_index().set_index(\"Time\")\r\ndf = df.resample(\"10S\").mean()\r\ndf = df.dropna(how=\"all\")\r\nprint(df)\r\n```\r\n\r\nI'm using standard worker configuration again for my machine, which is set by dask to be:\r\n\r\n![image](https://user-images.githubusercontent.com/35272820/97340186-bd430300-1883-11eb-8fbb-e3a9aadb92c9.png)\r\n\r\nThis performs a lot of operations making me believe it will work well this time, but well... it crashed on the read_csv operations...\r\n\r\n![image](https://user-images.githubusercontent.com/35272820/97342849-0b0d3a80-1887-11eb-832c-cd8db1d20eb5.png)\r\n\r\nError says however:\r\n\r\n![image](https://user-images.githubusercontent.com/35272820/97342936-2d06bd00-1887-11eb-98ca-8e730b81ff9f.png)\r\n\r\nSo I dont know really... Does using parquet help?",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/717424552/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/717461440",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-717461440",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 717461440,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNzQ2MTQ0MA==",
        "user": {
            "login": "klnrdknt",
            "id": 35272820,
            "node_id": "MDQ6VXNlcjM1MjcyODIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/35272820?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/klnrdknt",
            "html_url": "https://github.com/klnrdknt",
            "followers_url": "https://api.github.com/users/klnrdknt/followers",
            "following_url": "https://api.github.com/users/klnrdknt/following{/other_user}",
            "gists_url": "https://api.github.com/users/klnrdknt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/klnrdknt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/klnrdknt/subscriptions",
            "organizations_url": "https://api.github.com/users/klnrdknt/orgs",
            "repos_url": "https://api.github.com/users/klnrdknt/repos",
            "events_url": "https://api.github.com/users/klnrdknt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/klnrdknt/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-27T18:47:36Z",
        "updated_at": "2020-10-27T18:48:08Z",
        "author_association": "NONE",
        "body": "CHAPTER 3\r\n\r\nTrying other stuff with parquet. I modified the generator script to produce a faster testable example.\r\n\r\nI now only exports two rather small sets of parquet file. Want a spoiler? This also leads to a crash.\r\n\r\n```python\r\nn = int(120e4)\r\nfor i in range(2):\r\n    ids = np.random.randint(0, 50, size=n)\r\n    data = np.random.randn(n)\r\n    index = pp(pd.to_datetime(\"2020-01-01\"), pd.to_datetime(\"2020-03-30\"), n)\r\n    df = pd.DataFrame()\r\n    df[\"Id\"] = ids\r\n    df[\"Values\"] = data\r\n    df[\"Time\"] = index\r\n    # df.to_csv(f\"test_{i}.csv\", index=False)\r\n    df.to_parquet(f\"test_{i}.parquet\", partition_columns=[\"\"], index=False, engine=\"pyarrow\")\r\n```\r\nIn jupyter, I then also split part of the operations to see what works and what does not.\r\n\r\nI changed the import to:\r\n\r\n```python\r\n# df = dd.read_csv(\"test_*.csv\", assume_missing=True)\r\ndf = dd.read_parquet(\"test_*.parquet\", engine=\"pyarrow\")\r\ndf[\"Time\"] = dd.to_datetime(\r\n    df[\"Time\"],\r\n    format=\"%Y-%m-%d %H:%M:%S\"\r\n)\r\ndf = df.dropna(subset=[\"Time\"])\r\ndf[\"Time\"] = df[\"Time\"].dt.round(\"S\")\r\n# added this line ########\r\ndf = df.set_index(\"Time\").reset_index()\r\n```\r\n\r\nThis part is suuuuuper fast. Then:\r\n\r\n```python\r\ndf = df.categorize(columns=[\"Id\"])\r\ndf = df.pivot_table(\r\n    index=\"Time\",\r\n    columns=\"Id\",\r\n    values=\"Values\"\r\n)\r\nprint(df)\r\n\r\n```\r\n\r\nThis also works, but here comes the crash down to earth:\r\n\r\n```python\r\ndf = df.compute()\r\n#### killed it\r\n\r\nKilledWorker: (\"('pivot_table_sum-chunk-f827f0c57d7b6267a0d334b04a9cc465', 0, 0, 0)\", <Worker 'tcp://127.0.0.1:38857', name: 3, memory: 0, processing: 2>)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/717461440/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/717467774",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-717467774",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 717467774,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNzQ2Nzc3NA==",
        "user": {
            "login": "klnrdknt",
            "id": 35272820,
            "node_id": "MDQ6VXNlcjM1MjcyODIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/35272820?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/klnrdknt",
            "html_url": "https://github.com/klnrdknt",
            "followers_url": "https://api.github.com/users/klnrdknt/followers",
            "following_url": "https://api.github.com/users/klnrdknt/following{/other_user}",
            "gists_url": "https://api.github.com/users/klnrdknt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/klnrdknt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/klnrdknt/subscriptions",
            "organizations_url": "https://api.github.com/users/klnrdknt/orgs",
            "repos_url": "https://api.github.com/users/klnrdknt/repos",
            "events_url": "https://api.github.com/users/klnrdknt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/klnrdknt/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-27T18:59:01Z",
        "updated_at": "2020-10-27T19:02:09Z",
        "author_association": "NONE",
        "body": "CHAPTER 4\r\n\r\nOne thing that seems to work well (same data as the post before) is to use `groupby` instead of `pivot_table`.\r\n\r\n```python\r\n# df = dd.read_csv(\"test_*.csv\", assume_missing=True)\r\ndf = dd.read_parquet(\"test_*.parquet\", engine=\"pyarrow\")\r\ndf[\"Time\"] = dd.to_datetime(\r\n    df[\"Time\"],\r\n    format=\"%Y-%m-%d %H:%M:%S\"\r\n)\r\ndf = df.dropna(subset=[\"Time\"])\r\ndf[\"Time\"] = df[\"Time\"].dt.round(\"S\")\r\n# added this line ########\r\ndf = df.set_index(\"Time\").reset_index()\r\ndf = df.groupby([\"Time\", \"Id\"]).mean()\r\ndf = df.compute()\r\n```\r\n\r\nFor the smaller data set this runs flawlessly, and I can then simply do `.unstack()`.\r\n\r\nTrying this out with bigger data...\r\n\r\nEDIT: Fails on bigger data too, around 10 files with 100e5 points each.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/717467774/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/717915799",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-717915799",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 717915799,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxNzkxNTc5OQ==",
        "user": {
            "login": "sm-Fifteen",
            "id": 516999,
            "node_id": "MDQ6VXNlcjUxNjk5OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/516999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sm-Fifteen",
            "html_url": "https://github.com/sm-Fifteen",
            "followers_url": "https://api.github.com/users/sm-Fifteen/followers",
            "following_url": "https://api.github.com/users/sm-Fifteen/following{/other_user}",
            "gists_url": "https://api.github.com/users/sm-Fifteen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sm-Fifteen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sm-Fifteen/subscriptions",
            "organizations_url": "https://api.github.com/users/sm-Fifteen/orgs",
            "repos_url": "https://api.github.com/users/sm-Fifteen/repos",
            "events_url": "https://api.github.com/users/sm-Fifteen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sm-Fifteen/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-28T12:59:33Z",
        "updated_at": "2020-10-28T12:59:33Z",
        "author_association": "NONE",
        "body": "Ah, I had started doing some profiling for this before running into issues with my [FunctionTrace](https://functiontrace.com) dumps being too large to be read correctly, but then I already had a working (if slow) Pandas script so I kind of forgot about this.\r\n\r\nI'll see if I can dig up the test bench I had and get useful diagnosing data out of it as well.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/717915799/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/718290486",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-718290486",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 718290486,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcxODI5MDQ4Ng==",
        "user": {
            "login": "sm-Fifteen",
            "id": 516999,
            "node_id": "MDQ6VXNlcjUxNjk5OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/516999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sm-Fifteen",
            "html_url": "https://github.com/sm-Fifteen",
            "followers_url": "https://api.github.com/users/sm-Fifteen/followers",
            "following_url": "https://api.github.com/users/sm-Fifteen/following{/other_user}",
            "gists_url": "https://api.github.com/users/sm-Fifteen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sm-Fifteen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sm-Fifteen/subscriptions",
            "organizations_url": "https://api.github.com/users/sm-Fifteen/orgs",
            "repos_url": "https://api.github.com/users/sm-Fifteen/repos",
            "events_url": "https://api.github.com/users/sm-Fifteen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sm-Fifteen/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-10-29T00:49:58Z",
        "updated_at": "2020-10-29T00:49:58Z",
        "author_association": "NONE",
        "body": "Ok, so killed the profiler after a couple minutes instead of letting it finish (and ended with 1GB of json profiling data, which still fits in what the Firefox profiler viewer can take) and judging by what I can see:\r\n![image](https://user-images.githubusercontent.com/516999/97510829-fbabf100-195b-11eb-893b-35d89661b390.png)\r\n\r\nThe problem with dates mainly appears to be that all threads are trying to run [a very hot loop in pandas' code](https://github.com/pandas-dev/pandas/blob/f2ca0a2665b2d169c97de87b8e778dbed86aea07/pandas/core/arrays/datetimes.py#L474-L476) that calls two python functions repeatedly: [tz](https://github.com/pandas-dev/pandas/blob/f2ca0a2665b2d169c97de87b8e778dbed86aea07/pandas/core/arrays/datetimes.py#L500-L511) and [freq](https://github.com/pandas-dev/pandas/blob/v1.1.1/pandas/core/arrays/datetimelike.py#L1082-L1087), both of which are trivial but the threading appreas to cause enough GIL contention to make these pause for hundreds of milliseconds at a time.\r\n\r\nI would share the profiling dump, but most dumps I can generate where the process takes long enough for the problem to be apparent (making the CSVs \"too small\" causes processing to complete too quickly) are far too large for me to send them. The code I was profiling is the same as the original post, just with the \"fast\" bits removed and an import for `functiontrace`:\r\n\r\nGenerating the files:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport dask.dataframe as dd\r\nbig_matrix = np.zeros((350,50000))\r\npd_matrix = pd.DataFrame(data=big_matrix)\r\npd_matrix_v = pd_matrix.unstack().reset_index()\r\npd_matrix_v['level_0'] = pd.to_datetime(pd_matrix_v['level_0'] + 1_000_000_000, unit='s', origin='unix')\r\ndask_matrix_v = dd.from_pandas(pd_matrix_v, npartitions=10)\r\ndask_matrix_v = dask_matrix_v.categorize(columns='level_1')\r\n\r\ndd.to_csv(dask_matrix_v, \"./test.csv\")\r\n```\r\n\r\nPivoting everything:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\nfrom dask.distributed import Client\r\n\r\nimport functiontrace\r\nfunctiontrace.trace()\r\n\r\nif __name__ == '__main__':\r\n\t#client = Client()\r\n\r\n\tdask_matrix_v = dd.read_csv(\"./test.csv/*.part\", usecols=['level_0', 'level_1', '0'], dtype={'level_1': 'int32', '0': 'float32'}, parse_dates=['level_0'])\r\n\tdask_matrix_v = dask_matrix_v.categorize(columns='level_1')\r\n\r\n\t# Very slow (10 minutes)\r\n\tdask_matrix_v = dd.reshape.pivot_table(dask_matrix_v, index='level_0', columns='level_1', values='0')\r\n\r\n\tmy_mat = dask_matrix_v.compute()\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/718290486/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/758236780",
        "html_url": "https://github.com/dask/dask/issues/6562#issuecomment-758236780",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/6562",
        "id": 758236780,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc1ODIzNjc4MA==",
        "user": {
            "login": "sm-Fifteen",
            "id": 516999,
            "node_id": "MDQ6VXNlcjUxNjk5OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/516999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sm-Fifteen",
            "html_url": "https://github.com/sm-Fifteen",
            "followers_url": "https://api.github.com/users/sm-Fifteen/followers",
            "following_url": "https://api.github.com/users/sm-Fifteen/following{/other_user}",
            "gists_url": "https://api.github.com/users/sm-Fifteen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sm-Fifteen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sm-Fifteen/subscriptions",
            "organizations_url": "https://api.github.com/users/sm-Fifteen/orgs",
            "repos_url": "https://api.github.com/users/sm-Fifteen/repos",
            "events_url": "https://api.github.com/users/sm-Fifteen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sm-Fifteen/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-01-11T21:29:13Z",
        "updated_at": "2021-01-11T21:29:13Z",
        "author_association": "NONE",
        "body": "Could this issue actually be related to actual File I/O being what's causing GIL contention and making threads slow down? It's [apparently a problem with basic file reading in Python](http://l0stinsp4ce.com/index.php/blogggo/disk-io--python-threads/) and, while I know that Dask actually [defers its file I/O to Pandas](https://github.com/dask/dask/blob/master/dask/dataframe/io/csv.py#L333-L334), I figured that, given the issue from the original post happens on disk but not in memory, and that concurrent disk I/O is apparently a nightmare, that the problem could reside there.\r\n\r\nWhether or not this is actually the case is another story, given I'm not 100% sure of how to go about effectively benchmarking this and my understanding of Pandas' code is all surface-level. From a quick glance, Pandas' C file reader (which their CSV reader is built on top of, the C engine seems to be the default one?) releases the GIL while [tokenizing](https://github.com/pandas-dev/pandas/blob/6918b545a58ac70bd99be2faa71e40ea5fb8e61b/pandas/_libs/parsers.pyx#L832-L833) and [reading](https://github.com/pandas-dev/pandas/blob/6918b545a58ac70bd99be2faa71e40ea5fb8e61b/pandas/_libs/src/parser/tokenizer.c#L1352-L1363) data from the disk, so I would understand this to mean that the disk IO wouldn't by itself block other tasks and that the problem might really just be GIL contention with no blocking.\r\n\r\nIntegrated GIL monitoring (#6391) would probably be useful for this.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/758236780/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]