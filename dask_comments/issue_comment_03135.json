[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/362806513",
        "html_url": "https://github.com/dask/dask/issues/3135#issuecomment-362806513",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/3135",
        "id": 362806513,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjgwNjUxMw==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-02-03T13:33:43Z",
        "updated_at": "2018-02-03T13:33:43Z",
        "author_association": "MEMBER",
        "body": "So first, lack of shuffling on a single partitition dataframe should be expected.  There is only one partition so everything can be done within that one pandas dataframe.\r\n\r\nI'm going to walk through this problem narrating my thoughts.  Hopefully this helps in future debugging.\r\n\r\nOne problem here is that the first call to read_parquet doesn't seem to collect it's min/max index values, which is unfortunate (cc @martindurant):\r\n\r\n```python\r\nIn [12]: ddf = dd.read_parquet('/tmp/test.parquet')\r\nIn [13]: ddf.divisions  # None means unknown here\r\nOut[13]: (None, None)\r\n```\r\n\r\nAnd so when we repartition we get another dataframe, also with unknown partitions\r\n\r\n```python\r\nIn [14]: ddf2 = ddf.repartition(npartitions=100)\r\n\r\nIn [15]: ddf2.divisions[:10]\r\nOut[15]: (None, None, None, None, None, None, None, None, None, None)\r\n```\r\n\r\nOf course, we would still expect the eventual set_index operation to check the values for sortedness, and a quick check shows that yes, the data is sorted.\r\n\r\n```python\r\nIn [16]: s = ddf2.uid.compute()\r\n\r\nIn [17]: s.is_monotonic_increasing\r\nOut[17]: True\r\n```\r\n\r\nWhen I call the set index operation it does appear to do the right thing, and the dataframe with the index set has only a linear amount of extra tasks (3x in this case) not quadratic or `n*log(n)`\r\n\r\n```python\r\nIn [18]: ddf3 = ddf2.set_index('uid')\r\n\r\nIn [19]: ddf3\r\nOut[19]: \r\nDask DataFrame Structure:\r\n                                            x        y\r\nnpartitions=100                                       \r\n0008cc68-408d-4359-876b-d24bc7bbf124  float64  float64\r\n02f51302-785d-460d-b6f9-5038c50e7e14      ...      ...\r\n...                                       ...      ...\r\nfd6a1eeb-5613-40d1-9e4b-0b9fc75e8eda      ...      ...\r\nfffa93f8-4802-4194-bc1d-76cf0372949d      ...      ...\r\nDask Name: sort_index, 302 tasks  # <<------- there are hundreds of tasks here, not thousands\r\n```\r\n\r\nSimilarly for the groupby-apply\r\n\r\n```python\r\nIn [23]: ddf3.groupby('uid').apply(sum)\r\n/home/mrocklin/Software/anaconda/bin/ipython:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\r\n  Before: .apply(func)\r\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\r\n  or:     .apply(func, meta=('x', 'f8'))            for series result\r\n  #!/home/mrocklin/Software/anaconda/bin/python\r\nOut[23]: \r\nDask DataFrame Structure:\r\n                                            x        y\r\nnpartitions=100                                       \r\n0008cc68-408d-4359-876b-d24bc7bbf124  float64  float64\r\n02f51302-785d-460d-b6f9-5038c50e7e14      ...      ...\r\n...                                       ...      ...\r\nfd6a1eeb-5613-40d1-9e4b-0b9fc75e8eda      ...      ...\r\nfffa93f8-4802-4194-bc1d-76cf0372949d      ...      ...\r\nDask Name: _groupby_slice_apply, 402 tasks\r\n```\r\n\r\nSo in short, I can't reproduce.  But hopefully by going through the process above you can identify where our paths diverge.\r\n\r\n```python\r\nIn [25]: c.get_versions()['client']\r\nOut[25]: \r\n{'host': [('python', '3.6.2.final.0'),\r\n  ('python-bits', 64),\r\n  ('OS', 'Linux'),\r\n  ('OS-release', '4.13.0-26-generic'),\r\n  ('machine', 'x86_64'),\r\n  ('processor', 'x86_64'),\r\n  ('byteorder', 'little'),\r\n  ('LC_ALL', 'None'),\r\n  ('LANG', 'en_US.UTF-8'),\r\n  ('LOCALE', 'en_US.UTF-8')],\r\n 'packages': {'optional': [('numpy', '1.14.0'),\r\n   ('pandas', '0.22.0'),\r\n   ('bokeh', '0.12.14rc1'),\r\n   ('lz4', '0.10.1'),\r\n   ('blosc', '1.5.1')],\r\n  'required': [('dask', '0.16.1+42.gc736c53'),\r\n   ('distributed', '1.20.2+60.gfd9a68c'),\r\n   ('msgpack', '0.4.8'),\r\n   ('cloudpickle', '0.5.2'),\r\n   ('tornado', '4.5.2'),\r\n   ('toolz', '0.9.0')]}}\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/362806513/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/362862669",
        "html_url": "https://github.com/dask/dask/issues/3135#issuecomment-362862669",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/3135",
        "id": 362862669,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mjg2MjY2OQ==",
        "user": {
            "login": "bnaul",
            "id": 903655,
            "node_id": "MDQ6VXNlcjkwMzY1NQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/903655?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bnaul",
            "html_url": "https://github.com/bnaul",
            "followers_url": "https://api.github.com/users/bnaul/followers",
            "following_url": "https://api.github.com/users/bnaul/following{/other_user}",
            "gists_url": "https://api.github.com/users/bnaul/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bnaul/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bnaul/subscriptions",
            "organizations_url": "https://api.github.com/users/bnaul/orgs",
            "repos_url": "https://api.github.com/users/bnaul/repos",
            "events_url": "https://api.github.com/users/bnaul/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bnaul/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-02-03T22:58:30Z",
        "updated_at": "2018-02-03T22:58:30Z",
        "author_association": "CONTRIBUTOR",
        "body": "Think I may have (partially) cracked it. I tried the steps you posted above and I see:\r\n```\r\nIn [10]: ddf = dd.read_parquet('/tmp/fastparquet.parquet')\r\nIn [11]: print(ddf.divisions)\r\n(0, 499999)\r\n\r\nIn[12]: ddf2 = ddf.repartition(npartitions=100)\r\nIn[13]: print(ddf2.divisions[:10])\r\n(0, 4999, 9999, 14999, 19999, 24999, 29999, 34999, 39999, 44999)\r\n\r\nIn[14]: ddf3 = ddf2.set_index('uid')\r\nIn[15]: ddf3.groupby('uid').apply(sum)\r\nOut[15]:\r\nDask DataFrame Structure:\r\n                                            x        y\r\nnpartitions=100\r\n0000ea10-7114-4f33-8559-676920266aca  float64  float64\r\n02b24265-fe22-4cfc-aa6f-88caa0cba5d1      ...      ...\r\n...                                       ...      ...\r\nfd971227-cdac-4b7d-ad74-5550a230166b      ...      ...\r\nfffa32cc-6cee-47e8-81a3-91cf873234f7      ...      ...\r\nDask Name: _groupby_slice_apply, 3401 tasks\r\n```\r\n\r\nQuite different from your result even though my package versions seem comparable (`np==1.14`, `dask==0.16.1`, `distributed==1.20.2`). But I realized I didn't specify the parquet engine in my example; this was using `fastparquet` so I tried again with `pyarrow`:\r\n\r\n```\r\nIn[23]: dd.read_parquet('/tmp/pyarrow.parquet').divisions\r\nOut[23]: (None, None)  # etc.\r\n```\r\n\r\nSo I guess it's related to the storage engine? Not sure if that's expected behavior or not (also not sure if using compression would affect anything but I specified `None` in my example).",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/362862669/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/362864746",
        "html_url": "https://github.com/dask/dask/issues/3135#issuecomment-362864746",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/3135",
        "id": 362864746,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mjg2NDc0Ng==",
        "user": {
            "login": "martindurant",
            "id": 6042212,
            "node_id": "MDQ6VXNlcjYwNDIyMTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6042212?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martindurant",
            "html_url": "https://github.com/martindurant",
            "followers_url": "https://api.github.com/users/martindurant/followers",
            "following_url": "https://api.github.com/users/martindurant/following{/other_user}",
            "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions",
            "organizations_url": "https://api.github.com/users/martindurant/orgs",
            "repos_url": "https://api.github.com/users/martindurant/repos",
            "events_url": "https://api.github.com/users/martindurant/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martindurant/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-02-03T23:20:43Z",
        "updated_at": "2018-02-03T23:20:43Z",
        "author_association": "MEMBER",
        "body": "Generally, fastparquet does compute the max/min of every column (although you can choose not to, at least in the direct fastparquet API), and if you use a column with ordered max/min as the index on load, then you should get known divisions. \r\n\r\nI don't know about arrow. \r\n\r\nI would have thought that having known divisions should, if anything, reduce the number of tasks rather than increase them. Certainly compression and other encoding options will have no effect on the graph planning, only on how long one given data-loading task might take.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/362864746/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/362893230",
        "html_url": "https://github.com/dask/dask/issues/3135#issuecomment-362893230",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/3135",
        "id": 362893230,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mjg5MzIzMA==",
        "user": {
            "login": "xhochy",
            "id": 70274,
            "node_id": "MDQ6VXNlcjcwMjc0",
            "avatar_url": "https://avatars.githubusercontent.com/u/70274?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xhochy",
            "html_url": "https://github.com/xhochy",
            "followers_url": "https://api.github.com/users/xhochy/followers",
            "following_url": "https://api.github.com/users/xhochy/following{/other_user}",
            "gists_url": "https://api.github.com/users/xhochy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xhochy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xhochy/subscriptions",
            "organizations_url": "https://api.github.com/users/xhochy/orgs",
            "repos_url": "https://api.github.com/users/xhochy/repos",
            "events_url": "https://api.github.com/users/xhochy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xhochy/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-02-04T09:26:00Z",
        "updated_at": "2018-02-04T09:26:00Z",
        "author_association": "CONTRIBUTOR",
        "body": "`parquet-cpp` also computes by default min/max on every column. Probably we need to add more Python interfaces to access these values but in general, the divisions \"discovery\" should be engine-independent code.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/362893230/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/362909242",
        "html_url": "https://github.com/dask/dask/issues/3135#issuecomment-362909242",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/3135",
        "id": 362909242,
        "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjkwOTI0Mg==",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2018-02-04T14:06:55Z",
        "updated_at": "2018-02-04T14:16:20Z",
        "author_association": "MEMBER",
        "body": "### syntax tip\r\n\r\nYou can use `ddf.persist()` rather than `c.persist(ddf)` if desired.\r\n\r\n### Actual problem at hand\r\n\r\nOK, I'm explicitly specifying `engine='fastparquet'` now in both the to_parquet and read_parquet operations.\r\n\r\n```python\r\nIn [12]: ddf.divisions\r\nOut[12]: (0, 499999)\r\nIn [13]: ddf = c.persist(ddf.set_index('uid'))\r\nIn [14]: ddf.divisions\r\nOut[14]: \r\n('00013b78-2281-4eba-8433-1319150a2248',\r\n 'fffc1e9a-4674-4a6f-9f18-8b62a51b87e9')\r\n```\r\n\r\nOK, so now we repartition to 100 partitions.  Typically here we would just interpolate between the existing division values.  Unfortunately, we don't have nice code to do interpolation on strings, and so we give up and resort to unknown divisions.\r\n\r\n```python\r\nIn [15]: ddf = c.persist(ddf.repartition(npartitions=100))  # no shuffle if we d\r\nIn [16]: ddf.divisions[:10]\r\nOut[16]: (None, None, None, None, None, None, None, None, None, None)\r\n```\r\n\r\nAny groupby-apply on this will result in a full shuffle.  \r\n\r\n```python\r\nIn [27]: ddf.groupby('uid').apply(sum)\r\nDask DataFrame Structure:\r\n                       x        y\r\nnpartitions=100                  \r\n                 float64  float64\r\n                     ...      ...\r\n...                  ...      ...\r\n                     ...      ...\r\n                     ...      ...\r\nDask Name: _groupby_slice_apply, 3400 tasks  # <<-- note thousands of tasks\r\n```\r\n\r\nIronically we could avoid this by resetting and then re-setting the index.  This would force Dask to look at the values in the column.\r\n\r\n```python\r\nIn [30]: ddf2 = ddf.reset_index()\r\n    ...: ddf2 = ddf2.set_index('uid')\r\n    ...: ddf2\r\nDask DataFrame Structure:\r\n                                            x        y\r\nnpartitions=100                                       \r\n00013b78-2281-4eba-8433-1319150a2248  float64  float64\r\n026170b9-b54a-4838-b295-4f27c12babf7      ...      ...\r\n...                                       ...      ...\r\nfd818866-8b2c-43af-ab20-b4df229ee9ac      ...      ...\r\nfffc1e9a-4674-4a6f-9f18-8b62a51b87e9      ...      ...\r\nDask Name: sort_index, 400 tasks\r\n```\r\n\r\n\r\n### Possible solutions for Dask\r\n\r\n1.  We could develop code to interpolate between string index values.  See [code here](https://github.com/dask/dask/blob/master/dask/dataframe/core.py#L3875-L3919)\r\n\r\n    <details>\r\n\r\n    ```python\r\n        if (df.known_divisions and (np.issubdtype(divisions.dtype, np.datetime64) or\r\n                                    np.issubdtype(divisions.dtype, np.number))):\r\n            if np.issubdtype(divisions.dtype, np.datetime64):\r\n                divisions = divisions.values.astype('float64')\r\n\r\n            if isinstance(divisions, pd.Series):\r\n                divisions = divisions.values\r\n\r\n            n = len(divisions)\r\n            divisions = np.interp(x=np.linspace(0, n, npartitions + 1),\r\n                                  xp=np.linspace(0, n, n),\r\n                                  fp=divisions)\r\n            if np.issubdtype(original_divisions.dtype, np.datetime64):\r\n                divisions = pd.Series(divisions).astype(original_divisions.dtype).tolist()\r\n            elif np.issubdtype(original_divisions.dtype, np.integer):\r\n                divisions = divisions.astype(original_divisions.dtype)\r\n\r\n            if isinstance(divisions, np.ndarray):\r\n                divisions = divisions.tolist()\r\n\r\n            divisions = list(divisions)\r\n            divisions[0] = df.divisions[0]\r\n            divisions[-1] = df.divisions[-1]\r\n\r\n            return df.repartition(divisions=divisions)\r\n        else:\r\n            ratio = npartitions / df.npartitions\r\n            split_name = 'split-%s' % tokenize(df, npartitions)\r\n            dsk = {}\r\n            last = 0\r\n            j = 0\r\n            for i in range(df.npartitions):\r\n                new = last + ratio\r\n                if i == df.npartitions - 1:\r\n                    k = npartitions - j\r\n                else:\r\n                    k = int(new - last)\r\n                dsk[(split_name, i)] = (split_evenly, (df._name, i), k)\r\n                for jj in range(k):\r\n                    dsk[(new_name, j)] = (getitem, (split_name, i), jj)\r\n                    j += 1\r\n                last = new\r\n\r\n            divisions = [None] * (npartitions + 1)\r\n            return new_dd_object(merge(df.dask, dsk), new_name, df._meta, divisions)\r\n    ```\r\n    <details>\r\n2.  We could have groupby-apply's compute sortedness on the column before returning.  This would make them semi-immediate, which is something that we avoid.  This would be helpful in this case but bad in all others\r\n3.  We could track index-sortedness separately from divisions.  This would mean adding an additional piece of metadata throughout all computations (I'm somewhat against this for maintenance reasons).\r\n4.  ...",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/362909242/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]