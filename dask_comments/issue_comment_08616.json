[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020400289",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020400289",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020400289,
        "node_id": "IC_kwDOAbcwm8480hKh",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-24T18:17:40Z",
        "updated_at": "2022-01-24T18:17:40Z",
        "author_association": "MEMBER",
        "body": "cc @quasiben @jakirkham @mrocklin @jcrist (critical thoughts very welcome)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020400289/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020487451",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020487451",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020487451,
        "node_id": "IC_kwDOAbcwm84802cb",
        "user": {
            "login": "gjoseph92",
            "id": 3309802,
            "node_id": "MDQ6VXNlcjMzMDk4MDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gjoseph92",
            "html_url": "https://github.com/gjoseph92",
            "followers_url": "https://api.github.com/users/gjoseph92/followers",
            "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
            "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
            "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
            "repos_url": "https://api.github.com/users/gjoseph92/repos",
            "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-24T19:56:46Z",
        "updated_at": "2022-01-24T19:56:46Z",
        "author_association": "MEMBER",
        "body": "@rjzamora thanks for writing this up; I've been wanting to discuss something similar.\r\n\r\nLayer-by-layer will be hard to schedule well. It might make root task overproduction even worse (https://github.com/dask/distributed/issues/5555 / https://github.com/dask/distributed/issues/5223). Since layers are essentially breadth-first, and execution order should be essentially depth-first, I don't think we'd actually be able to overlap the computation with the communication very well. Or rather, if we did so, we'd be doing lots of the _wrong_ computations while waiting for the next communication to happen. Or, if we did so smartly, we'd basically say \"okay, I got this root layer of 100,000 tasks, but I'm only scheduling 100 of them and then waiting to get the next layer\"\u2014in which case, you're not getting much benefit from overlapping computation and communication, since those 100 may well finish before the next layer is even received.\r\n\r\nOTOH, we could look at streaming the full graph to the scheduler in priority order\u2014basically, allow computation to start before all of the tasks have been received. This sounds nice, until you realize that it still requires materializing the full graph on the client (so you can prioritize it) before sending anything to the scheduler. So you're still not getting much benefit there with large graphs.\r\n\r\n-----\r\n\r\nWhat I've been thinking about instead is whether we can avoid materializing the graph at all\u2014even on the scheduler\u2014by leveraging the high-level structure of the graph for scheduling purposes.\r\n\r\n**Basically, could we define an interface on HLGs where you could iterate through tasks in the HLG in approximate priority order (\"good scheduling order\", aka depth-first order) without materializing the layers?**\r\n\r\nThis way, you could have an HLG of arbitrary size (billions of tasks), but you'd hopefully only have about `nthreads` tasks actually in memory on the scheduler at once.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020487451/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020500047",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020500047",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020500047,
        "node_id": "IC_kwDOAbcwm84805hP",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-24T20:10:14Z",
        "updated_at": "2022-01-24T20:10:14Z",
        "author_association": "MEMBER",
        "body": ">Basically, could we define an interface on HLGs where you could iterate through tasks in the HLG in approximate priority order (\"good scheduling order\", aka depth-first order) without materializing the layers?\r\n\r\nThis idea may be similar to what Mads had in mind, and what I poorly described at the end of the issue:  Perhaps we just want to be able to pickle some kind of generator function that the scheduler can simply unpickle and use to generate tasks in a reasonable order (not necessarily by layer, but maybe by required output key).  However, in order to get something like this working without a serialization nightmare, it seems like the user would need to opt in to \"unpickling on the scheduler\". The reason I'm suggesting partial graph execution in general, is so distributed exectution could still \"work\" for users who do not want \"unpickling on the scheduler\". In that case, the user would use the same generator function on the client side to materialize subgraphs and send along to the scheduler.\r\n\r\nI completely agree that a layer-by-layer approach may not lead to optimal scheduling. In the long run we definitely want to revise the HLG (or perhaps high-level expression) system to generate tasks is a more \"functional\" way. My suggestions to send the graph is Layer batches is mostly as a path to prototype the partial-exectution idea with minimal changes. However, if you think that the graph-generator logic should come first, I am also open to that idea :)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020500047/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020746039",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020746039",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020746039,
        "node_id": "IC_kwDOAbcwm84811k3",
        "user": {
            "login": "gjoseph92",
            "id": 3309802,
            "node_id": "MDQ6VXNlcjMzMDk4MDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gjoseph92",
            "html_url": "https://github.com/gjoseph92",
            "followers_url": "https://api.github.com/users/gjoseph92/followers",
            "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
            "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
            "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
            "repos_url": "https://api.github.com/users/gjoseph92/repos",
            "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T02:25:36Z",
        "updated_at": "2022-01-25T02:25:36Z",
        "author_association": "MEMBER",
        "body": "> Perhaps we just want to be able to pickle some kind of generator function that the scheduler can simply unpickle and use to generate tasks in a reasonable order (not necessarily by layer, but maybe by required output key)\r\n\r\nI think we're circling around the idea that you, me, @ian-r-rose, and @GenevieveBuckley talked about. With low-level graphs, you figure out the outputs given the inputs. For high-level graphs to be effective, you need to work backwards: you must figure out the inputs given the outputs. You have to invert all the graph-generating logic currently in dask/dask, often coming up with new algorithms. It's a lot of work\u2014it's not just wrapping existing code in a new interface. But if you don't do this, there's not much benefit to high-level graphs, since you're just deferring the materialization to some other point in time.\r\n\r\nIn order to make this iterate-in-priority-order concept work\u2014which is where I think we'd _really_ see HLGs pay off\u2014we'd have to go all-in on flipping these graph-generation algorithms on their heads.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020746039/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020747493",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020747493",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020747493,
        "node_id": "IC_kwDOAbcwm848117l",
        "user": {
            "login": "gjoseph92",
            "id": 3309802,
            "node_id": "MDQ6VXNlcjMzMDk4MDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gjoseph92",
            "html_url": "https://github.com/gjoseph92",
            "followers_url": "https://api.github.com/users/gjoseph92/followers",
            "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
            "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
            "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
            "repos_url": "https://api.github.com/users/gjoseph92/repos",
            "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T02:28:55Z",
        "updated_at": "2022-01-25T02:28:55Z",
        "author_association": "MEMBER",
        "body": "> My suggestions to send the graph is Layer batches is mostly as a path to prototype the partial-exectution idea with minimal changes\r\n\r\nI like the incremental approach, I just don't see how much value it would add. Worst-case, it would make an existing problem (root-task execution) even worse. But best-case, communication and computation just wouldn't overlap much, so you wouldn't gain much performance. I just don't think there's much of a way to work around needing depth-first scheduling, but having breadth-first communication.\r\n\r\nThat said, I'd love to be able to stream the transmission and especially unpacking of HLGs, even if we don't start executing them until the whole thing is materialized. Currently, as you mentioned, materializing the HLG (or deserializing and processing a low-level graph) on the scheduler is just as slow as on the client\u2014but it's even worse to do it on the scheduler, because then you block the event loop for tens of seconds at a time! If you have tasks already submitted to the scheduler, and then you submit another big graph to the scheduler, things can go very sideways as you essentially take the scheduler offline for half a minute to materialize your graph.\r\n\r\n----\r\n\r\nWhat about this incremental approach:\r\n1. Materialize the whole graph on the client. Stream it task-by-task, in priority order (depth-first), to the scheduler. Since it's in priority order, the scheduler can start scheduling tasks as soon as it gets them, without waiting to receive every task. Total compute time is about the same as today (materialize is the same, transfer is slower but now overlaps with compute), but a) you're no longer blocking the scheduler's event loop, improving stability and b) HLG serialization and unpicking on the scheduler becomes a non-issue, since it all happens on the client, so developing HLGs becomes vastly easier.\r\n2. Implement backpressure from the scheduler to the client to say \"hey, I have enough (processing) tasks already to saturate my workers (plus a bit of a buffer to hide my latency talking to you), don't send me any more\". This would effectively fix root-task overproduction without implementing https://github.com/dask/distributed/issues/3974, because the scheduler wouldn't even be aware of the extra root tasks, therefore it couldn't send them to workers. (You probably do still want STA, combined with this, in the long run though.) With task backpressure, you also wouldn't have to have every task in scheduler memory the whole time (task count would increase linearly until the very end). There are big questions here about whether this would make scheduling too \"dumb\"\u2014what happens when some tasks are completing faster than others, but the priority order is just sending along dependents of the slow tasks? Would there be some protocol for the scheduler to request dependents of certain keys, rather than blind priority order? (This would probably add way too much latency.)\r\n2. Stream results _back_ from the cluster to the client https://github.com/dask/distributed/issues/4754. Now, we can stream both into and out of the scheduler. As soon as an output key is complete and handed back to the client, the scheduler can forget about it. So the number of tasks in play on the scheduler is usually `\u0398(nthreads)`, regardless of the total graph size. This is kinda https://github.com/dask/distributed/issues/5630, except you're \"spilling\" the unneeded tasks to the client instead of disk!\r\n2. Make it possible to iterate through HLGs in priority order without materialization. Now, when you stream your graph from the client to the scheduler, a) whole graph is never materialized _anywhere_ (client or scheduler), b) work starts nearly instantly, since you don't have to wait for full materialization to start sending the scheduler tasks, c) graph size becomes effectively unlimited for embarrassingly-parallel workloads; the practical limits are the number of workers, and the size of the non-embarrassingly-parallel substructures within the graph.\r\n3. At this point, things seem pretty great to me. Compute starts instantly, you don't have limitations on graph size, and scheduling is better without root-task overproduction. But if you want even more, _maybe_ you move HLG unpacking back to the scheduler, and iterate through them there. Now, HLGs are essentially a compression format as well. And you have to deal with serialization again. But a) the client connection is less critical to scheduling, and b) the scheduler could pick which parts of the HLG to unpack next without client<->scheduler latency (if the interface even supported this).\r\n\r\nI'll note that with these ideas, the client becomes an increasingly-integral part of the system, since the cluster no longer holds the full graph, nor all of the results, in memory. That flies in the face of my concerns in https://github.com/dask/distributed/issues/5684. I don't like being increasingly dependent on a fast, stable, long-lived connection to the client, but:\r\n1. The benefits seem significant enough, it's probably worth it.\r\n1. You could use `afar`-like patterns to turn one of the workers into your client, or maybe even spin up a sidecar Python process on the scheduler, etc., to improve resilience.\r\n\r\ncc @fjetter @crusaderky ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020747493/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020757059",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020757059",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020757059,
        "node_id": "IC_kwDOAbcwm84814RD",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T02:50:51Z",
        "updated_at": "2022-01-25T02:50:51Z",
        "author_association": "MEMBER",
        "body": ">In order to make this iterate-in-priority-order concept work\u2014which is where I think we'd really see HLGs pay off\u2014we'd have to go all-in on flipping these graph-generation algorithms on their heads.\r\n\r\nYes - I get the feeling that the most-promising solution is to take the time to flip all common graph-construction algorithms on their heads, or develop a good expression system to somehow simplify/automate this (not that I have such a HLE solution in my head).\u2028\u2028\r\n\r\nRegarding your graph streaming suggestion/plan:  This approach is certainly in line with what I was thinking. Perhaps we could also start by looping over partition/chunk-specific subgraphs on the client (to avoid the need to materialize the entire graph at once). This is similar to something we do in NVTabular to ensure the entire subgraph for a specific output key is completely executed on the same worker.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020757059/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1020984494",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1020984494",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1020984494,
        "node_id": "IC_kwDOAbcwm8482vyu",
        "user": {
            "login": "fjetter",
            "id": 8629629,
            "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fjetter",
            "html_url": "https://github.com/fjetter",
            "followers_url": "https://api.github.com/users/fjetter/followers",
            "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
            "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
            "organizations_url": "https://api.github.com/users/fjetter/orgs",
            "repos_url": "https://api.github.com/users/fjetter/repos",
            "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fjetter/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T09:32:15Z",
        "updated_at": "2022-01-25T09:32:15Z",
        "author_association": "MEMBER",
        "body": "> I'll note that with these ideas, the client becomes an increasingly-integral part of the system, since the cluster no longer holds the full graph, nor all of the results, in memory.\r\n\r\nOur global state machine is complex and complicated enough as it is without adding a third distinct actor to the mix.\r\nIf we want to start streaming the tasks in priority order, that may be something to consider. We'd still need to fully materialize the graph but I would suggest to not couple too many problems at once and would defer this to a later discussion. Implementing some kind of backpressure/feedback mechanism that involves the Client directly or indirectly in scheduling decisions is something I am very strongly against.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1020984494/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1021327032",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1021327032",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1021327032,
        "node_id": "IC_kwDOAbcwm8484Da4",
        "user": {
            "login": "mrocklin",
            "id": 306380,
            "node_id": "MDQ6VXNlcjMwNjM4MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrocklin",
            "html_url": "https://github.com/mrocklin",
            "followers_url": "https://api.github.com/users/mrocklin/followers",
            "following_url": "https://api.github.com/users/mrocklin/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions",
            "organizations_url": "https://api.github.com/users/mrocklin/orgs",
            "repos_url": "https://api.github.com/users/mrocklin/repos",
            "events_url": "https://api.github.com/users/mrocklin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrocklin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T15:48:07Z",
        "updated_at": "2022-01-25T15:48:07Z",
        "author_association": "MEMBER",
        "body": "My gut reaction to this is skepticism.  Large graphs are less of a problem\nthan optimal scheduling.  This approach helps with large graphs at the cost\nof optimal scheduling.  I think that the difficulties around HLGs are due\nto poor design so far, and that can be fixed.  HLGs also provide a variety\nof other long-term benefits to the scheduler, which would have a lot more\nsemantic information about users' computations, which I think will be\nvaluable in the future.\n\nI think that trying to schedule things iteratively is a cool idea, but is\nlikely to be even more complex than high level graphs.\n\nOn Tue, Jan 25, 2022 at 3:32 AM Florian Jetter ***@***.***>\nwrote:\n\n> I'll note that with these ideas, the client becomes an\n> increasingly-integral part of the system, since the cluster no longer holds\n> the full graph, nor all of the results, in memory.\n>\n> Our global state machine is complex and complicated enough as it is\n> without adding a third distinct actor to the mix.\n> If we want to start streaming the tasks in priority order, that may be\n> something to consider. We'd still need to fully materialize the graph but I\n> would suggest to not couple too many problems at once and would defer this\n> to a later discussion. Implementing some kind of backpressure/feedback\n> mechanism that involves the Client directly or indirectly in scheduling\n> decisions is something I am very strongly against.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/8616#issuecomment-1020984494>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACKZTGVTWQRSQXJJ4Z5ZZTUXZU2ZANCNFSM5MWDCL7A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1021327032/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1021458506",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1021458506",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1021458506,
        "node_id": "IC_kwDOAbcwm8484jhK",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T17:57:19Z",
        "updated_at": "2022-01-25T17:58:42Z",
        "author_association": "MEMBER",
        "body": "Thanks for the feedback @mrocklin!  Note that I have been completely on the fence about this idea from the start (and I still very-much am) :) Overall, I agree with everything you said.  With that in mind\u2026\r\n\r\n>Large graphs are less of a problem than optimal scheduling.\r\n\r\nTotally agree. However, the only reason we are materializing HLG layers on the scheduler is because of graph communication time (which only seems to matter for large graphs and/or remote client-scheduler connections).  I\u2019m okay with saying graph size isn\u2019t a real concern, but then it seems strange that we are taking the time to struggle with multi-step serialization to ultimately minimize the graph-communication cost.\r\n\r\n>This approach helps with large graphs at the cost of optimal scheduling.\r\n\r\nYes - A naive layer-by-layer approach would likely inhibit optimal scheduling, but that suggestion was more of a \u201ctest\u201d (not how we would ever communicate/execute in practice).  However, I would\u2019t expect the long-term procedure to hurt scheduling at all.  The core idea is just that we allow the client to send an incomplete graph to the scheduler.  Ideally, that subgraph would correspond to tasks that should be scheduled before the following subgraph anyway.\r\n\r\n>I think that the difficulties around HLGs are due to poor design so far, and that can be fixed.\r\n\r\nAgree that HLGs should be revised, but are you focusing on pack/unpack (serialization) design, or something else?\r\n\r\n>HLGs also provide a variety of other long-term benefits to the scheduler, which would have a lot more\r\nsemantic information about users' computations, which I think will be valuable in the future.\u2028\r\n\r\nI agree that we can get a lot out of HLG layers, but I am also a bit skeptical that we really need to avoid materializing the low-level graph representation (within the Layer) before communicating it to the scheduler.  For example, the annotations that we currently ship with the layer have nothing to do with whether that layer is materialized or not.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1021458506/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1060543864",
        "html_url": "https://github.com/dask/dask/issues/8616#issuecomment-1060543864",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8616",
        "id": 1060543864,
        "node_id": "IC_kwDOAbcwm84_Np14",
        "user": {
            "login": "crusaderky",
            "id": 6213168,
            "node_id": "MDQ6VXNlcjYyMTMxNjg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/crusaderky",
            "html_url": "https://github.com/crusaderky",
            "followers_url": "https://api.github.com/users/crusaderky/followers",
            "following_url": "https://api.github.com/users/crusaderky/following{/other_user}",
            "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions",
            "organizations_url": "https://api.github.com/users/crusaderky/orgs",
            "repos_url": "https://api.github.com/users/crusaderky/repos",
            "events_url": "https://api.github.com/users/crusaderky/events{/privacy}",
            "received_events_url": "https://api.github.com/users/crusaderky/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-07T11:12:59Z",
        "updated_at": "2022-03-07T11:14:15Z",
        "author_association": "MEMBER",
        "body": "I really like @gjoseph92's idea.\r\nI think there are a few issues with it though:\r\n\r\n1. as pointed out already, Client<->Scheduler connectivity is frequently less reliable/predictable than Scheduler<->Worker\r\n2. Client<->Scheduler bandwidth and latency can be much, much slower than the one inside the cluster - namely when the Client is on somebody's laptop and connects to a scheduler on AWS. Worst case scenario for these first two points: the laptop is connecting over 5G or a Starbucks WiFi.\r\n3. This means that a client cannot be closed anymore after ``publish_dataset``, or that alternatively ``publish_dataset`` must eagerly send to the scheduler the remainder of the graph\r\n4. The Client may hold its own event loop precious too, and this design clogs it heavily. This is not the typical case of Jupyter Notebooks, but it is that of e.g. a web or RPC server that uses dask to offload computation.\r\n\r\nI would like to propose an improvement on top of @gjoseph92's design: a new Server between the Client and the Scheduler, which I'll call PreScheduler.\r\n\r\n1. Each Client connects to a PreScheduler and uploads whole, unmaterialized, unoptimized, HLGs. \r\n2. The PreScheduler materializes and optimizes the graph\r\n3. After this point, everything is as @gjoseph92 described above: the PreScheduler internally defines order and sends keys to the scheduler on a as-needed basis\r\n\r\n- This whole new service is *optional* - it is straightforward to flick a switch to run everything it does inside either the Scheduler or the Client process/event loop, minus either one of the network connections\r\n- The PreScheduler runs near the Scheduler (e.g. on AWS).\r\n- You can have as many PreSchedulers as there are Clients.\r\n- You can write adaptive logic that starts and stops on demand new PreSchedulers on Kubernetes/other depending on the amount of Clients connected\r\n- You can also have a simpler system that spawns a new local process on the Scheduler host every time a new client connects.\r\n\r\nThe connection process goes as follows:\r\n1.  1a. A pool of statically started ``dask-prescheduler`` processes registers to the Scheduler, exactly like ``dask-worker`` does, OR\r\n    1b. Scheduler starts a fixed-size pool of subprocesses; once there are more Clients than subprocesses, more than one Client will start connecting to the same PreScheduler, OR\r\n    1c. Scheduler has the capability to spawn and destroy cloud containers running ``dask-prescheduler`` on demand\r\n2. Client connects to Scheduler\r\n3. The Scheduler *might* start a new process or cloud container; see point 1. This is invisible to the Client.\r\n4. Scheduler informs Client of the IP:port of the PreScheduler\r\n5. Client connects to PreScheduler\r\n6. After the Client disconnects, wait a certain timeout (in case it reconnects) and then shut down the PreScheduler",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1060543864/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]