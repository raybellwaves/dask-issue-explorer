{
    "url": "https://api.github.com/repos/dask/dask/issues/9197",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9197/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9197/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9197/events",
    "html_url": "https://github.com/dask/dask/pull/9197",
    "id": 1275575058,
    "node_id": "PR_kwDOAbcwm8454eWH",
    "number": 9197,
    "title": "[WIP] Revise aggregate_files behavior in read_parquet",
    "user": {
        "login": "rjzamora",
        "id": 20461013,
        "node_id": "MDQ6VXNlcjIwNDYxMDEz",
        "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rjzamora",
        "html_url": "https://github.com/rjzamora",
        "followers_url": "https://api.github.com/users/rjzamora/followers",
        "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
        "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
        "organizations_url": "https://api.github.com/users/rjzamora/orgs",
        "repos_url": "https://api.github.com/users/rjzamora/repos",
        "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rjzamora/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2022-06-17T23:17:16Z",
    "updated_at": "2022-11-04T21:35:32Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "draft": true,
    "pull_request": {
        "url": "https://api.github.com/repos/dask/dask/pulls/9197",
        "html_url": "https://github.com/dask/dask/pull/9197",
        "diff_url": "https://github.com/dask/dask/pull/9197.diff",
        "patch_url": "https://github.com/dask/dask/pull/9197.patch",
        "merged_at": null
    },
    "body": "Closes #9043\r\nCloses #9051 \r\nCloses #8829\r\n\r\nThis PR (mostly) preserves the existing `chunksize`/`aggregate_files` options in `read_parquet` by adding two new arguments:\r\n\r\n- `sort_input_paths` (default `True`): Whether or not Dask should re-order the files in the dataset to use \"natural\" ordering. Note that this new feature *could* be added in a separate PR, but I wanted to make sure that the new design allows for such an argument to exist.\r\n- `file_groups` (default `None`): A dictionary mapping paths to \"file-group\" indices, or a list of directory-partitioned-column names that must match for two or more files to belong to the same \"file group.\"  This PR introduces the **file group** concept to `dd.read_parquet`. The meaning is simple: Two files must belong to the same file group for Dask to consider aggregating them into the same output DataFrame partition.  Matching file-group membership is necessary, but **not** sufficient, for file aggregation. That is, there must be some other option (like `aggregate_files=int|True` or `chunksize`) to specify **how** files should be aggregated within each group. The engines are always allowed to reorder paths by file group to improve file-aggregation behavior (even if `sort_input_paths=False`).  Note that I orginally added this option in order to drop support for `str` arguments to `aggregate_files` in favor of `int` support (explained below). However, it is worth noting that this option is also much more flexible/powerful than the original `aggregate_files=<str>` behavior.\r\n\r\nIn addition to these new arguments, I also modified the existing `aggregate_files` argument to only accept `bool` or `int` types. That is, the `aggregate_files` option is now the \"file equivalent\" of `split_row_groups`. Specifying `aggregate_files=100` means that 100 files from the same file group may be aggregated into the same output partition.\r\n\r\nThe most important result of this PR is likely the support for `aggregate_files=<int>` (in combination with `file_groups=`). For example. In `main`, one would need to use `chunksize` (or `split_row_groups`) with `aggregate_files=\"year\"` to read in a single large partition for each distinct year in a partitioned NYC-taxi dataset:\r\n\r\n```python\r\nimport dask.dataframe as dd\r\n\r\nddf = dd.read_parquet(\r\n    \"s3://ursa-labs-taxi-data/\",\r\n    engine=\"pyarrow\",\r\n    storage_options={\"anon\": True},\r\n    dataset={\r\n        \"partitioning\": [\"year\", \"month\"],\r\n        \"partition_base_dir\": \"ursa-labs-taxi-data\",\r\n    },\r\n    chunksize=\"6GB\",\r\n    aggregate_files=\"year\",\r\n)\r\n# (takes several seconds on my workstation - but scales poorly with the total number of row groups)\r\n```\r\n\r\nHowever, with this branch, file aggregation *can* be much faster/simpler:\r\n\r\n```python\r\nddf = dd.read_parquet(\r\n    \"s3://ursa-labs-taxi-data/\",\r\n    engine=\"pyarrow\",\r\n    storage_options={\"anon\": True},\r\n    dataset={\r\n        \"partitioning\": [\"year\", \"month\"],\r\n        \"partition_base_dir\": \"ursa-labs-taxi-data\",\r\n    },\r\n    file_groups=[\"year\"],\r\n    aggregate_files=True,\r\n)\r\n# (takes less than 1s on my workstation)\r\n```\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9197/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9197/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}