{
    "url": "https://api.github.com/repos/dask/dask/issues/7930",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/7930/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/7930/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/7930/events",
    "html_url": "https://github.com/dask/dask/issues/7930",
    "id": 951917904,
    "node_id": "MDU6SXNzdWU5NTE5MTc5MDQ=",
    "number": 7930,
    "title": "Issue opening h5py arrays",
    "user": {
        "login": "N4321D",
        "id": 35295509,
        "node_id": "MDQ6VXNlcjM1Mjk1NTA5",
        "avatar_url": "https://avatars.githubusercontent.com/u/35295509?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/N4321D",
        "html_url": "https://github.com/N4321D",
        "followers_url": "https://api.github.com/users/N4321D/followers",
        "following_url": "https://api.github.com/users/N4321D/following{/other_user}",
        "gists_url": "https://api.github.com/users/N4321D/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/N4321D/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/N4321D/subscriptions",
        "organizations_url": "https://api.github.com/users/N4321D/orgs",
        "repos_url": "https://api.github.com/users/N4321D/repos",
        "events_url": "https://api.github.com/users/N4321D/events{/privacy}",
        "received_events_url": "https://api.github.com/users/N4321D/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862305,
            "node_id": "MDU6TGFiZWwyNDI4NjIzMDU=",
            "url": "https://api.github.com/repos/dask/dask/labels/array",
            "name": "array",
            "color": "006b75",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 6,
    "created_at": "2021-07-23T21:52:44Z",
    "updated_at": "2021-10-12T07:19:13Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I used dask (and xarray) to combine a set of H5py files into a dataframe. \r\nThis worked great until I updated dask from 2.28 to 2021.07.1.\r\n\r\nIf I run the same script now, I always run out of memory, just after loading the files and doing any operation on the \r\ndataframe. \r\n\r\nI first thought it was an xarray issue (reported here: https://github.com/pydata/xarray/issues/5585). \r\nSo I switched to a pure dask solution (based on: https://docs.dask.org/en/latest/array-creation.html)\r\n\r\nHowever, I keep running into the same problem. \r\n\r\nI tried 3 different methods:\r\n1. converting a h5py to a numpy dict and load as pandas df with dask.delayed. Then construct a dask dataframe from the delayed\r\n2. loading the individual files as dask arrays and convert them to dask dataframe per file, then combine the dataframes\r\n3. load the files as dask arrays, combine them into a big dask array, then convert that to dataframe.\r\nAll with the same results: Kernel died, due to memory\r\n\r\nI tried different chunk sizes. Smaller chunks just makes everything slower (incl overloading the RAM). Bigger chunks just make it faster but stilll overload the memory. In any case the kernel still crashes early in the process, due to memory overload. \r\nAlso when trying to save the dataframe, nothing/just a few kbs are written. \r\nTherefore I suspect that there is a problem with the creation of the dataframe, not saving it.\r\n\r\nFirst I suspected the transformation form array to dataframe to be the issue. \r\nBut running calculation on the array itself or saving it also cause the same issue. \r\nThat's why I think it has to do with the loading of an array from h5py.\r\n\r\nIf am doing something principally wrong here, please let me know. But this code worked great for me on earlier versions of dask.\r\nThank you!\r\n\r\nHere is code to reproduce. I have 16GB  RAM, so to reproduce you might need to adjust the number of files loaded if you have more RAM: \r\n\r\n## Create Datafiles (note this takes a couple of GB disk space) ##\r\n```\r\nimport numpy as np\r\nimport h5py\r\n\r\nfrom shutil import copyfile\r\n\r\n\r\ndef makefile(data, n, nfiles):\r\n    for i in range(nfiles):\r\n        print(f\"\\rCreating File: {i} / {nfiles - 1}       \", end=\"\")\r\n        if i == 0:\r\n            with h5py.File(f\"{i}.h5\", 'w') as file:\r\n                for par in range(n):\r\n                    file.create_dataset(f'data/{par}',\r\n                                        data=data,\r\n                                        dtype='f4',\r\n                                        maxshape=(None,),\r\n                                        chunks= (32000,), # (dlength,),\r\n                                        compression='gzip',\r\n                                        compression_opts=5,\r\n                                        fletcher32=True,\r\n                                        shuffle=True,\r\n                                    )\r\n        else:\r\n            copyfile('0.h5', f\"{i}.h5\")\r\n\r\n\r\ndata = np.random.randint(0, 0xFFFF, int(1e7))\r\nmakefile(data, 10, 100)    # ~100 files is enough to create an error on my 16GB RAM 24GB swap, increase if you have more RAM?\r\ndel data\r\n\r\n```\r\n\r\n##  Setup ##\r\n```\r\nimport os\r\nimport h5py\r\nimport dask\r\nimport dask.dataframe as dd\r\nimport dask.array as da\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\nfrom dask.diagnostics import ProgressBar\r\nProgressBar().register()\r\n\r\nfile_list = sorted([f for f in os.listdir(\".\") if f.endswith('.h5')])\r\n```\r\n## Method 1 (Pandas) ##\r\n\r\n```\r\n@dask.delayed\r\ndef hdfs_2_pd(f):\r\n    file = h5py.File(f, 'r')[\"data/\"]\r\n    return pd.DataFrame.from_dict({k:v[:] for k, v in file.items()})\r\n    \r\nddf = dd.from_delayed([hdfs_2_pd(f) for f in file_list])\r\n\r\n```\r\n\r\n## Method 2 (ddf per file) ##\r\n```\r\nout = []\r\nfor fn in file_list:\r\n    f = h5py.File(fn, \"r\") # HDF5 file\r\n    d = f['/data/']\r\n    x = da.stack([da.from_array(i, chunks=(100000,)) for i in d.values()], axis=1)\r\n    keys = d.keys()\r\n    out.append(dd.from_dask_array(x, columns=keys))\r\nddf = dd.concat(out)\r\n```\r\n\r\n## Method 3: combine arrays ##\r\n```\r\nout = []\r\n\r\nfor fn in file_list:\r\n    f = h5py.File(fn, \"r\") # HDF5 file\r\n    d = f['/data/']\r\n    x = da.stack([da.from_array(i, chunks=(100000,)) for i in d.values()], axis=1)\r\n    keys = d.keys()\r\n    out.append(x)\r\n\r\ncombined = da.concatenate(out)\r\nddf = dd.from_dask_array(combined, columns=keys)\r\n```\r\n## Testing ddf ##\r\nAll these methods produce a dask dataframe. However in the next steps Memory will fully load \r\nand the kernel will crash:\r\n\r\n```\r\nddf['0'].mean().compute()\r\n```\r\noutput:\r\n`[                                        ] | 0% Completed |  6min 57.9s`\r\n\r\n```\r\nddf.to_hdf(\"combined.h5\", \r\n           key='/data*',\r\n           complevel=5, \r\n           complib=\"blosc:lz4hc\", \r\n          )\r\n```\r\noutput: \r\n`[#                                       ] | 3% Completed | 13min  8.7s`\r\n\r\n```\r\nddf.to_parquet(\"conbined.pq\", \r\n               compression=\"gzip\", \r\n               append=False,\r\n               overwrite=True,\r\n)\r\n```\r\noutput:\r\n`[##                                      ] | 5% Completed |  4min 11.9s`\r\n\r\n\r\n```\r\nda.to_hdf5('./combined_arr.h5', \r\n           {f\"/test\": combined}, \r\n           compression='gzip', \r\n           shuffle=True, \r\n           compression_opts=5)\r\n```\r\noutput:\r\n`[#                                       ] | 3% Completed |  5min 57.9s`\r\n\r\n\r\n\r\n\r\n**Environment**:\r\n\r\n- Dask version: '2021.07.0'\r\n- h5py version: '2.10.0'\r\n- Python version: Python 3.8.2 (Ananconda)\r\n- Operating System: Ubuntu 21.04\r\n- Install method (conda, pip, source): conda\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/7930/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/7930/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}