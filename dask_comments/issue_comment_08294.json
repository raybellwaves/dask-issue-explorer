[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/953489043",
        "html_url": "https://github.com/dask/dask/issues/8294#issuecomment-953489043",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8294",
        "id": 953489043,
        "node_id": "IC_kwDOAbcwm8441RaT",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-10-28T04:18:43Z",
        "updated_at": "2021-10-28T04:18:43Z",
        "author_association": "MEMBER",
        "body": "Hi @DahnJ\r\n\r\nYou might have already seen this [related discussion](https://github.com/dask/distributed/pull/5435) about how the shuflle service could potentially be redesigned to be more robust. [Link to document](https://github.com/dask/distributed/pull/5435/files).\r\n\r\nIt's great to see people trying this stuff out",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/953489043/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/953498864",
        "html_url": "https://github.com/dask/dask/issues/8294#issuecomment-953498864",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8294",
        "id": 953498864,
        "node_id": "IC_kwDOAbcwm8441Tzw",
        "user": {
            "login": "GenevieveBuckley",
            "id": 30920819,
            "node_id": "MDQ6VXNlcjMwOTIwODE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30920819?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GenevieveBuckley",
            "html_url": "https://github.com/GenevieveBuckley",
            "followers_url": "https://api.github.com/users/GenevieveBuckley/followers",
            "following_url": "https://api.github.com/users/GenevieveBuckley/following{/other_user}",
            "gists_url": "https://api.github.com/users/GenevieveBuckley/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GenevieveBuckley/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GenevieveBuckley/subscriptions",
            "organizations_url": "https://api.github.com/users/GenevieveBuckley/orgs",
            "repos_url": "https://api.github.com/users/GenevieveBuckley/repos",
            "events_url": "https://api.github.com/users/GenevieveBuckley/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GenevieveBuckley/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-10-28T04:46:54Z",
        "updated_at": "2021-10-28T04:46:54Z",
        "author_association": "MEMBER",
        "body": "Also, this blogpost might also be interesting if you haven't had a chance to read it: https://coiled.io/blog/better-shuffling-in-dask-a-proof-of-concept/",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/953498864/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/956579944",
        "html_url": "https://github.com/dask/dask/issues/8294#issuecomment-956579944",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8294",
        "id": 956579944,
        "node_id": "IC_kwDOAbcwm845BEBo",
        "user": {
            "login": "gjoseph92",
            "id": 3309802,
            "node_id": "MDQ6VXNlcjMzMDk4MDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gjoseph92",
            "html_url": "https://github.com/gjoseph92",
            "followers_url": "https://api.github.com/users/gjoseph92/followers",
            "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
            "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
            "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
            "repos_url": "https://api.github.com/users/gjoseph92/repos",
            "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-11-01T20:51:58Z",
        "updated_at": "2021-11-01T20:51:58Z",
        "author_association": "MEMBER",
        "body": "Hey @DahnJ, thanks for trying this out. Sorry it's taken a while to get back to you.\r\n\r\nFirst:\r\n* How are you deploying the cluster? How many workers? CPUs, threads, memory, and disk per worker?\r\n* How many partitions in your parquet dataset, and how large is each partition?\r\n* Did you restart the cluster after each failure (`client.restart()`?) Did you try recreating it from scratch?\r\n  * If you're restarting instead of creating from scratch, are your workers using Nannies? You can check this with `{addr: info['nanny'] is not None for addr, info in client.scheduler_info()['workers'].items()}`\r\n\r\nObvious question, but a good first check: do you actually have enough disk space to hold both the input dataset and the shuffled output dataset? As in, >6.8TB? Because AFAICT you're not overwriting your input dataset.\r\n\r\n> I got to the last step (`to_parquet`), but then ran out of disk space with over 4TB being used by the shuffle\r\n\r\n`to_parquet` isn't really the \"last step\"\u2014that is, it's not like the entire shuffle happens, and then the `to_parquet` happens. Instead, they're interleaved.\r\n\r\nBasically, the way the shuffle works is that the entire input DataFrame gets temporarily written to disk in little fragments. At the midpoint in the shuffle, all of the data is on disk. Past the midpoint, we're reading data from disk back into memory for each output partition\u2014on the dashboard, you'll see this as tasks called `unpack`. After an `unpack` task runs, the `to_parquet` task for that partition will run immediately, writing the shuffled partition back to disk. (Otherwise, if you wanted for _all_ the `unpack`s to run before doing any `to_parquet`, you'd have to have all 3.4TB of data in memory at once!)\r\n\r\nSo at the point at which the very first `to_parquet` runs, I'd expect ~3.4TB of disk to be used by all these temporary fragments. Probably more (maybe less), because we're using a different file format.\r\n\r\nNow, each `unpack` does delete its temporary fragment from disk, so in theory you should be able to \"trade\" the disk space taken by the temporary fragment for the output parquet partition.\r\n\r\nBut in practice, the temporary fragments are using a different file format (that's uncompressed and possibly not that efficient), so they're probably not exactly 3.4TB. If they're a lot larger than parquet, then maybe we're using so much space with the temporary fragments that there isn't enough room for `cache-imported.gz.parq`. This still doesn't quite make sense for me, since in this case I'd expect peak disk usage to be at the midpoint, and with every `unpack->to_parquet` it would go down. So if you were running out of disk space, I'd expect to see an error in `transfer` (before the midpoint), not in `to_parquet`.\r\n\r\n\r\n> I did not get over the `set_index` step. Workers seem to die and the computation hangs, always towards the end.\r\n\r\nCan you try completely recreating the cluster before doing these subsets? #8223 is very bad at cleaning up after itself after an error. There may be some internal state that's broken after your first try failed.\r\n\r\nFrom one of your error messages, it's looking like the `re-quantiles` step in `set_index` is failing. This isn't actually part of the shuffle, per se; it's when dask computes your entire input data once to pick good divisions for you automatically. If you pass `divisions=` to `set_index`, you can skip this.\r\n\r\nIt seems like your workers are running out of memory? That may be why they're crashing. Knowing the resources you have and the size of the input partitions would help understand this.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/956579944/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/957980566",
        "html_url": "https://github.com/dask/dask/issues/8294#issuecomment-957980566",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8294",
        "id": 957980566,
        "node_id": "IC_kwDOAbcwm845GZ-W",
        "user": {
            "login": "DahnJ",
            "id": 18722560,
            "node_id": "MDQ6VXNlcjE4NzIyNTYw",
            "avatar_url": "https://avatars.githubusercontent.com/u/18722560?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/DahnJ",
            "html_url": "https://github.com/DahnJ",
            "followers_url": "https://api.github.com/users/DahnJ/followers",
            "following_url": "https://api.github.com/users/DahnJ/following{/other_user}",
            "gists_url": "https://api.github.com/users/DahnJ/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/DahnJ/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/DahnJ/subscriptions",
            "organizations_url": "https://api.github.com/users/DahnJ/orgs",
            "repos_url": "https://api.github.com/users/DahnJ/repos",
            "events_url": "https://api.github.com/users/DahnJ/events{/privacy}",
            "received_events_url": "https://api.github.com/users/DahnJ/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-11-02T17:40:53Z",
        "updated_at": "2021-11-02T17:40:53Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thank you for the reply @gjoseph92 !\r\n\r\n\r\n> * How are you deploying the cluster? How many workers? CPUs, threads, memory, and disk per worker?\r\n\r\nI am starting a `LocalCluster` on a single machine with these parameters:\r\n\r\n```python\r\nLocalCluster(n_workers=10, memory_limit='25GB', threads_per_worker=2)\r\n```\r\n\r\nSince it's a local cluster, I assume the disk per worker is only limited by the disk space left on the device.\r\n\r\n> * How many partitions in your parquet dataset, and how large is each partition?\r\n\r\n`5800` partitions, each about `620M`, when gzipped (as in `.gz.parquet`)\r\n\r\n> * Did you restart the cluster after each failure (`client.restart()`?) Did you try recreating it from scratch?\r\n\r\nYes. I run the cluster from a Jupyter notebook and I restart the kernel each time, which I believe should clean everything completely. Some of the worker files are only deleted when the new cluster starts.\r\n\r\n\r\n> Obvious question, but a good first check: do you actually have enough disk space to hold both the input dataset and the shuffled output dataset? As in, >6.8TB? Because AFAICT you're not overwriting your input dataset.\r\n\r\nYes, at the time of writing I had 7.4TB. I might be able to try with even more.\r\n\r\n> > I got to the last step (`to_parquet`), but then ran out of disk space with over 4TB being used by the shuffle\r\n> \r\n> `to_parquet` isn't really the \"last step\"\u2014that is, it's not like the entire shuffle happens, and then the `to_parquet` happens. Instead, they're interleaved.\r\n\r\nWhat I meant by that is that the `set_index` partition calculation already ran through.\r\n\r\n\r\n> But in practice, the temporary fragments are using a different file format (that's uncompressed and possibly not that efficient), so they're probably not exactly 3.4TB. \r\n\r\nAha, if it's uncompressed, then that would mean I probably need quite a bit more storage! When I uncompress, the `620M` partition becomes `1.3G`. So I would probably need at least 7-8TB + whatever inefficiency in the temporary storage requires, is that correct?\r\n\r\n> From one of your error messages, it's looking like the `re-quantiles` step in `set_index` is failing. This isn't actually part of the shuffle, per se; it's when dask computes your entire input data once to pick good divisions for you automatically. If you pass `divisions=` to `set_index`, you can skip this.\r\n\r\nGood point, I will try to provide some divisions and see what happens. Not a straightforward thing, but suppose I could e.g. sample the string ids and split them by first `n` characters.\r\n\r\n> It seems like your workers are running out of memory? That may be why they're crashing. Knowing the resources you have and the size of the input  #partitions would help understand this.\r\n\r\nPerhaps I could lower the number of workers/threads and increase the memory limit per worker.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/957980566/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/961301624",
        "html_url": "https://github.com/dask/dask/issues/8294#issuecomment-961301624",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8294",
        "id": 961301624,
        "node_id": "IC_kwDOAbcwm845TEx4",
        "user": {
            "login": "gjoseph92",
            "id": 3309802,
            "node_id": "MDQ6VXNlcjMzMDk4MDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gjoseph92",
            "html_url": "https://github.com/gjoseph92",
            "followers_url": "https://api.github.com/users/gjoseph92/followers",
            "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
            "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
            "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
            "repos_url": "https://api.github.com/users/gjoseph92/repos",
            "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-11-04T18:19:15Z",
        "updated_at": "2021-11-04T18:19:15Z",
        "author_association": "MEMBER",
        "body": "> I am starting a `LocalCluster` on a single machine\r\n\r\nOverall note: this new shuffle is designed specifically for multi-machine clusters. On a single machine, you'd probably be better off using `shuffle=\"disk\"`. It's sort of similar to this new shuffle, except it may have more efficient serialization, plus it skips the unnecessary networking step of sending the data back to itself.\r\n\r\n> When I uncompress, the `620M` partition becomes `1.3G`. So I would probably need at least 7-8TB + whatever inefficiency in the temporary storage requires, is that correct?\r\n\r\nYes, I think this is the problem!\r\n1. You just don't have enough disk space. You'd need space for the entire compressed input dataset, plus the entire _uncompressed_ input dataset as temporary files. Plus maybe a little extra.\r\n2. 1.3G partitions are really large. In general, dask won't perform very well when partition sizes are that big.\r\n\r\n    How much memory does your machine have?\r\n\r\n    Most operations will involve duplicating the input partition, sometimes multiple times. Also, dask can be slow to clean up data (https://github.com/dask/distributed/issues/5114), and data transfer involves even more copies (https://github.com/dask/distributed/pull/5208). So when your partitions are 100MB, a couple extra copies per thread is still bad, but it's a lot less total memory than a couple extra copies of a 1.3GB thing.\r\n\r\n    I also haven't tested the shuffle service on partitions that large. There are a bunch of hardcoded parameters in it that are all tuned to partitions of ~200MB. And the shuffling process itself involves making intermediate copies, which we try to drop a quickly as possible, but sometimes have to hang onto for a little while.\r\n\r\n> Good point, I will try to provide some divisions and see what happens. Not a straightforward thing, but suppose I could e.g. sample the string ids and split them by first n characters.\r\n\r\nAnother think you can do is run `reindexed = df.set_index(...)` once, and then just look at `reindexed.divisions` and hardcode whatever dask calculated for next time :)",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/961301624/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/962453589",
        "html_url": "https://github.com/dask/dask/issues/8294#issuecomment-962453589",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/8294",
        "id": 962453589,
        "node_id": "IC_kwDOAbcwm845XeBV",
        "user": {
            "login": "DahnJ",
            "id": 18722560,
            "node_id": "MDQ6VXNlcjE4NzIyNTYw",
            "avatar_url": "https://avatars.githubusercontent.com/u/18722560?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/DahnJ",
            "html_url": "https://github.com/DahnJ",
            "followers_url": "https://api.github.com/users/DahnJ/followers",
            "following_url": "https://api.github.com/users/DahnJ/following{/other_user}",
            "gists_url": "https://api.github.com/users/DahnJ/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/DahnJ/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/DahnJ/subscriptions",
            "organizations_url": "https://api.github.com/users/DahnJ/orgs",
            "repos_url": "https://api.github.com/users/DahnJ/repos",
            "events_url": "https://api.github.com/users/DahnJ/events{/privacy}",
            "received_events_url": "https://api.github.com/users/DahnJ/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-11-06T13:41:12Z",
        "updated_at": "2021-11-06T14:03:01Z",
        "author_association": "CONTRIBUTOR",
        "body": "> On a single machine, you'd probably be better off using `shuffle=\"disk\"`.\r\n\r\nAha! Thanks, it never ceases to amaze me how many Dask details I miss out on. I will give it a try and see if it works for my case.\r\n\r\n>    How much memory does your machine have?\r\n\r\n~230GB available\r\n\r\n>    Most operations will involve duplicating the input partition, sometimes multiple times. Also, dask can be slow to clean up data ([[Idea] Could workers sometimes know when to release keys on their own?\u00a0distributed#5114](https://github.com/dask/distributed/issues/5114)), and data transfer involves even more copies ([Deserialization: zero-copy merge subframes when possible [better than last PR]\u00a0distributed#5208](https://github.com/dask/distributed/pull/5208)). So when your partitions are 100MB, a couple extra copies per thread is still bad, but it's a lot less total memory than a couple extra copies of a 1.3GB thing.\r\n\r\nAgain, this was very helpful and enlightening, thanks! \r\n\r\nI guess this is different from using parquet on Hadoop, where I've seen the size of around 1GB recommended?\r\n\r\n> Another think you can do is run `reindexed = df.set_index(...)` once, and then just look at `reindexed.divisions` and hardcode whatever dask calculated for next time :)\r\n\r\nGood point",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/962453589/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]