{
    "url": "https://api.github.com/repos/dask/dask/issues/10991",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10991/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10991/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10991/events",
    "html_url": "https://github.com/dask/dask/issues/10991",
    "id": 2178797517,
    "node_id": "I_kwDOAbcwm86B3c_N",
    "number": 10991,
    "title": "Combined save and calculation is using excessive memory",
    "user": {
        "login": "pp-mo",
        "id": 2089069,
        "node_id": "MDQ6VXNlcjIwODkwNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2089069?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pp-mo",
        "html_url": "https://github.com/pp-mo",
        "followers_url": "https://api.github.com/users/pp-mo/followers",
        "following_url": "https://api.github.com/users/pp-mo/following{/other_user}",
        "gists_url": "https://api.github.com/users/pp-mo/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/pp-mo/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/pp-mo/subscriptions",
        "organizations_url": "https://api.github.com/users/pp-mo/orgs",
        "repos_url": "https://api.github.com/users/pp-mo/repos",
        "events_url": "https://api.github.com/users/pp-mo/events{/privacy}",
        "received_events_url": "https://api.github.com/users/pp-mo/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3880424463,
            "node_id": "LA_kwDOAbcwm87nSpQP",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20triage",
            "name": "needs triage",
            "color": "eeeeee",
            "default": false,
            "description": "Needs a response from a contributor"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2024-03-11T10:39:33Z",
    "updated_at": "2024-03-28T11:26:09Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "Not clear if this is truly a \"bug\", but it seems highly undesirable + I can't work out how to avoid it\r\n\r\n# Describe the issue:\r\nWe are trying to save large data arrays to file, and *at the same time* check the data for any occurrences of a given value\r\nAll while, hopefully, loading and processing each data chunk only once.\r\n\r\nFor each array, we use a `dask.delayed` function which takes as argument both the (delayed) `da.store` and the collision check computation on the same data.\r\nBut this seems to cause Dask to load the entire dataset into memory, \r\nwhich, for big enough data, simply crashes. \r\n\r\n### a bit more detail\r\nWhen saving data to netcdf, we create a file and stream data from Dask arrays into the file variables with 'dask.array.store'.\r\n( typically, out source data is also derived from netcdf files -- but this is not required to provoke the problem ).\r\nWe need at the same time to perform a check on the data, which determines whether there are any data points which are masked, or unmasked points matching a proposed \"fill value\".\r\n\r\nOur existing code combines a delayed 'store' operation with computing the check function.\r\nSince one is a \"store\" and one a \"compute\", they are combined by creating a delayed function which takes both as arguments.\r\n\r\nThe aim of this is that the data should only be fetched once, and streamed to the file one chunk at a time, so that we can handle files larger than memory.\r\n\r\nWhat we have found is that in some cases, this operation is using memory equivalent to the size of the ***entire*** data variable, rather than a small number of its chunks\r\n\r\n# Minimal Complete Verifiable Example\r\n\r\n```python\r\nimport tracemalloc\r\nimport dask\r\nimport dask.array as da\r\nimport numpy as np\r\n\r\n# construct test data as a stack of random arrays\r\nnt, nd = 50, 1000000\r\nlazydata_all = da.stack([\r\n    da.random.uniform(\r\n        0, 1,\r\n        size=nd\r\n    )\r\n    for _ in range(nt)\r\n])\r\n# existing \"target\" array which we will store the result into\r\nstore_target = np.zeros((nt, nd), dtype=np.float64)\r\n\r\ndef array_size_mb(arr: np.ndarray):\r\n    return arr.nbytes * 1.0e-6\r\n\r\nprint(f\"\\nData full size {str(lazydata_all.shape).rjust(16)} = {array_size_mb(lazydata_all):8.1f} Mib\")\r\nprint(f\" .. chunk size {str(lazydata_all[0].shape).rjust(16)} = {array_size_mb(lazydata_all[0]):8.1f} Mib\")\r\n\r\n# Construct the combined store-and-calculate operation, as a delayed\r\n@dask.delayed\r\ndef store_data_and_compute(store_operation, calculation):\r\n    return calculation\r\n\r\ndelayed_save = da.store(lazydata_all, store_target, compute=False, lock=False)\r\nlazy_calculation = lazydata_all[0, 0]\r\ndelayed_result = store_data_and_compute(delayed_save, lazy_calculation)\r\n\r\n# Measure peak additional memory claimed by operations\r\ndef compute_memory_mb(delayed_calc):\r\n    tracemalloc.start()\r\n    delayed_calc.compute()\r\n    _, peak_mem_bytes = tracemalloc.get_traced_memory()\r\n    tracemalloc.stop()\r\n    return peak_mem_bytes * 1.0e-6\r\n\r\nprint(\"\\nCombined calculation:\")\r\ncombined_operation_mb = compute_memory_mb(delayed_result)\r\nchunk_mb = array_size_mb(lazydata_all[0])\r\nprint(f\"Consumed memory ~ {combined_operation_mb:6.2f} Mb.\")\r\nprint(f\"  --> {combined_operation_mb / chunk_mb:.1f} * chunksize\")\r\n\r\n```\r\nSample output :\r\n```\r\nData full size    (50, 1000000) =    400.0 Mib\r\n .. chunk size       (1000000,) =      8.0 Mib\r\n\r\nCombined calculation:\r\nConsumed memory ~ 400.25 Mb.\r\n  --> 50.0 * chunksize\r\n``` \r\n\r\n### NOTE: the individual operations (store or calculate) do ***not*** consume large amounts of memory\r\n```\r\nstore_only_mb = compute_memory_mb(delayed_save)\r\nprint(f\"\\nStore alone, takes {store_only_mb} Mb.\")\r\ncalc_only_mb = compute_memory_mb(lazy_calculation)\r\nprint(f\"Calculate alone, takes {calc_only_mb} Mb.\")\r\n```\r\nResulting\r\n```\r\nStore alone, takes 32.230089 Mb.\r\nCalculate alone, takes 8.018998 Mb.\r\n```\r\n**NOTE:** this on a machine with 4 CPUs, 4 dask workers\r\nHence 32 ~4*8, seems to makes sense\r\n\r\n## Anything else we need to know?\r\n### Environment\r\n\r\n- Dask version: 2023.09.01 and 2024.02.01\r\n- Python version: 3.11\r\n- Operating System: linux\r\n- Machine : 4 CPUs\r\n- Install method (conda, pip, source): conda\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10991/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10991/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}