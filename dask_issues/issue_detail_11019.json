{
    "url": "https://api.github.com/repos/dask/dask/issues/11019",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/11019/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/11019/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/11019/events",
    "html_url": "https://github.com/dask/dask/issues/11019",
    "id": 2204204903,
    "node_id": "I_kwDOAbcwm86DYX9n",
    "number": 11019,
    "title": "Hash join transfer with error cannot pickle '_contextvars.ContextVar' object",
    "user": {
        "login": "guozhans",
        "id": 9510342,
        "node_id": "MDQ6VXNlcjk1MTAzNDI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/9510342?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/guozhans",
        "html_url": "https://github.com/guozhans",
        "followers_url": "https://api.github.com/users/guozhans/followers",
        "following_url": "https://api.github.com/users/guozhans/following{/other_user}",
        "gists_url": "https://api.github.com/users/guozhans/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/guozhans/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/guozhans/subscriptions",
        "organizations_url": "https://api.github.com/users/guozhans/orgs",
        "repos_url": "https://api.github.com/users/guozhans/repos",
        "events_url": "https://api.github.com/users/guozhans/events{/privacy}",
        "received_events_url": "https://api.github.com/users/guozhans/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 3798450416,
            "node_id": "LA_kwDOAbcwm87iZ8Dw",
            "url": "https://api.github.com/repos/dask/dask/labels/p2",
            "name": "p2",
            "color": "ffc133",
            "default": false,
            "description": "Affects more than a few users but doesn't prevent core functions"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 6,
    "created_at": "2024-03-24T06:37:20Z",
    "updated_at": "2024-04-12T09:15:18Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\nHi\r\nI encountered this error, and don't know what happened under the hood. Therefore, I open it for better tracking.\r\n\r\nI have some spatial datasets in parquet format with row group size 64MB that contains nodes and coordinates. I never intend to do any re-partition or shuffle operations, but it could happen during merge, or set index operation, etc.. The hash join issue was found with the dataset bigger more than one row group size enough to create few partitions in dataframe on hash join operations. The error always showed with hash-join-transfer-xxxxxxxxx operation.\r\n\r\nTo workaround this issue, i changed shuffle method back to \"tasks\".\r\n\r\n***Error messages:***\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/project/src/main/python/utils/shuffle_test.py\", line 52, in <module>\r\n    main()\r\n  File \"/opt/project/src/main/python/utils/shuffle_test.py\", line 48, in main\r\n    print(nodes[\"longitude\"].compute())\r\n  File \"/usr/local/lib/python3.10/dist-packages/dask/base.py\", line 375, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/dask/base.py\", line 661, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/loky/process_executor.py\", line 370, in _sendback_result\r\n    result_queue.put(\r\n  File \"/usr/local/lib/python3.10/dist-packages/loky/backend/queues.py\", line 230, in put\r\n    obj = dumps(obj, reducers=self._reducers)\r\n  File \"/usr/local/lib/python3.10/dist-packages/loky/backend/reduction.py\", line 215, in dumps\r\n    dump(obj, buf, reducers=reducers, protocol=protocol)\r\n  File \"/usr/local/lib/python3.10/dist-packages/loky/backend/reduction.py\", line 208, in dump\r\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\r\n  File \"/usr/local/lib/python3.10/dist-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\nTypeError: cannot pickle '_contextvars.ContextVar' object\r\n\r\n```\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport faulthandler\r\nimport logging\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nimport dask\r\nimport dask.dataframe as dd\r\n\r\nfrom distributed import WorkerPlugin, Worker, LocalCluster, Client, progress, wait\r\nfrom loky import ProcessPoolExecutor\r\n\r\n\r\nclass TaskExecutorPool(WorkerPlugin):\r\n    def __init__(self, logger, name):\r\n        self.logger = logger\r\n        self.worker = None\r\n        self.name = name\r\n\r\n    def setup(self, worker: Worker):\r\n        faulthandler.enable()\r\n        executor = ProcessPoolExecutor(max_workers=worker.state.nthreads)\r\n        worker.executors[self.name] = executor\r\n        self.worker = worker\r\n\r\n    def transition(self, key, start, finish, *args, **kwargs):\r\n        if finish == 'error':\r\n            ts = self.worker.tasks[key]\r\n            exc_info = (type(ts.exception), ts.exception, ts.traceback)\r\n            print(f\"Task traceback: {ts.traceback}\")\r\n            print(f\"Task exception: {exc_info}\")\r\n            self.logger.error(f\"Error during computation of {key}, caused by {str(ts.exception)}.\")\r\n\r\n\r\ndef main():\r\n    cluster = LocalCluster(processes=False, silence_logs=logging.DEBUG)\r\n    with Client(cluster) as client:\r\n        client.register_plugin(TaskExecutorPool(logging, \"process\"), name=\"process\")\r\n        with dask.annotate(executor=\"process\", retries=10):\r\n            nodes = dd.read_parquet(\"node parquet file with multiple row group size\", columns=[\"id\", \"tags\"])\r\n            node_coordinates = dd.read_parquet(\"node parquet file with multiple row group size\"\r\n                                               , columns=[\"id\", \"latitude\", \"longitude\"])\r\n            \r\n            # This re-partition operation can make different partitions to nodes, and later merge operation will have hash join operation inside to make error showed more often\r\n            node_coordinates = node_coordinates.repartition(partition_size=\"some size\")\r\n            nodes = nodes.merge(\r\n                node_coordinates, how=\"left\", left_on=[\"id\"], right_on=[\"id\"], suffixes=(False, False), shuffle_method=\"p2p\")\r\n            client.persist(nodes)\r\n            progress(nodes)\r\n            print(nodes[\"latitude\"].compute())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.2.1\r\n- Python version: 3.10\r\n- Operating System: Docker installed Ubuntu 22.04, Host is also Ubuntu 22.04\r\n- Install method (conda, pip, source):\r\n  loky 3.4.1\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/11019/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/11019/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}