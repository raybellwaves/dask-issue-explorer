{
    "url": "https://api.github.com/repos/dask/dask/issues/10626",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10626/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10626/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10626/events",
    "html_url": "https://github.com/dask/dask/issues/10626",
    "id": 1987241706,
    "node_id": "I_kwDOAbcwm852cubq",
    "number": 10626,
    "title": "Sensitivity to parquet file RG splits",
    "user": {
        "login": "fjetter",
        "id": 8629629,
        "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fjetter",
        "html_url": "https://github.com/fjetter",
        "followers_url": "https://api.github.com/users/fjetter/followers",
        "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
        "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
        "organizations_url": "https://api.github.com/users/fjetter/orgs",
        "repos_url": "https://api.github.com/users/fjetter/repos",
        "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
        "received_events_url": "https://api.github.com/users/fjetter/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2023-11-10T09:25:46Z",
    "updated_at": "2023-12-13T08:47:36Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "It turns out we do have performance tests that cover the edge case of very large files w/ row groups after all, see https://github.com/dask/dask/pull/10600\r\n\r\nThe data we have stored in https://s3.console.aws.amazon.com/s3/buckets/coiled-runtime-ci?prefix=ookla-open-data/type%3Dfixed/&region=us-east-2\r\n\r\nThe files in there are 400-500MB on disk (Still have to check details about the RGs. We're splitting this up into 61 parts, i.e. we have on average 4.7 RGs/tasks per file)\r\n\r\nThis is used by the `test_read_hive_partitioned_data` in coiled benchmarks\r\n\r\n`Wall Clock`\r\n![image](https://github.com/dask/dask/assets/8629629/fd77e1ea-b887-4ef4-aef2-e4d422aae672)\r\n\r\n`Average Memory`\r\n![image](https://github.com/dask/dask/assets/8629629/85289319-6ae0-42f4-9150-793887cb42c7)\r\n\r\n\r\nWith this change, this test no longer splits the files into smaller tasks. Therefore, the memory goes up since the operation performed on this dataset reduces immediately. This is expected.\r\nThe walltime performance is however very misleading since there are a couple of things happening here\r\n- Since we no longer split the files we now have fewer tasks and can't utilize all the workers. That alone explains a drop of at least 13% (13/15)\r\n- There is some sort of S3 caching problem going on as well that seems to be exacerbated by having fewer tasks. If I warm up s3, this runtime goes down from 50 to ~13s (vs 9s with a warm cache for the split row groups w/ already cached PQ statistics). This puts us in a range where the performance difference may be explained by the change in available, participating workers (we're still missing 10-15% but I also didn't measure the statistics gathering here)\r\n\r\n\r\nWhat I find interesting with this is that with the split into row groups, the effect of having S3 be warm seems to be much \r\nless severe.\r\n\r\n|   | Cold S3 |  Warm S3 | Delta |\r\n| - | - | - | - |\r\n| No Split  | 45-50  | 13 | -75% |\r\n| Split  | ~18  | 9 | -50% |\r\n\r\nI suspect that with the split rowgroups approach, S3 naturally warms up since we are touching the files multiple times such that the first row group access is slow but the later ones are fast.\r\n\r\nI wonder if we can leverage this somehow. Can we warm up files that are going to be loaded later on with very simple requests (like reading the footer N times)? Can this be done by a scheduler plugin or a dangling extra task such that workers will never/rarely encounter cold files?\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10626/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10626/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}