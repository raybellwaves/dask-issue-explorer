{
    "url": "https://api.github.com/repos/dask/dask/issues/9522",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9522/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9522/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9522/events",
    "html_url": "https://github.com/dask/dask/issues/9522",
    "id": 1389735782,
    "node_id": "I_kwDOAbcwm85S1a9m",
    "number": 9522,
    "title": "[Discussion] Cluster-aware graph logic",
    "user": {
        "login": "ian-r-rose",
        "id": 5728311,
        "node_id": "MDQ6VXNlcjU3MjgzMTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5728311?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ian-r-rose",
        "html_url": "https://github.com/ian-r-rose",
        "followers_url": "https://api.github.com/users/ian-r-rose/followers",
        "following_url": "https://api.github.com/users/ian-r-rose/following{/other_user}",
        "gists_url": "https://api.github.com/users/ian-r-rose/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ian-r-rose/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ian-r-rose/subscriptions",
        "organizations_url": "https://api.github.com/users/ian-r-rose/orgs",
        "repos_url": "https://api.github.com/users/ian-r-rose/repos",
        "events_url": "https://api.github.com/users/ian-r-rose/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ian-r-rose/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1342304743,
            "node_id": "MDU6TGFiZWwxMzQyMzA0NzQz",
            "url": "https://api.github.com/repos/dask/dask/labels/core",
            "name": "core",
            "color": "000000",
            "default": false,
            "description": ""
        },
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2022-09-28T17:47:36Z",
    "updated_at": "2022-10-31T02:14:31Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "Historically, Dask has been execution-model agnostic when constructing graphs. This has had some notable advantages, since it means that dask task graphs/collections can be executed using any of a number of different schedulers. So the same graph can be computed using local schedulers, the distributed scheduler, and even third-party schedulers like Ray. It also enforces a certain amount of separation-of-concerns so that `dask/dask` and `dask/distributed` can be developed in a semi-independent fashion.\r\n\r\nThere is, however, some downside to this cluster-agnostic approach. Often there are specific tuning parameters and configuration options that significantly affect the performance of computations. Choosing good values for these parameters typically requires fairly advanced knowledge of Dask, its optimizations, and distributed systems in general. As developers, we would like to remove the onus from users to possess this information, but often *cannot do that without knowledge of the cluster that they will be running it on.*\r\n\r\nOne example of this that I ran into recently was with groupby/aggregations and the `split_every` parameter. It defaults to 8, but there are some easy-to-hit cases where that winds up being a [very bad choice](https://github.com/dask/dask/pull/9453#discussion_r961801460). And making a better choice requires knowing, at a minimum, the number of workers you will likely be using. This doesn't seem like an unreasonable ask for the task graph, but it's not easily available today.\r\n\r\nSome tuning parameters which might benefit from having more context about the cluster:\r\n\r\n* `chunks` (array creation and rechunking)\r\n* `split_every` (array reductions)\r\n* `merge_chunks` (array reshaping)\r\n* `split_every`, `split_out` (dataframe reductions)\r\n* `npartitions`, `broadcast` (dataframe merges)\r\n* `divisions`, `npartitions`, `partition_size` (dataframe creation and repartitioning)\r\n* `split_every` (bag reductions)\r\n* `dataframe.parquet.metadata-task-size-remote` (parquet metadata processing)\r\n\r\nThere are some caveats to the above assertion that dask task graphs are cluster-unaware:\r\n1. The default cluster implementation depends on whether there is an active `distributed.Client`. This usually does the right thing, but can cause problems if the graph is constructed before the client starts.\r\n2. p2p shuffling does some out-of-task-graph work on the workers.\r\n\r\nThese are both interesting antecedents for reasoning about the execution environment, but it's not clear to me how reproducible they are in other contexts. From my perspective, I'd like to have something like the following:\r\n\r\n1. A repeatable pattern for learning about a cluster during graph construction. Today, the default shuffle implementation goes through the dask config system, but I might want something a bit more formalized and thread-safe. It should also be able to be run on the scheduler: if we are delaying graph construction until an expression reaches the scheduler, we should be able to take advantage of more detailed information.\r\n2. Possibly have an abstract HLG/HLE for repartitioning and rechunking. In particular, knowing some things about the number of workers and how much memory they have would help to avoid having too much or too little work-per-worker.\r\n\r\nSome questions I don't have good answers to:\r\n\r\n1. Even if we do have cluster information available during graph construction, can we write down good defaults for some of the tuning parameters? In some cases, maybe yes, in other cases, maybe no.\r\n2. Is it possible all of the above is wildly overthinking things? Could a half-dozen blocks like the following do everything I want?\r\n    ```python\r\n    try:\r\n        import distributed\r\n        info = distributed.get_default_client().scheduler_info()\r\n        # Set some defaults based on the info.\r\n    except ImportError, ValueError:\r\n        # Set some default defaults.\r\n    ```\r\n3. Is any of the above doable in the absence of HLG refactoring (cf discussion in #9159 and linked issues)? If the answer is no, I still think this is a valuable thing to think about when designing and implementing their replacement.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9522/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9522/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}