[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1263667820",
        "html_url": "https://github.com/dask/dask/issues/9522#issuecomment-1263667820",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9522",
        "id": 1263667820,
        "node_id": "IC_kwDOAbcwm85LUgps",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-30T14:43:51Z",
        "updated_at": "2022-09-30T14:45:59Z",
        "author_association": "MEMBER",
        "body": "I think it makes a lot of sense to (optionally) allow the client/scheduler to use cluster information to choose how to build the graph. While working on https://github.com/dask/dask/issues/9216, I realized that the new HLG/Layer design could easily be used to label each individual task with a preferred worker. The only changes outside of HLG design would be:\r\n\r\n1. We would need to allow the client to (optionally) check if there is an active distributed cluster, and to use that information to encode worker preferences in the `Layer`-materialization logic.\r\n2. We would need a simple mechanism to annotate individual tasks with preferred workers, and we would need to add the scheduling logic to **use** the worker preference when assigning tasks.\r\n\r\nWith these two features in place, it would be relatively simple for appropriate worker preferences to be assigned as an HLG is materialized. For example, consider a simple HLG comprising 1+ `Blockwise` layers, followed by a `groupby` aggregation:\r\n\r\n![Screen Shot 2022-09-30 at 9 44 14 AM](https://user-images.githubusercontent.com/20461013/193295551-2a7b3932-7af5-4dcc-a2ee-e2d8751597e4.png)\r\n\r\n\r\nIf feature (1) was available in Dask, the aggregation layer could use information about the expected worker set to specify graph-materialization parameters, like `split_every`. For example, if there are two workers in the detected cluster, `split_every` could be chosen so that the aggregation could be perfectly balanced between the two workers:\r\n\r\n![Screen Shot 2022-09-30 at 9 44 46 AM](https://user-images.githubusercontent.com/20461013/193295631-8f7780e1-e8e9-4352-9a87-df3d95debe9f.png)\r\n\r\nAt graph-materialization time, a layer will be materialized within a method like `Layer.subgraph(keys: Set, color: str | dict | None = None)`. Within the `subgraph` logic, the cluster-aware aggregation layer could (optionally) assign a **worker color** (or perhaps even a specific worker address) to each task it materializes (prioritizing the `color` specified in the function signature if it isn't `None`):\r\n\r\n![Screen Shot 2022-09-30 at 9 45 11 AM](https://user-images.githubusercontent.com/20461013/193295710-d42717b7-298a-4b19-b204-b9c794e6c161.png)\r\n\r\nSince each layer will also be responsible for specifying which keys it wants from dependency layers (*only* after it has been materialized by a `subgraph` call), the returned key-dependency information can be organized by **dependency color**. This way, the preferred worker dependencies can propagate through the entire graph as it is materialized:\r\n\r\n![Screen Shot 2022-09-30 at 9 45 35 AM](https://user-images.githubusercontent.com/20461013/193295831-a536b8cf-eb63-426d-836e-9a368c0942c1.png)\r\n\r\nThe final result is that (1) we are making better decisions about graph topology, and (2) are making it easier for the scheduler to properly co-locate distinct tasks on the same worker.  If done properly, I suppose this design could even make successful HLG-level fusion less critical.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1263667820/reactions",
            "total_count": 2,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 2,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]