{
    "url": "https://api.github.com/repos/dask/dask/issues/5769",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/5769/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/5769/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/5769/events",
    "html_url": "https://github.com/dask/dask/issues/5769",
    "id": 546109123,
    "node_id": "MDU6SXNzdWU1NDYxMDkxMjM=",
    "number": 5769,
    "title": "Retaining chunking when reading HDF files (on cloud storage)",
    "user": {
        "login": "chrisroat",
        "id": 1053153,
        "node_id": "MDQ6VXNlcjEwNTMxNTM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1053153?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/chrisroat",
        "html_url": "https://github.com/chrisroat",
        "followers_url": "https://api.github.com/users/chrisroat/followers",
        "following_url": "https://api.github.com/users/chrisroat/following{/other_user}",
        "gists_url": "https://api.github.com/users/chrisroat/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/chrisroat/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/chrisroat/subscriptions",
        "organizations_url": "https://api.github.com/users/chrisroat/orgs",
        "repos_url": "https://api.github.com/users/chrisroat/repos",
        "events_url": "https://api.github.com/users/chrisroat/events{/privacy}",
        "received_events_url": "https://api.github.com/users/chrisroat/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862305,
            "node_id": "MDU6TGFiZWwyNDI4NjIzMDU=",
            "url": "https://api.github.com/repos/dask/dask/labels/array",
            "name": "array",
            "color": "006b75",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 9,
    "created_at": "2020-01-07T06:40:36Z",
    "updated_at": "2021-10-13T03:27:58Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "We have a project creating microscope O(1000) images in chunked HDF format, which we ship off the machine ASAP and into a GCS cloud bucket.  We are then attempting to process them later using dask.  When attempting this on a the processes scheduler, I run into the standard problem of needing to open the unpicklable objects in a delayed function.  (This probably needed anyway, since opening a GCS file can take seconds -- better to shard all that file I/O.)  \r\n\r\nThe first thing I hit was #5763 , where I shouldn't return an HDF5 dataset from my delayed function, since it doesn't seem to have the right API that dask is expecting.   As @mrocklin suggested, I should just return the materialized array directly.  This has the disadvantage of throwing away the chunking.\r\n\r\nIs the code below the recommended approach? \r\n\r\nIs the alternative of returning a dask array (also suggested by @mrocklin) during creation viable?  It seems to recover the chunking and is much faster for a random lookup.  *It oddly seems to need a \".compute().compute()\" in order to fully compute a result.  Is that something people do?*\r\n\r\n```\r\nimport dask\r\nimport dask.array as da\r\nfrom dask.diagnostics import ProgressBar\r\nimport gcsfs\r\nimport h5py\r\nimport numpy as np\r\n\r\ngcs = gcsfs.GCSFileSystem()\r\n\r\n# Use a function to lazily load an h5py.File on a worker, \r\n# since they aren't picklable\r\ndef get_data(fname, group):\r\n    h5f = gcs.open(fname, 'rb')\r\n    arr = h5py.File(h5f, 'r').get(group)\r\n    return arr[()]  # Cannot return `arr` directly, since it doesn't have correct API for dask.delayed\r\n    # Another option would be to return an array, but that requires .compute().compute()!\r\n    # return da.from_array(arr, chunks=(16, 128, 128))\r\n    \r\nrnd = []\r\nfor fov in range(8):\r\n    fname = 'gs://dask-hdf-gcs/dataset/data-R00-F%03d.h5' % fov\r\n    acq = []\r\n    # Lazily load h5py files\r\n    for c in range(4):\r\n        delayed = dask.delayed(get_data)(fname, '/channel%d' % c)\r\n        # Shape and type need to be known!\r\n        chn = da.from_delayed(delayed, shape=(96, 2048, 2048), dtype=np.uint16)\r\n        acq.append(chn)\r\n        \r\n    rnd.append(da.stack(acq))\r\ndata = da.concatenate(rnd, axis=3)\r\n\r\nwith ProgressBar():\r\n    result = data[2,40,1000,5000].compute()  # Need 2nd .compute() call if get_data returns dask array\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/5769/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/5769/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}