{
    "url": "https://api.github.com/repos/dask/dask/issues/8172",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8172/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8172/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8172/events",
    "html_url": "https://github.com/dask/dask/issues/8172",
    "id": 1005556634,
    "node_id": "I_kwDOAbcwm84775Oa",
    "number": 8172,
    "title": "botocore error when writing parquet to S3",
    "user": {
        "login": "cliffplaysdrums",
        "id": 17258650,
        "node_id": "MDQ6VXNlcjE3MjU4NjUw",
        "avatar_url": "https://avatars.githubusercontent.com/u/17258650?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/cliffplaysdrums",
        "html_url": "https://github.com/cliffplaysdrums",
        "followers_url": "https://api.github.com/users/cliffplaysdrums/followers",
        "following_url": "https://api.github.com/users/cliffplaysdrums/following{/other_user}",
        "gists_url": "https://api.github.com/users/cliffplaysdrums/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/cliffplaysdrums/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/cliffplaysdrums/subscriptions",
        "organizations_url": "https://api.github.com/users/cliffplaysdrums/orgs",
        "repos_url": "https://api.github.com/users/cliffplaysdrums/repos",
        "events_url": "https://api.github.com/users/cliffplaysdrums/events{/privacy}",
        "received_events_url": "https://api.github.com/users/cliffplaysdrums/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2021-09-23T15:15:23Z",
    "updated_at": "2021-11-07T10:42:38Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I'm unable to write a particular dataframe to S3.\r\n\r\n## Code overview\r\nRead a parquet file from S3\r\n```python\r\nimport dask.dataframe as dd\r\ndf = dd.read_parquet(path=[f's3://{bucket}/{path_to_file}'],  # single file\r\n    kwargs={'dataset': {'schema': schema}},  # Tried with & without this\r\n    storage_options={'client_kwargs': s3_conf})\r\n```\r\nExtract rows with an na value to troubleshoot later\r\n```python\r\nnull_df = df[df.isna().any(axis=1)]\r\n```\r\nSome columns that should be int have na values we can't directly cast. Find na values in columns that are supposed to be int, and replace them with a representative value (-1)\r\n```python\r\nnull_df[col_names] = null_df[col_names].where(~null_df[col_names].isna(), -1.0)\r\n```\r\nNow types can be safely converted\r\n```python\r\nnull_df = null_df.astype(dict_mapping_cols_to_np_types)\r\n```\r\n(I also replaced na values in float type columns with 0 to be safe).\r\n\r\nWrite the parquet file to a new S3 location\r\n```python\r\n# pyarrow_schema is the pyarrow equivalent to our numpy type mapping from earlier \r\nnull_df.to_parquet(path=f's3://{bucket}/{new_path}.parquet', engine='pyarrow', schema=pyarrow_schema, storage_options={'client_kwargs': s3_conf})\r\n```\r\n\r\n## The error\r\nThe primary resulting error:\r\n```Traceback (most recent call last):\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/s3fs/core.py\", line 248, in _call_s3\r\n    out = await method(**additional_kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/aiobotocore/client.py\", line 155, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (MalformedACLError) when calling the CreateMultipartUpload operation: The XML you provided was not well-formed or did not validate against our published schema.\r\n```\r\n\r\n<details><summary>The full trace</summary>\r\n<p>\r\nTraceback (most recent call last):\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/s3fs/core.py\", line 248, in _call_s3\r\n    out = await method(**additional_kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/aiobotocore/client.py\", line 155, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (MalformedACLError) when calling the CreateMultipartUpload operation: The XML you provided was not well-formed or did not validate against our published schema.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/pyarrow/parquet.py\", line 1987, in write_table\r\n    writer.write_table(table, row_group_size=row_group_size)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/pyarrow/parquet.py\", line 693, in write_table\r\n    self.writer.write_table(table, row_group_size=row_group_size)\r\n  File \"pyarrow/_parquet.pyx\", line 1439, in pyarrow._parquet.ParquetWriter.write_table\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/fsspec/spec.py\", line 1401, in write\r\n    self.flush()\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/fsspec/spec.py\", line 1437, in flush\r\n    self._initiate_upload()\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/s3fs/core.py\", line 1871, in _initiate_upload\r\n    self.mpu = self._call_s3(\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/s3fs/core.py\", line 1863, in _call_s3\r\n    return self.fs.call_s3(method, self.s3_additional_kwargs, *kwarglist, **kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/fsspec/asyn.py\", line 88, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/fsspec/asyn.py\", line 69, in sync\r\n    raise result[0]\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/fsspec/asyn.py\", line 25, in _runner\r\n    result[0] = await coro\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/s3fs/core.py\", line 268, in _call_s3\r\n    raise err\r\nOSError: [Errno 22] The XML you provided was not well-formed or did not validate against our published schema.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"src/main.py\", line 379, in <module>\r\n    handle_run_id(s3_client=s3_client,\r\n  File \"/home/lz1sx9/project_repos/AVMMS_124381_Build_map-builder/features/post_processing/src/utils/decorators.py\", line 35, in wrapper\r\n    return func(logger=logger, *args, **kwargs)\r\n  File \"src/main.py\", line 258, in handle_run_id\r\n    handle_geohash(s3_conf=s3_conf, run_id=run_id, bucket=bucket, geohash=geohash, all_files=all_files_carto,\r\n  File \"/home/lz1sx9/project_repos/AVMMS_124381_Build_map-builder/features/post_processing/src/utils/decorators.py\", line 35, in wrapper\r\n    return func(logger=logger, *args, **kwargs)\r\n  File \"src/main.py\", line 198, in handle_geohash\r\n    s3_utils.write_null_rows_to_s3(df_to_process=road_edges,\r\n  File \"/home/lz1sx9/project_repos/AVMMS_124381_Build_map-builder/features/post_processing/src/utils/decorators.py\", line 35, in wrapper\r\n    return func(logger=logger, *args, **kwargs)\r\n  File \"/home/lz1sx9/project_repos/AVMMS_124381_Build_map-builder/features/post_processing/src/utils/s3_utils.py\", line 167, in write_null_rows_to_s3\r\n    larger_df.to_parquet(path=f's3://{bucket}/feature_testing/cliff-testing/test.parquet',\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/dataframe/core.py\", line 4540, in to_parquet\r\n    return to_parquet(self, path, *args, **kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/dataframe/io/parquet/core.py\", line 725, in to_parquet\r\n    return compute_as_if_collection(\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/base.py\", line 315, in compute_as_if_collection\r\n    return schedule(dsk2, keys, **kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/threaded.py\", line 79, in get\r\n    results = get_async(\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/local.py\", line 517, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/local.py\", line 325, in reraise\r\n    raise exc\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/local.py\", line 223, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/utils.py\", line 35, in apply\r\n    return func(*args, **kwargs)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py\", line 946, in write_partition\r\n    pq.write_table(\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/pyarrow/parquet.py\", line 1987, in write_table\r\n    writer.write_table(table, row_group_size=row_group_size)\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/pyarrow/parquet.py\", line 678, in __exit__\r\n    self.close()\r\n  File \"/home/lz1sx9/anaconda3/envs/dask/lib/python3.8/site-packages/pyarrow/parquet.py\", line 700, in close\r\n    self._metadata_collector.append(self.writer.metadata)\r\n  File \"pyarrow/_parquet.pyx\", line 1453, in pyarrow._parquet.ParquetWriter.metadata.__get__\r\nRuntimeError: file metadata is only available after writer close\r\n</p></details>\r\n\r\n## Other notes\r\nI have no issues writing other dask dataframes to this location with the same approach. I can also split my dataframe with `null_df.random_split(frac=[.4, .6], random_state=1)`, and the smaller portion will write but not the larger. I've compared the smaller and larger by inspecting `df.columns`, `df.dtypes`, and `df.info` and see no differences.\r\n\r\nThe parquet file I originally read from S3 is 5.91 MB (not sure how that changes after reading to a local location), so I assume `null_df` is significantly smaller since it's a subset of the original.\r\n\r\n## Environment\r\n\r\n- Dask version: 2021.7.2, same result with 2021.09.1\r\n- Python version: 3.8\r\n- Operating System: Ubuntu 18.04.5 LTS\r\n- Install method (conda, pip, source): pip from inside dedicated conda env\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8172/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8172/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}