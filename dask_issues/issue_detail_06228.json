{
    "url": "https://api.github.com/repos/dask/dask/issues/6228",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/6228/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/6228/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/6228/events",
    "html_url": "https://github.com/dask/dask/issues/6228",
    "id": 622742045,
    "node_id": "MDU6SXNzdWU2MjI3NDIwNDU=",
    "number": 6228,
    "title": "Cache doesn't cull unreachable tasks after replacement",
    "user": {
        "login": "gjoseph92",
        "id": 3309802,
        "node_id": "MDQ6VXNlcjMzMDk4MDI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gjoseph92",
        "html_url": "https://github.com/gjoseph92",
        "followers_url": "https://api.github.com/users/gjoseph92/followers",
        "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
        "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
        "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
        "repos_url": "https://api.github.com/users/gjoseph92/repos",
        "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1342304743,
            "node_id": "MDU6TGFiZWwxMzQyMzA0NzQz",
            "url": "https://api.github.com/repos/dask/dask/labels/core",
            "name": "core",
            "color": "000000",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2020-05-21T19:52:11Z",
    "updated_at": "2021-10-13T05:34:12Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "Using the `Cache` class, tasks which are no longer needed\u2014because their results have been found in the cache\u2014may still be run.\r\n\r\n```python\r\nIn [1]: import time\r\nIn [2]: import dask\r\nIn [3]: import dask.cache\r\n\r\nIn [4]: @dask.delayed\r\n   ...: def slow(x):\r\n   ...:     print(f\"slow {x}\")\r\n   ...:     time.sleep(2)\r\n   ...:     return x\r\n   ...:\r\n\r\nIn [5]: @dask.delayed\r\n   ...: def fast_add(x, y):\r\n   ...:     print(f\"fast_add {x} {y}\")\r\n   ...:     return x + y\r\n   ...:\r\n\r\nIn [6]: result = fast_add(slow(1), slow(2))\r\n\r\nIn [7]: cache = dask.cache.Cache(1e6)\r\n\r\nIn [8]: # as expected, the first run takes 2sec and all functions run\r\n   ...: %%time\r\n   ...: with cache:\r\n   ...:     result.compute()\r\n   ...:\r\nslow 1\r\nslow 2\r\nfast_add 1 2\r\nCPU times: user 6.66 ms, sys: 3.49 ms, total: 10.1 ms\r\nWall time: 2.01 s\r\n\r\nIn [9]: # also as expected, the second run is fast, since all results are in the cache---no functions run\r\n   ...: %%time\r\n   ...: with cache:\r\n   ...:     result.compute()\r\n   ...:\r\nCPU times: user 422 \u00b5s, sys: 24 \u00b5s, total: 446 \u00b5s\r\nWall time: 442 \u00b5s\r\n\r\nIn [10]: cache.cache.data\r\nOut[10]:\r\n{'slow-3d3a6713-ce9f-49ea-9308-a765127fa68e': 1,\r\n 'slow-670ba8b8-6ab1-4843-87d2-c5bdc7e0a730': 2,\r\n 'fast_add-2432fe5a-e323-4bc5-b11d-7f357ac40f55': 3}\r\n\r\nIn [11]: # what if we had a cache miss on the upstream `slow` functions,\r\n    ...: # but still a cache hit on the result we're actually trying to compute?\r\n    ...: # we'd expect the slow functions to still not run, because with the result\r\n    ...: # already in the cache, those upstream dependencies are essentially unreachable\r\n    ...: # keys---nothing depends on them anymore.\r\n\r\nIn [11]: del cache.cache.data['slow-3d3a6713-ce9f-49ea-9308-a765127fa68e']\r\n    ...: del cache.cache.data['slow-670ba8b8-6ab1-4843-87d2-c5bdc7e0a730']\r\n\r\nIn [12]: # to confirm: the only result key we need is still in the cache\r\n    ...: result.__dask_keys__()                                                                                                                                                               \r\nOut[12]: ['fast_add-2432fe5a-e323-4bc5-b11d-7f357ac40f55']\r\n\r\nIn [13]: cache.cache.data                                                                                                                                                                     \r\nOut[13]: {'fast_add-2432fe5a-e323-4bc5-b11d-7f357ac40f55': 3}\r\n\r\nIn [14]: # yet the `slow` functions still run, even though they're unneeded. `fast_add` doesn't, because it's a cache hit.\r\n    ...: %%time\r\n    ...: with cache:\r\n    ...:     result.compute()\r\n    ...:\r\nslow 1\r\nslow 2\r\nCPU times: user 3.44 ms, sys: 1.46 ms, total: 4.9 ms\r\nWall time: 2.01 s\r\n```\r\n\r\nThe Cache class simply inserts cached values into the dask prior to computation:\r\nhttps://github.com/dask/dask/blob/3b92efff2e779f59e95e05af9b8f371d56227d02/dask/cache.py#L47-L49\r\n\r\nHowever, the implementation of `get_async` assumes the graph it's passed has already been optimized and culled. Since Cache is a Callback, and Callbacks operate post-optimization, the graph it produces won't ever get optimized. Therefore, the now-unreachable tasks in the graph are still computed.\r\n\r\nThere are a couple ways we could solve this:\r\n\r\n1. Have `get_async` always cull the graph. IMO, the contract of `get_async` is just that it'll compute the `result` keys from the given `dsk`\u2014_not_ that it'll execute every task in the graph. To me, it's just an unfortunate implementation detail that it schedules tasks to run which are not direct dependencies of the `result` keys.\r\n\r\n    I wouldn't necessarily implement this by just calling `cull` in `get_asyc` (though it may not be the worst idea). Instead, I imagine that [`start_state_from_dask`](https://github.com/dask/dask/blob/3b92efff2e779f59e95e05af9b8f371d56227d02/dask/local.py#L139-L198) could be refactored in such a way that it walks the graph starting from the result keys, which would probably be about the same performance-wise, but have the property of culling the graph along the way.\r\n\r\n2. The Callback interface doesn't seem suited to callbacks that modify the graph, because they're not told which keys are being requested. If `_start` were called with `(dsk, keys)` instead of just `(dsk)`, then the callback could just call `cull` itself inside `_start`. But without `keys`, it doesn't have enough information to do any optimizations.\r\n\r\n    This would be a pretty simple change, but a breaking change to the Callback interface.\r\n\r\n3. Perhaps callbacks really shouldn't be modifying the graph like this\u2014should cache-replacement be considered an optimizer instead? That interface calls `optimize(dsk, keys)`, so it would work. `Cache.__enter__` could temporarily add a function to `config[\"optimizations\"]` that replaces the keys in the graph with their cached values, then culls it.\r\n\r\n    However, that won't work with the current [`collections_to_dsk`](https://github.com/dask/dask/blob/3b92efff2e779f59e95e05af9b8f371d56227d02/dask/base.py#L204-L231) implementation, which is where `config[\"optimizations\"]` are applied: it applies `config[\"optimizations\"]` _before_ any collection-specific optimizations. Since the cache uses the `_posttask` callback for cache puts, where the keys are in their fully-optimized form, the cache gets also need to operate on the fully-optimized graph. Basically, cache replacement always has to be the very last optimization pass, and there's no interface for that right now.\r\n\r\n    Two options for this:\r\n        a. Flip these sections:\r\n        https://github.com/dask/dask/blob/e3f80df7fff7627082688b7cccff1e9131001446/dask/base.py#L213-L221\r\n\r\n      so that `optimizations` are applied after the collection-specific optimizations. Easy, but possibly a breaking change for anyone using the `optimizations=` interface.\r\n\r\n      EDIT: looks like this flip actually just happened in #6137?! Hope that doesn't break anything for anyone...\r\n\r\n      b. Add another kwarg for `\"post_optimizations\"`?\r\n\r\nFor context, I found this because I wrote a custom caching callback, and referred to the `Cache` implementation while writing it. I recently found this bug in mine, and because of all the limitations I've described above, I can't find any way to work around it without making some change to dask.\r\n\r\nWhich direction (cull in `get_async`, pass keys to `_start`, or register in `config[\"optimizations\"]`) do you think would be the best way to solve this?",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/6228/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/6228/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}