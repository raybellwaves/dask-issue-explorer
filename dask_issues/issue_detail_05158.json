{
    "url": "https://api.github.com/repos/dask/dask/issues/5158",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/5158/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/5158/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/5158/events",
    "html_url": "https://github.com/dask/dask/issues/5158",
    "id": 473175535,
    "node_id": "MDU6SXNzdWU0NzMxNzU1MzU=",
    "number": 5158,
    "title": "Why uneven task allocation on many identical operations?",
    "user": {
        "login": "birdsarah",
        "id": 1796208,
        "node_id": "MDQ6VXNlcjE3OTYyMDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1796208?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/birdsarah",
        "html_url": "https://github.com/birdsarah",
        "followers_url": "https://api.github.com/users/birdsarah/followers",
        "following_url": "https://api.github.com/users/birdsarah/following{/other_user}",
        "gists_url": "https://api.github.com/users/birdsarah/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/birdsarah/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/birdsarah/subscriptions",
        "organizations_url": "https://api.github.com/users/birdsarah/orgs",
        "repos_url": "https://api.github.com/users/birdsarah/repos",
        "events_url": "https://api.github.com/users/birdsarah/events{/privacy}",
        "received_events_url": "https://api.github.com/users/birdsarah/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 386719400,
            "node_id": "MDU6TGFiZWwzODY3MTk0MDA=",
            "url": "https://api.github.com/repos/dask/dask/labels/scheduler",
            "name": "scheduler",
            "color": "D10945",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 5,
    "created_at": "2019-07-26T04:48:32Z",
    "updated_at": "2021-10-13T01:16:24Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "This may be just be a docs issue as I just don't understand what's happening here.\r\n\r\nI am on @rjzamora's #5157 branch, but I doubt that's the cause of this issue.\r\n\r\nDask was set up using dask yarn:\r\n\r\n```\r\nprofile = {\r\n    'worker_vcores': 2,\r\n    'worker_memory': '2.75GB',\r\n    'n_workers': 78,\r\n}\r\n```\r\n\r\nMy code is very simple:\r\n\r\n```\r\ndf = dd.read_parquet(s3_parquet, engine='fast_parquet', gather_statistics=False)\r\nlen(df)\r\n```\r\nMy parquet file is ~74k partitions.\r\n\r\nSo ~74k tasks are created to read_parquet and get the len. Specifically\r\n\r\n![Screenshot from 2019-07-25 23-42-08](https://user-images.githubusercontent.com/1796208/61926356-dd27a080-af35-11e9-8703-ae2998c37b0f.png)\r\n\r\nI am having a pretty poor success rate in getting this to run. It fails more times than it passes. The failure is a credentials error which I posted on gitter about. \r\n\r\nThat's a bit mysterious, and specific, so I just want to keep this issue about why the task allocation is so uneven. Here are some example shots of the distribution of the number of items processing:\r\n\r\n![Screenshot from 2019-07-25 23-44-01](https://user-images.githubusercontent.com/1796208/61926476-51624400-af36-11e9-8532-02871fc96372.png)\r\n![Screenshot from 2019-07-25 23-37-31](https://user-images.githubusercontent.com/1796208/61926477-51624400-af36-11e9-95f4-ff6a672845c7.png)\r\n![Screenshot from 2019-07-25 23-45-48](https://user-images.githubusercontent.com/1796208/61926492-5fb06000-af36-11e9-8b7b-0921eb311f50.png)\r\n![Screenshot from 2019-07-25 23-47-23](https://user-images.githubusercontent.com/1796208/61926539-92f2ef00-af36-11e9-8d45-f0db8895ad7d.png)\r\n\r\nWhy are all my workers not chugging away at the queue evenly. It appears to me (happy to be wrong) that things would be running faster if the tasks were more evenly allocated.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/5158/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/5158/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}