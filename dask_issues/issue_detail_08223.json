{
    "url": "https://api.github.com/repos/dask/dask/issues/8223",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8223/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8223/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8223/events",
    "html_url": "https://github.com/dask/dask/pull/8223",
    "id": 1016761474,
    "node_id": "PR_kwDOAbcwm84st65P",
    "number": 8223,
    "title": "[Never Merge] Prototype for scalable dataframe shuffle",
    "user": {
        "login": "gjoseph92",
        "id": 3309802,
        "node_id": "MDQ6VXNlcjMzMDk4MDI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gjoseph92",
        "html_url": "https://github.com/gjoseph92",
        "followers_url": "https://api.github.com/users/gjoseph92/followers",
        "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}",
        "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions",
        "organizations_url": "https://api.github.com/users/gjoseph92/orgs",
        "repos_url": "https://api.github.com/users/gjoseph92/repos",
        "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gjoseph92/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 4,
    "created_at": "2021-10-05T20:08:42Z",
    "updated_at": "2022-08-03T14:15:47Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "draft": true,
    "pull_request": {
        "url": "https://api.github.com/repos/dask/dask/pulls/8223",
        "html_url": "https://github.com/dask/dask/pull/8223",
        "diff_url": "https://github.com/dask/dask/pull/8223.diff",
        "patch_url": "https://github.com/dask/dask/pull/8223.patch",
        "merged_at": null
    },
    "body": "_This is a duplicate of https://github.com/dask/dask/pull/8209, but I'm taking over from @fjetter since I may want to occasionally push small fixes here. Original message copied below, with edits._\r\n\r\nThis is a prototype implementation of a new `dask.dataframe` shuffle algorithm which runs on a `dask.distributed` cluster. Different to the task based shuffle algorithm this uses an out-of-band communication and administration approach to circumvent scheduler imposed bottlenecks.\r\n\r\n## How to try / Feedback requested\r\nInstall this branch of `dask/dask` (`pip install -U git+https://github.com/gjoseph92/dask@shuffle_service`) and run a shuffle workload (`set_index`, `groupby`, etc.), _passing the keyword argument `shuffle=\"service\"`_. Until distributed 2021.10.0 or later is released, you'll also need to install `dask/distributed` from `main` (`pip install -U git+https://github.com/dask/distributed`).\r\n\r\nWith this PR, we've been able to easily do shuffles that crash the cluster currently. Additionally, since this writes intermediate data to disk, you can shuffle larger-than-memory DataFrames. Note that the data written to disk won't show up as spilled-to-disk on the dashboard. Similarly, you'll see high unmanged memory on workers while the shuffle is working.\r\n\r\nAs a rule of thumb:\r\n* total cluster disk space needs to fit the full dataset\r\n* worker RAM needs to fit ~2GiB + partition_size * nthreads * ~5x safety factor?\r\n\r\nAdditionally, more threads don't improve performance much (since everything is GIL-bound), so we recommend 2 threads unless other parts of your workload require more.\r\n\r\nWe would love you to try this out and report back to us. This implementation is targeted for large scale data processing and we would appreciate people trying this out and giving us feedback about it. Especially if you have large datasets sitting around. If you encounter any stability or performance related issues, please _open a dedicated ticket_ and link to this PR such that we can structure discussions a bit. \r\n\r\n## \u26a0\ufe0f Warnings \u26a0\ufe0f\r\nThis is experimental. We do not expect this PR to ever be merged. Instead, we'll take ideas (and feedback) from this PR into a different one that's better-designed, stable, and maintainable.\r\n\r\nWith that explained, here are things to look out for:\r\n* Doesn't work for `merge` yet, because that requires multiple simultaneous shuffles\r\n* Requires distributed>=2021.10.0 which doesn't exist yet, so until it does, you need to install distributed from `main`\r\n* All workers must have >= 2 threads\r\n* The cluster must have enough total disk space to hold the entire dataset (but can have much less RAM than that)\r\n* If a worker runs out of disk space, the whole shuffle will error\r\n* Workers sometimes run out of memory and die randomly during the `transfer` phase\r\n* If a worker dies during the `transfer` phase, the cluster will deadlock for 15 minutes typically (distributed's 300s connect timeout * 3 retries), then the task will error\r\n* If a worker dies during the `unpack` phase, the cluster will deadlock indefinitely\r\n* Multiple shuffles at the same time will fail in strange ways. Running a shuffle more than once on a cluster without restarting it could possibly behave oddly too.\r\n* Mostly tested on synthetic data from `dask.datasets.timeseries`. Real data with uneven distributions and input partition sizes may behave poorly.\r\n* It's slower than it should be and mostly GIL-bound (though probably still faster than a standard task-based shuffle): https://github.com/dask/distributed/issues/5258#issuecomment-914957989, https://github.com/pandas-dev/pandas/issues/43155#issuecomment-915689887\r\n* Remember to pass `shuffle=\"service\"`!\r\n\r\n## Reviews\r\nFor all who are brave enough to review this I would only encourage a high level pass. There are many moving parts and many open TODOs. We're discussing breaking off some parts of the implementation to allow for easier review (or move some parts to dask/distributed). This is still TBD but suggestions are welcome.\r\n\r\n## High level design\r\nThe concurrency model driving this is rather complex and is made of multiple coroutines and threads to deal with grouping, concatenating, sending and receiving data. This process is kicked off in the `transfer` task which is applied on every input partition. This allows computation and network to efficiently overlap. Data is buffered efficiently such that network overhead for small sized data chunks, _shards_, is minimal.\r\n\r\nThe receiving end of these submissions is a small extension on the Worker which accepts incoming data and caches it (on disk, see below) for later processing. The task graph currently employs a barrier task for synchronization and buffer flushing. The output partitions will then be picked up by the `unpack` task which collects the data stored on the given worker and extracts it into a runnable task. From there on, everything is BAU.\r\n\r\nTo enable larger than (cluster) memory dataset shuffles there is an efficient spill to disk implementation which caches all received shards on disk while the shuffle is still running. This is currently not optional. There is currently no persistence hierarchy implemented as is usual for a Worker holding data.\r\n\r\n## References\r\n* https://github.com/dask/dask/issues/6314\r\n* https://github.com/dask/dask/issues/3514\r\n* https://github.com/dask/dask/issues/6164\r\n\r\ncc @mrocklin , @gjoseph92 , @quasiben , @madsbk , ...?",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8223/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8223/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}