[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1251194186",
        "html_url": "https://github.com/dask/dask/issues/9502#issuecomment-1251194186",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9502",
        "id": 1251194186,
        "node_id": "IC_kwDOAbcwm85Kk7VK",
        "user": {
            "login": "jrbourbeau",
            "id": 11656932,
            "node_id": "MDQ6VXNlcjExNjU2OTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jrbourbeau",
            "html_url": "https://github.com/jrbourbeau",
            "followers_url": "https://api.github.com/users/jrbourbeau/followers",
            "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}",
            "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions",
            "organizations_url": "https://api.github.com/users/jrbourbeau/orgs",
            "repos_url": "https://api.github.com/users/jrbourbeau/repos",
            "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jrbourbeau/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-19T15:39:15Z",
        "updated_at": "2022-09-19T15:39:15Z",
        "author_association": "MEMBER",
        "body": "Thanks for raising this issue @LUOXIAO92. It's not clear to me if this is a `dask/dask` issue or a `dask/dask-mpi` issue. Could you post logs / tracebacks? A message like `\"Waiting to connect to Scheduler\"` makes me think this is deployment-related ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1251194186/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1251934588",
        "html_url": "https://github.com/dask/dask/issues/9502#issuecomment-1251934588",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9502",
        "id": 1251934588,
        "node_id": "IC_kwDOAbcwm85KnwF8",
        "user": {
            "login": "LUOXIAO92",
            "id": 89069722,
            "node_id": "MDQ6VXNlcjg5MDY5NzIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/89069722?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LUOXIAO92",
            "html_url": "https://github.com/LUOXIAO92",
            "followers_url": "https://api.github.com/users/LUOXIAO92/followers",
            "following_url": "https://api.github.com/users/LUOXIAO92/following{/other_user}",
            "gists_url": "https://api.github.com/users/LUOXIAO92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LUOXIAO92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LUOXIAO92/subscriptions",
            "organizations_url": "https://api.github.com/users/LUOXIAO92/orgs",
            "repos_url": "https://api.github.com/users/LUOXIAO92/repos",
            "events_url": "https://api.github.com/users/LUOXIAO92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LUOXIAO92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-20T07:11:21Z",
        "updated_at": "2022-09-20T07:11:21Z",
        "author_association": "NONE",
        "body": "> Thanks for raising this issue @LUOXIAO92. It's not clear to me if this is a `dask/dask` issue or a `dask/dask-mpi` issue. Could you post logs / tracebacks? A message like `\"Waiting to connect to Scheduler\"` makes me think this is deployment-related\r\n\r\nThanks, I posted the log at the last.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1251934588/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1253514333",
        "html_url": "https://github.com/dask/dask/issues/9502#issuecomment-1253514333",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9502",
        "id": 1253514333,
        "node_id": "IC_kwDOAbcwm85Ktxxd",
        "user": {
            "login": "LUOXIAO92",
            "id": 89069722,
            "node_id": "MDQ6VXNlcjg5MDY5NzIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/89069722?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LUOXIAO92",
            "html_url": "https://github.com/LUOXIAO92",
            "followers_url": "https://api.github.com/users/LUOXIAO92/followers",
            "following_url": "https://api.github.com/users/LUOXIAO92/following{/other_user}",
            "gists_url": "https://api.github.com/users/LUOXIAO92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LUOXIAO92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LUOXIAO92/subscriptions",
            "organizations_url": "https://api.github.com/users/LUOXIAO92/orgs",
            "repos_url": "https://api.github.com/users/LUOXIAO92/repos",
            "events_url": "https://api.github.com/users/LUOXIAO92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LUOXIAO92/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-21T10:32:38Z",
        "updated_at": "2022-09-21T11:04:23Z",
        "author_association": "NONE",
        "body": "Here is more information:\r\nThis issue occurred at arm64 cluster that the cpus are Fujitsu's A64FX, which is 48 cores. The usable memory is 28 Gbytes per node. \r\nI suspected that maybe only 1 mpi 1 thread for the scheduler is not enough performance to manage the large number of  workers or task graphs, so I tried to use 64 nodes, set -np 128 and 24 threads per worker(the scheduler should be the same setting). But it still did not work. \r\nThen I set -np 64 and 48 threads, it did not work and gave the following massage\r\n<details>\r\n<summary>Out put log:</summary>\r\n\r\n```\r\n...\r\n2022-09-21 02:30:48,470 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-21 02:30:48,470 - distributed.worker - INFO -               Threads:                         48\r\n2022-09-21 02:30:48,470 - distributed.worker - INFO -                Memory:                  26.08 GiB\r\n2022-09-21 02:30:48,470 - distributed.worker - INFO -       Local Directory: /work/03/wo22i012/w17001/test/dask/tmp3/dask-worker-space/worker-4bv91gei\r\n2022-09-21 02:30:48,470 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-21 02:30:48,473 - distributed.worker - INFO -       Start worker at:    tcp://10.10.140.7:44529\r\n2022-09-21 02:30:48,473 - distributed.worker - INFO -          Listening to:    tcp://10.10.140.7:44529\r\n2022-09-21 02:30:48,473 - distributed.worker - INFO -           Worker name:                         61\r\n2022-09-21 02:30:48,474 - distributed.worker - INFO -          dashboard at:          10.10.140.7:33151\r\n2022-09-21 02:30:48,474 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:30:48,474 - distributed.worker - INFO - -------------------------------------------------\r\n2022-09-21 02:30:48,474 - distributed.worker - INFO -               Threads:                         48\r\n2022-09-21 02:30:48,474 - distributed.worker - INFO -                Memory:                  26.08 GiB\r\n2022-09-21 02:30:48,474 - distributed.worker - INFO -       Local Directory: /work/03/wo22i012/w17001/test/dask/tmp3/dask-worker-space/worker-xia3owst\r\n2022-09-21 02:30:48,475 - distributed.worker - INFO - -------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 491, in connect\r\n    stream = await self.client.connect(\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/tornado/tcpclient.py\", line 275, in connect\r\n    af, addr, stream = await connector.start(connect_timeout=timeout)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/asyncio/tasks.py\", line 490, in wait_for\r\n    return fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/comm/core.py\", line 291, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/asyncio/tasks.py\", line 492, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/work/03/wo22i012/w17001/test/dask/dasktest2.py\", line 20, in <module>\r\n    client = Client()\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/client.py\", line 943, in __init__\r\n    self.start(timeout=timeout)\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/client.py\", line 1133, in start\r\n    sync(self.loop, self._start, **kwargs)\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/utils.py\", line 406, in sync\r\n    raise exc.with_traceback(tb)\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/utils.py\", line 379, in f\r\n    result = yield future\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/tornado/gen.py\", line 762, in run\r\n    value = future.result()\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/client.py\", line 1213, in _start\r\n    await self._ensure_connected(timeout=timeout)\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/client.py\", line 1276, in _ensure_connected\r\n    comm = await connect(\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/comm/core.py\", line 317, in connect\r\n    raise OSError(\r\nOSError: Timed out trying to connect to tcp://10.10.108.79:42453 after 30 s\r\n2022-09-21 02:31:18,386 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,386 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,389 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,390 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,393 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,396 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,400 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,401 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,403 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,407 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,408 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,408 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,409 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,410 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,410 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,412 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,415 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,416 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,419 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,419 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,423 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,423 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,424 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,427 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,428 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,431 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,432 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,434 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,436 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,436 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,438 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,438 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,439 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,445 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,445 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,446 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,448 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,452 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,452 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,452 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,454 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,456 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,457 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,458 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,459 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,466 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,466 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,467 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,467 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,467 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,475 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,476 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,480 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,481 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,482 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,482 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,483 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,485 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,486 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,487 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,492 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:18,499 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 491, in connect\r\n    stream = await self.client.connect(\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/tornado/tcpclient.py\", line 275, in connect\r\n    af, addr, stream = await connector.start(connect_timeout=timeout)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/asyncio/tasks.py\", line 490, in wait_for\r\n    return fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/comm/core.py\", line 291, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/asyncio/tasks.py\", line 492, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/utils.py\", line 379, in f\r\n    result = yield future\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/tornado/gen.py\", line 762, in run\r\n    value = future.result()\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/client.py\", line 1213, in _start\r\n    await self._ensure_connected(timeout=timeout)\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/client.py\", line 1276, in _ensure_connected\r\n    comm = await connect(\r\n  File \"/work/wo22i012/w17001/.conda/envs/py3.9-dask/lib/python3.9/site-packages/distributed/comm/core.py\", line 317, in connect\r\n    raise OSError(\r\nOSError: Timed out trying to connect to tcp://10.10.108.79:42453 after 30 s\r\n2022-09-21 02:31:48,506 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:48,508 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n2022-09-21 02:31:48,509 - distributed.worker - INFO - Waiting to connect to:   tcp://10.10.108.79:42453\r\n...\r\n```\r\n</details>\r\n\r\nHowever, the same code can be computed on the x86 64 clusters, which's cpu are Xeon Gold 6126 x 2, and have 192 Gbytes memory per node. It can compute up to Dcut=16 by using 8 nodes.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1253514333/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]