{
    "url": "https://api.github.com/repos/dask/dask/issues/3487",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/3487/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/3487/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/3487/events",
    "html_url": "https://github.com/dask/dask/issues/3487",
    "id": 322005830,
    "node_id": "MDU6SXNzdWUzMjIwMDU4MzA=",
    "number": 3487,
    "title": "Storing multiple files of data",
    "user": {
        "login": "jakirkham",
        "id": 3019665,
        "node_id": "MDQ6VXNlcjMwMTk2NjU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3019665?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jakirkham",
        "html_url": "https://github.com/jakirkham",
        "followers_url": "https://api.github.com/users/jakirkham/followers",
        "following_url": "https://api.github.com/users/jakirkham/following{/other_user}",
        "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions",
        "organizations_url": "https://api.github.com/users/jakirkham/orgs",
        "repos_url": "https://api.github.com/users/jakirkham/repos",
        "events_url": "https://api.github.com/users/jakirkham/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jakirkham/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862305,
            "node_id": "MDU6TGFiZWwyNDI4NjIzMDU=",
            "url": "https://api.github.com/repos/dask/dask/labels/array",
            "name": "array",
            "color": "006b75",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 11,
    "created_at": "2018-05-10T17:01:30Z",
    "updated_at": "2021-10-12T05:30:28Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "Thinking about this primarily in terms of Dask Arrays (after discussion with a colleague), but maybe this applies elsewhere as well.\r\n\r\nA typical use case we encounter is a user has a directory full of files (usually TIFFs, but sometimes other things). They would like to load this data into a single Dask Array, which they would like to perform some operations on. At the end of this, they would like to spit out another directory of files (again usually TIFFs, but sometimes other things).\r\n\r\nThe current recommendation with loading is a user can call functions wrapped with `delayed` and convert those into arrays using known metadata, which they can then run `stack` and `concatenate` to their hearts content. A nice example of this procedure is in issue ( https://github.com/dask/dask/issues/672 ). Though hopefully these days users just use [`block`]( http://dask.pydata.org/en/latest/array-api.html#dask.array.block ). There are some things like [`dask-imread`]( https://github.com/dask-image/dask-imread ) that do this for users. When it finally comes to storing data, the user is encouraged to use [`store`]( http://dask.pydata.org/en/latest/array-api.html#dask.array.store ), which works fine for things that support a NumPy-like interface.\r\n\r\nHowever when it comes to storing Dask Arrays, there are only a handful of things that could store a large array via a NumPy-like interface in the way that Dask would like to. If the storage format you have in mind (normally determined by what else it needs to be used with) does not fit this interface, you're stuck. In particular some cases are better resolved by writing out a bunch of small files (in our case TIFFs work well). Certainly one could home brew a NumPy-like interface that allows this, but this seems unnecessary since the chunks that Dask Array already operates on are perfect candidates for being mapped one-to-one to files on disk. With such a mapping one is also guaranteed parallel writes, which is desirable combination with Dask. In fact users are already hacking things like `map_blocks` to get this behavior (even though that is not their intend use).\r\n\r\nSo how could we make this better for users. One thought would be an operation to convert Dask Array chunks into proper Dask Arrays as envisioned in issue ( https://github.com/dask/dask/issues/3274 ). This would allow one to use `store` semantics with chunks and individual files. Though there would still be some home brewing to use the NumPy API for each chunk. Not to mention users would need to come up with a naming scheme. Another thought would be to simply implement an improved `map_blocks` interface for this intent called `store_blocks`. This would handle optimization of a Dask Array and take a function that performs the storage operation. The function could take a `block_id`/`chunk_id` parameter to help with file naming. There may be other options I have not thought of that would work as well. Feedback welcome. :)",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/3487/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/3487/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}