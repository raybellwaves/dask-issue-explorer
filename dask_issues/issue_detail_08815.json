{
    "url": "https://api.github.com/repos/dask/dask/issues/8815",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8815/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8815/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8815/events",
    "html_url": "https://github.com/dask/dask/issues/8815",
    "id": 1170152453,
    "node_id": "I_kwDOAbcwm85FvxwF",
    "number": 8815,
    "title": "`test_partition_on_duplicates` failure on windows python3.8 CI",
    "user": {
        "login": "jsignell",
        "id": 4806877,
        "node_id": "MDQ6VXNlcjQ4MDY4Nzc=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4806877?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jsignell",
        "html_url": "https://github.com/jsignell",
        "followers_url": "https://api.github.com/users/jsignell/followers",
        "following_url": "https://api.github.com/users/jsignell/following{/other_user}",
        "gists_url": "https://api.github.com/users/jsignell/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jsignell/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jsignell/subscriptions",
        "organizations_url": "https://api.github.com/users/jsignell/orgs",
        "repos_url": "https://api.github.com/users/jsignell/repos",
        "events_url": "https://api.github.com/users/jsignell/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jsignell/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 1887344368,
            "node_id": "MDU6TGFiZWwxODg3MzQ0MzY4",
            "url": "https://api.github.com/repos/dask/dask/labels/tests",
            "name": "tests",
            "color": "a0f9b4",
            "default": false,
            "description": "Unit tests and/or continuous integration"
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2022-03-15T20:00:49Z",
    "updated_at": "2022-04-18T02:12:04Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "Just noticed this in CI so I wanted to document it. Mild ping to @martindurant in case something has changed on the fastparquet side. \r\n\r\n```python-traceback\r\n__________________ test_partition_on_duplicates[fastparquet] __________________\r\n[gw3] win32 -- Python 3.8.12 C:\\Miniconda3\\envs\\test-environment\\python.exe\r\n\r\ntmpdir = 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-runneradmin\\\\pytest-0\\\\popen-gw3\\\\test_partition_on_duplicates_f0'\r\nengine = 'fastparquet'\r\n\r\n    def test_partition_on_duplicates(tmpdir, engine):\r\n        # https://github.com/dask/dask/issues/6445\r\n        tmpdir = str(tmpdir)\r\n        df = pd.DataFrame(\r\n            {\r\n                \"a1\": np.random.choice([\"A\", \"B\", \"C\"], size=100),\r\n                \"a2\": np.random.choice([\"X\", \"Y\", \"Z\"], size=100),\r\n                \"data\": np.random.random(size=100),\r\n            }\r\n        )\r\n        d = dd.from_pandas(df, npartitions=2)\r\n    \r\n        for _ in range(2):\r\n            d.to_parquet(tmpdir, partition_on=[\"a1\", \"a2\"], engine=engine)\r\n    \r\n>       out = dd.read_parquet(tmpdir, engine=engine).compute()\r\n\r\ndask\\dataframe\\io\\tests\\test_parquet.py:1298: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ndask\\base.py:292: in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\ndask\\base.py:575: in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\ndask\\threaded.py:81: in get\r\n    results = get_async(\r\ndask\\local.py:508: in get_async\r\n    raise_exception(exc, tb)\r\ndask\\local.py:316: in reraise\r\n    raise exc\r\ndask\\local.py:221: in execute_task\r\n    result = _execute_task(task, data)\r\ndask\\core.py:119: in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\ndask\\optimization.py:990: in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\ndask\\core.py:149: in get\r\n    result = _execute_task(task, cache)\r\ndask\\core.py:119: in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\ndask\\dataframe\\io\\parquet\\core.py:90: in __call__\r\n    return read_parquet_part(\r\ndask\\dataframe\\io\\parquet\\core.py:498: in read_parquet_part\r\n    dfs = [\r\ndask\\dataframe\\io\\parquet\\core.py:499: in <listcomp>\r\n    func(fs, rg, columns.copy(), index, **toolz.merge(kwargs, kw))\r\ndask\\dataframe\\io\\parquet\\fastparquet.py:1027: in read_partition\r\n    return cls.pf_to_pandas(\r\ndask\\dataframe\\io\\parquet\\fastparquet.py:1119: in pf_to_pandas\r\n    pf.read_row_group_file(\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\api.py:336: in read_row_group_file\r\n    core.read_row_group(\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:608: in read_row_group\r\n    read_row_group_arrays(file, rg, columns, categories, schema_helper,\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:580: in read_row_group_arrays\r\n    read_col(column, schema_helper, file, use_cat=name+'-catdef' in out,\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\core.py:435: in read_col\r\n    se = schema_helper.schema_element(cmd.path_in_schema)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <Parquet Schema with 3 entries>, name = ['__null_dask_index__']\r\n\r\n    def schema_element(self, name):\r\n        \"\"\"Get the schema element with the given name or path\"\"\"\r\n        root = self.root\r\n        if isinstance(name, str):\r\n            name = name.split('.')\r\n        for part in name:\r\n>           root = root[\"children\"][part]\r\nE           KeyError: '__null_dask_index__'\r\n\r\nC:\\Miniconda3\\envs\\test-environment\\lib\\site-packages\\fastparquet\\schema.py:119: KeyError\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8815/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8815/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}