[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1553463348",
        "html_url": "https://github.com/dask/dask/issues/10298#issuecomment-1553463348",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10298",
        "id": 1553463348,
        "node_id": "IC_kwDOAbcwm85cl_g0",
        "user": {
            "login": "ianozsvald",
            "id": 273210,
            "node_id": "MDQ6VXNlcjI3MzIxMA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/273210?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ianozsvald",
            "html_url": "https://github.com/ianozsvald",
            "followers_url": "https://api.github.com/users/ianozsvald/followers",
            "following_url": "https://api.github.com/users/ianozsvald/following{/other_user}",
            "gists_url": "https://api.github.com/users/ianozsvald/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ianozsvald/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ianozsvald/subscriptions",
            "organizations_url": "https://api.github.com/users/ianozsvald/orgs",
            "repos_url": "https://api.github.com/users/ianozsvald/repos",
            "events_url": "https://api.github.com/users/ianozsvald/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ianozsvald/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-18T18:32:05Z",
        "updated_at": "2023-05-18T18:32:05Z",
        "author_association": "NONE",
        "body": "This is a drive-by suggestion, please forgive if it doesn't get you closer. I'm not a maintainer, just a long time user so I'm trying to suggest some ideas.\r\n\r\nFirst I'd remove the `re` `cell` code to make the job in both Pandas and Dask as simple as possible (just in case the `findall` line isn't doing the job you expect - it is written a little differently between Pandas and the Dask version). Maybe also set `include_path_column=False` as that's not present in the Pandas demo.\r\n\r\nIf you still get the same behaviour I'd read about the new less-eager scheduler https://www.coiled.io/blog/reducing-dask-memory-usage as maybe the many-small-files are being eagerly loaded (which won't happen in your serial case) and that's overwhelming the scheduler? That came in Dask version 2022.11.\r\n\r\nIt might also help to add an example of the data files, perhaps with a bit of code that auto-generates equivalent fake zip files, such that the issue could be reproduced. Take a read in http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports to help follow the community norms for a bug report - it would help to confirm which version of Dask you're using and which platform you're on.\r\n\r\nGood luck!",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1553463348/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1554298653",
        "html_url": "https://github.com/dask/dask/issues/10298#issuecomment-1554298653",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10298",
        "id": 1554298653,
        "node_id": "IC_kwDOAbcwm85cpLcd",
        "user": {
            "login": "guidocioni",
            "id": 12760310,
            "node_id": "MDQ6VXNlcjEyNzYwMzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/12760310?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guidocioni",
            "html_url": "https://github.com/guidocioni",
            "followers_url": "https://api.github.com/users/guidocioni/followers",
            "following_url": "https://api.github.com/users/guidocioni/following{/other_user}",
            "gists_url": "https://api.github.com/users/guidocioni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guidocioni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guidocioni/subscriptions",
            "organizations_url": "https://api.github.com/users/guidocioni/orgs",
            "repos_url": "https://api.github.com/users/guidocioni/repos",
            "events_url": "https://api.github.com/users/guidocioni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guidocioni/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-19T09:30:48Z",
        "updated_at": "2023-05-19T09:30:48Z",
        "author_association": "NONE",
        "body": "> This is a drive-by suggestion, please forgive if it doesn't get you closer. I'm not a maintainer, just a long time user so I'm trying to suggest some ideas.\r\n> \r\n> First I'd remove the `re` `cell` code to make the job in both Pandas and Dask as simple as possible (just in case the `findall` line isn't doing the job you expect - it is written a little differently between Pandas and the Dask version). Maybe also set `include_path_column=False` as that's not present in the Pandas demo.\r\n\r\nThis is the point: in Dask the way of working is completely different. I'm not going to apply regex to a chunk of the file but rather let it do it for me (which is what the `findall` method should do)..Then this should be automatically applied in parallel on the chunks of data. I cannot really make the same exact example In Dask and pandas as the concept is different (otherwise it wouldn't make sense to use Dask in the first place \ud83d\ude1b).\r\n\r\n> \r\n> If you still get the same behaviour I'd read about the new less-eager scheduler https://www.coiled.io/blog/reducing-dask-memory-usage as maybe the many-small-files are being eagerly loaded (which won't happen in your serial case) and that's overwhelming the scheduler? That came in Dask version 2022.11.\r\n\r\nEven if they are, with 4 workers I shouldn't get more than 4 files open at the same time, which definitely does not justify the large memory footprint.\r\n\r\n> \r\n> It might also help to add an example of the data files, perhaps with a bit of code that auto-generates equivalent fake zip files, such that the issue could be reproduced. Take a read in http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports to help follow the community norms for a bug report - it would help to confirm which version of Dask you're using and which platform you're on.\r\n> \r\n\r\nI'll try to but I have the feeling the problem here is my method \r\n\r\n\r\n> Good luck!\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1554298653/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1556793986",
        "html_url": "https://github.com/dask/dask/issues/10298#issuecomment-1556793986",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/10298",
        "id": 1556793986,
        "node_id": "IC_kwDOAbcwm85cysqC",
        "user": {
            "login": "guidocioni",
            "id": 12760310,
            "node_id": "MDQ6VXNlcjEyNzYwMzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/12760310?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guidocioni",
            "html_url": "https://github.com/guidocioni",
            "followers_url": "https://api.github.com/users/guidocioni/followers",
            "following_url": "https://api.github.com/users/guidocioni/following{/other_user}",
            "gists_url": "https://api.github.com/users/guidocioni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guidocioni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guidocioni/subscriptions",
            "organizations_url": "https://api.github.com/users/guidocioni/orgs",
            "repos_url": "https://api.github.com/users/guidocioni/repos",
            "events_url": "https://api.github.com/users/guidocioni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guidocioni/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-22T08:42:02Z",
        "updated_at": "2023-05-22T08:42:02Z",
        "author_association": "NONE",
        "body": "Apparently the problem is related to `include_path_column`. If I remove that then the execution time and resource usage is comparable to the one of the pandas serial version (although it should be faster as I'm using multiple workers but hey oh...).\r\n\r\nAnyone have any idea why using `include_path_column` slows everything down and causes excessive use of resources? ",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1556793986/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]