{
    "url": "https://api.github.com/repos/dask/dask/issues/9164",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9164/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9164/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9164/events",
    "html_url": "https://github.com/dask/dask/issues/9164",
    "id": 1261955716,
    "node_id": "I_kwDOAbcwm85LN-qE",
    "number": 9164,
    "title": "Persistent caching",
    "user": {
        "login": "multimeric",
        "id": 5019367,
        "node_id": "MDQ6VXNlcjUwMTkzNjc=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5019367?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/multimeric",
        "html_url": "https://github.com/multimeric",
        "followers_url": "https://api.github.com/users/multimeric/followers",
        "following_url": "https://api.github.com/users/multimeric/following{/other_user}",
        "gists_url": "https://api.github.com/users/multimeric/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/multimeric/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/multimeric/subscriptions",
        "organizations_url": "https://api.github.com/users/multimeric/orgs",
        "repos_url": "https://api.github.com/users/multimeric/repos",
        "events_url": "https://api.github.com/users/multimeric/events{/privacy}",
        "received_events_url": "https://api.github.com/users/multimeric/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        },
        {
            "id": 3798450420,
            "node_id": "LA_kwDOAbcwm87iZ8D0",
            "url": "https://api.github.com/repos/dask/dask/labels/feature",
            "name": "feature",
            "color": "b0f0fa",
            "default": false,
            "description": "Something is missing"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2022-06-06T14:51:23Z",
    "updated_at": "2022-07-11T02:13:41Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "I'm interesting in writing distributed code that runs using Dask, but that checkpoints at certain points, so that it can resume from these points in case of a failure. This is distinct from the [opportunistic caching](https://docs.dask.org/en/stable/caching.html) already built into dask, which doesn't persist between restarts of the scheduler.\r\n\r\n[Previously I talked about this on Twitter](https://twitter.com/dask_dev/status/1442866143073427465) and there was mention from the dev team that this might be out of scope as Dask is trying to stay low-level, but if we pushed the caching layer down to dask:\r\n\r\n* Users don't have to bring in heavyweight frameworks like Prefect when they don't need them\r\n* We can use specific insight into the dask graph to implement this caching in an efficient and clean way\r\n* We can leverage the existing filesystem APIs provided by task (e.g. writing to S3 or local disk)\r\n* Users don't have to define functions or methods (as in Prefect) to get access to caching, which makes the code cleaner and much nicer to use in notebooks\r\n\r\nTo provide a concrete example, let's say I want to do some very heavy processing on a `dask.DataFrame` `ddf`. In the below example, if the second `apply()` fails here, at least I have the intermediate result from the first `apply()`, but dask doesn't know that it can just load this parquet file instead of re-running the first `apply()`:\r\n\r\n```python\r\nddf.apply(...).to_parquet(\"foo\").apply(...)\r\n```\r\n\r\nSo I could use some conditional logic, but this would get very tedious:\r\n\r\n```python\r\nif os.path.exists(\"foo\"):\r\n    ddf = dd.read_parquet(\"foo\")\r\nelse:\r\n    ddf = ddf.apply(...)\r\n\r\nddf.apply(...)\r\n```\r\n\r\nI could refactor this logic into a decorator (as Prefect has broadly done), and this is much better, but as a user I would rather that this is a stable, public API offered by Dask rather than something I have to write:\r\n```python\r\n@dask.cache(...)\r\ndef task_a(ddf):\r\n    return ddf.apply(...)\r\n\r\ntask_b(task_a(ddf))\r\n```\r\n\r\nUnfortunately, the above code is pretty ugly (needing to define a new function, and breaking the method chain), so at this point I'd like to suggest a nice checkpointing API that can be added to all futures (doesn't have to just be DataFrames or even collections):\r\n```python\r\nddf.apply(...).checkpoint(\"foo\", ...).apply(...).checkpoint(\"bar\")\r\n```\r\n\r\nSince Dask is lazy, this works well, since it doesn't actually compute anything at the time of being called, but rather it would insert the conditional caching logic into the DAG. I'm not sure if it would manipulate the DAG at call time or at compute time (I would need a bit of help here) but either way it should produce a DAG like this: (simplified to treat Apply as a single node):\r\n\r\n```mermaid\r\ngraph TD;\r\n    A{Does bar exist?}-- Yes -->B[Read Parquet] --> End;\r\n    C{Does foo exist?}-- Yes -->D[Read Parquet] --> I;\r\n    A-- No --> C;\r\n    C-- No -->G[\"Apply (1)\"] --> H[Save Parquet];\r\n    H --> I[\"Apply (2)\"] --> J[Save Parquet] --> End;\r\n```\r\n\r\nSo my questions are:\r\n* Is there any interest in having persistent caching in Dask in general?\r\n* If yes, is there interest in my proposed API?\r\n\r\nI think I have some time to dedicate to this, if there is interest in such a feature. But of course I wanted to run it by the maintainers first, since it may still be considered out of scope.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9164/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9164/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}