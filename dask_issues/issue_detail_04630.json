{
    "url": "https://api.github.com/repos/dask/dask/issues/4630",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/4630/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/4630/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/4630/events",
    "html_url": "https://github.com/dask/dask/issues/4630",
    "id": 424945678,
    "node_id": "MDU6SXNzdWU0MjQ5NDU2Nzg=",
    "number": 4630,
    "title": "Forget history and task overhead investigation",
    "user": {
        "login": "rabernat",
        "id": 1197350,
        "node_id": "MDQ6VXNlcjExOTczNTA=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1197350?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rabernat",
        "html_url": "https://github.com/rabernat",
        "followers_url": "https://api.github.com/users/rabernat/followers",
        "following_url": "https://api.github.com/users/rabernat/following{/other_user}",
        "gists_url": "https://api.github.com/users/rabernat/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rabernat/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rabernat/subscriptions",
        "organizations_url": "https://api.github.com/users/rabernat/orgs",
        "repos_url": "https://api.github.com/users/rabernat/repos",
        "events_url": "https://api.github.com/users/rabernat/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rabernat/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 386719400,
            "node_id": "MDU6TGFiZWwzODY3MTk0MDA=",
            "url": "https://api.github.com/repos/dask/dask/labels/scheduler",
            "name": "scheduler",
            "color": "D10945",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 22,
    "created_at": "2019-03-25T14:52:19Z",
    "updated_at": "2021-10-12T08:15:19Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "Another issue about dask performance and optimization, slightly related to #107.\r\n\r\nI frequently end up creating dask graphs with 1M+ tasks. Graphs this big cause the scheduler to start to choke. One idea I have had to mitigate this is to call `.persist()` on some intermediate results. I would  essentially like to save some results in the memory of my dask cluster, and then do further computations on this data.\r\n\r\nHowever, the problem is that `.persist()` doesn't seem to reduce the number of tasks in the scheduler's memory. Even though the data I need are all memory on the worker nodes, I can't erase the expensive task history.\r\n\r\nSpecifically I would like to do something like this\r\n```python\r\nraw_data = load_data_from_storage(...)\r\nintermediate_result = big_function_that_creates_lots_of_tasks(raw_data)\r\nintermediate_result.persist(forget=True)\r\n# intermediate result now only has 1 task per chunk\r\nfinal_result = downstream_function(intermediate_result)\r\n```\r\n\r\nAn alternative way to phrase this is that I would like to use the dask cluster as an in-memory distributed storage object, so perhaps there is a different way to achieve the same result.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/4630/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/4630/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}