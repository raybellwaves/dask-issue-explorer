{
    "url": "https://api.github.com/repos/dask/dask/issues/11067",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/11067/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/11067/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/11067/events",
    "html_url": "https://github.com/dask/dask/issues/11067",
    "id": 2263044231,
    "node_id": "I_kwDOAbcwm86G41CH",
    "number": 11067,
    "title": "Control order of execution with query planner",
    "user": {
        "login": "jtilly",
        "id": 6807275,
        "node_id": "MDQ6VXNlcjY4MDcyNzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6807275?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jtilly",
        "html_url": "https://github.com/jtilly",
        "followers_url": "https://api.github.com/users/jtilly/followers",
        "following_url": "https://api.github.com/users/jtilly/following{/other_user}",
        "gists_url": "https://api.github.com/users/jtilly/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jtilly/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jtilly/subscriptions",
        "organizations_url": "https://api.github.com/users/jtilly/orgs",
        "repos_url": "https://api.github.com/users/jtilly/repos",
        "events_url": "https://api.github.com/users/jtilly/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jtilly/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3880424463,
            "node_id": "LA_kwDOAbcwm87nSpQP",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20triage",
            "name": "needs triage",
            "color": "eeeeee",
            "default": false,
            "description": "Needs a response from a contributor"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2024-04-25T08:51:31Z",
    "updated_at": "2024-04-29T12:13:58Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "**Describe the issue**:\r\n\r\nWe often use dask together with [plateau](https://github.com/data-engineering-collective/plateau) to perform the following task in parallel:\r\n- load a chunk of data\r\n- do a transformation\r\n- write the chunk of data to disk\r\n\r\nWe typically do this with datasets that don't fit into our machine's RAM. This used to work fine, but with the new query planner, we typically run out of memory, because dask now loads _all_ the data before doing a transformation. Before, dask would operate chunk by chunk.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nI tried to come up with a simple example that doesn't involve plateau (or a lot of data):\r\n\r\n```python\r\nimport dask\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\n\r\nfrom dask.distributed import Client, LocalCluster\r\n\r\n# load the data from disk\r\n@dask.delayed\r\ndef _load(x):\r\n    print(f\"Loading chunk {x}\")\r\n    return pd.DataFrame({\"x\": x, \"y\": [1, 2, 3]})\r\n\r\n# transform and store\r\ndef _transform_and_store(df):\r\n    # this example doesn't transform or store anything, but you get the idea\r\n    x = df[\"x\"].unique().item()\r\n    print(f\"Storing chunk {x}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    ddf = dd.from_delayed([_load(x) for x in range(10)], meta={\"x\": \"int64\", \"y\": \"int64\"})\r\n\r\n    with LocalCluster(n_workers=1, threads_per_worker=1) as cluster:\r\n        with Client(cluster):\r\n            ddf.map_partitions(_transform_and_store, meta={}).compute()\r\n```\r\n\r\nThe behavior without the query planner is:\r\n```\r\nDASK_DATAFRAME__QUERY_PLANNING=False python example.py \r\nLoading chunk 9\r\nStoring chunk 9\r\nLoading chunk 8\r\nStoring chunk 8\r\nLoading chunk 7\r\nStoring chunk 7\r\nLoading chunk 6\r\nStoring chunk 6\r\nLoading chunk 5\r\nStoring chunk 5\r\nLoading chunk 4\r\nStoring chunk 4\r\nLoading chunk 3\r\nStoring chunk 3\r\nLoading chunk 2\r\nStoring chunk 2\r\nLoading chunk 1\r\nStoring chunk 1\r\nLoading chunk 0\r\nStoring chunk 0\r\n```\r\n\r\nSo, we load, transform and store and won't run out of memory.\r\n\r\n```\r\nDASK_DATAFRAME__QUERY_PLANNING=True python example.py \r\nLoading chunk 7\r\nLoading chunk 2\r\nLoading chunk 3\r\nLoading chunk 0\r\nLoading chunk 9\r\nLoading chunk 1\r\nLoading chunk 8\r\nLoading chunk 5\r\nLoading chunk 4\r\nLoading chunk 6\r\nStoring chunk 9\r\nStoring chunk 8\r\nStoring chunk 7\r\nStoring chunk 6\r\nStoring chunk 5\r\nStoring chunk 4\r\nStoring chunk 3\r\nStoring chunk 2\r\nStoring chunk 1\r\nStoring chunk 0\r\n```\r\n\r\nHere, we first load everything and so we'll run out of memory.\r\n\r\n\r\n- Is there a way to tell the query planner to operate chunk-by-chunk when it encounters `map_partitions`?\r\n- Should I not be using `map_partitions` for something like this?\r\n- NB: `DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=1` doesn't help\r\n\r\n\r\n**Anything else that we should know?**\r\n\r\nNote that it's really tricky to turn off the query planner (other than through environment variables). When using `dask.config` from within Python, it's important that the config is set before the first import of `dask.dataframe`, which is difficult to control.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.4.2\r\n- Python version: 3.12.3\r\n- Operating System: linux\r\n- Install method (conda, pip, source): conda\r\n\r\n\r\nThank you!",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/11067/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/11067/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}