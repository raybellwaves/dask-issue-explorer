{
    "url": "https://api.github.com/repos/dask/dask/issues/9034",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9034/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9034/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9034/events",
    "html_url": "https://github.com/dask/dask/issues/9034",
    "id": 1226838397,
    "node_id": "I_kwDOAbcwm85JIBF9",
    "number": 9034,
    "title": "Handle empty dataframes and/or mismatched columns in `dd.from_delayed`",
    "user": {
        "login": "phobson",
        "id": 1163939,
        "node_id": "MDQ6VXNlcjExNjM5Mzk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1163939?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/phobson",
        "html_url": "https://github.com/phobson",
        "followers_url": "https://api.github.com/users/phobson/followers",
        "following_url": "https://api.github.com/users/phobson/following{/other_user}",
        "gists_url": "https://api.github.com/users/phobson/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/phobson/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/phobson/subscriptions",
        "organizations_url": "https://api.github.com/users/phobson/orgs",
        "repos_url": "https://api.github.com/users/phobson/repos",
        "events_url": "https://api.github.com/users/phobson/events{/privacy}",
        "received_events_url": "https://api.github.com/users/phobson/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 996497175,
            "node_id": "MDU6TGFiZWw5OTY0OTcxNzU=",
            "url": "https://api.github.com/repos/dask/dask/labels/good%20second%20issue",
            "name": "good second issue",
            "color": "5319e7",
            "default": false,
            "description": "Clearly described, educational, but less trivial than \"good first issue\"."
        },
        {
            "id": 3798450422,
            "node_id": "LA_kwDOAbcwm87iZ8D2",
            "url": "https://api.github.com/repos/dask/dask/labels/p3",
            "name": "p3",
            "color": "ffff33",
            "default": false,
            "description": "Affects a small number of users or is largely cosmetic"
        },
        {
            "id": 3798602129,
            "node_id": "LA_kwDOAbcwm87iahGR",
            "url": "https://api.github.com/repos/dask/dask/labels/enhancement",
            "name": "enhancement",
            "color": "C2E0C6",
            "default": true,
            "description": "Improve existing functionality or make things work better"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2022-05-05T15:32:52Z",
    "updated_at": "2022-08-25T17:27:13Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "I notice that if I'm looping through several arguments to a Dask-delayed function and building a dataframe, I get an error when the dataframe is empty or if the columns are slightly different.\r\n\r\nThe following demonstrates the two cases:\r\n\r\n```python\r\nimport pandas\r\nfrom dask import delayed\r\nimport dask.dataframe as dd\r\n\r\n\r\n@delayed\r\ndef make_dataframe_with_empty(n):\r\n    if n != 5:\r\n        return pandas.DataFrame({\"a\": [n] * 2, \"b\": [n*2] * 2})\r\n    else:\r\n        return pandas.DataFrame()\r\n    \r\n    \r\nddf1 = dd.from_delayed([make_dataframe_with_empty(n) for n in range(10)])\r\nddf1.compute()\r\n```\r\n\r\nWhich makes:\r\n\r\n<details>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [5], in <module>\r\n     11         return pandas.DataFrame()\r\n     14 ddf1 = dd.from_delayed([make_dataframe_with_empty(n) for n in range(10)])\r\n---> 15 ddf1.compute()\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/base.py:290, in DaskMethodsMixin.compute(self, **kwargs)\r\n    266 def compute(self, **kwargs):\r\n    267     \"\"\"Compute this dask collection\r\n    268 \r\n    269     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    288     dask.base.compute\r\n    289     \"\"\"\r\n--> 290     (result,) = compute(self, traverse=False, **kwargs)\r\n    291     return result\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/base.py:573, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    570     keys.append(x.__dask_keys__())\r\n    571     postcomputes.append(x.__dask_postcompute__())\r\n--> 573 results = schedule(dsk, keys, **kwargs)\r\n    574 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/threaded.py:81, in get(dsk, result, cache, num_workers, pool, **kwargs)\r\n     78     elif isinstance(pool, multiprocessing.pool.Pool):\r\n     79         pool = MultiprocessingPoolExecutor(pool)\r\n---> 81 results = get_async(\r\n     82     pool.submit,\r\n     83     pool._max_workers,\r\n     84     dsk,\r\n     85     result,\r\n     86     cache=cache,\r\n     87     get_id=_thread_get_id,\r\n     88     pack_exception=pack_exception,\r\n     89     **kwargs,\r\n     90 )\r\n     92 # Cleanup pools associated to dead threads\r\n     93 with pools_lock:\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/local.py:506, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\r\n    504         _execute_task(task, data)  # Re-execute locally\r\n    505     else:\r\n--> 506         raise_exception(exc, tb)\r\n    507 res, worker_id = loads(res_info)\r\n    508 state[\"cache\"][key] = res\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/local.py:314, in reraise(exc, tb)\r\n    312 if exc.__traceback__ is not tb:\r\n    313     raise exc.with_traceback(tb)\r\n--> 314 raise exc\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/local.py:219, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\r\n    217 try:\r\n    218     task, data = loads(task_info)\r\n--> 219     result = _execute_task(task, data)\r\n    220     id = get_id()\r\n    221     result = dumps((result, id))\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)\r\n    115     func, args = arg[0], arg[1:]\r\n    116     # Note: Don't assign the subtask results to a variable. numpy detects\r\n    117     # temporaries by their reference count and can execute certain\r\n    118     # operations in-place.\r\n--> 119     return func(*(_execute_task(a, cache) for a in args))\r\n    120 elif not ishashable(arg):\r\n    121     return arg\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/dataframe/utils.py:407, in check_meta(x, meta, funcname, numeric_equal)\r\n    401         return x\r\n    402     errmsg = \"Partition type: `{}`\\n{}\".format(\r\n    403         typename(type(meta)),\r\n    404         asciitable([\"\", \"dtype\"], [(\"Found\", x.dtype), (\"Expected\", meta.dtype)]),\r\n    405     )\r\n--> 407 raise ValueError(\r\n    408     \"Metadata mismatch found%s.\\n\\n\"\r\n    409     \"%s\" % ((\" in `%s`\" % funcname if funcname else \"\"), errmsg)\r\n    410 )\r\n\r\n```\r\n\r\n</details>\r\n\r\n```\r\nValueError: Metadata mismatch found in `from_delayed`.\r\n\r\nPartition type: `pandas.core.frame.DataFrame`\r\n+--------+-------+----------+\r\n| Column | Found | Expected |\r\n+--------+-------+----------+\r\n| 'a'    | -     | int64    |\r\n| 'b'    | -     | int64    |\r\n+--------+-------+----------+\r\n\r\n```\r\n\r\nAnd then:\r\n\r\n```python\r\n@delayed\r\ndef make_dataframe_diff_col_order(n):\r\n    df = pandas.DataFrame({\"a\": [n] * 2, \"b\": [n*2] * 2})\r\n    \r\n    if n == 5:\r\n        df = df.loc[:, [\"b\", \"a\"]]\r\n    \r\n    return df\r\n    \r\n\r\nddf2 = dd.from_delayed([make_dataframe_diff_col_order(n) for n in range(10)])\r\nddf2.compute()\r\n```\r\n\r\n<details>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [3], in <module>\r\n     11     return df\r\n     13 ddf2 = dd.from_delayed([\r\n     14       make_dataframe_diff_col_order(n) for n in range(10)\r\n     15 ])\r\n---> 17 ddf2.compute()\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/base.py:290, in DaskMethodsMixin.compute(self, **kwargs)\r\n    266 def compute(self, **kwargs):\r\n    267     \"\"\"Compute this dask collection\r\n    268 \r\n    269     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    288     dask.base.compute\r\n    289     \"\"\"\r\n--> 290     (result,) = compute(self, traverse=False, **kwargs)\r\n    291     return result\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/base.py:573, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    570     keys.append(x.__dask_keys__())\r\n    571     postcomputes.append(x.__dask_postcompute__())\r\n--> 573 results = schedule(dsk, keys, **kwargs)\r\n    574 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/threaded.py:81, in get(dsk, result, cache, num_workers, pool, **kwargs)\r\n     78     elif isinstance(pool, multiprocessing.pool.Pool):\r\n     79         pool = MultiprocessingPoolExecutor(pool)\r\n---> 81 results = get_async(\r\n     82     pool.submit,\r\n     83     pool._max_workers,\r\n     84     dsk,\r\n     85     result,\r\n     86     cache=cache,\r\n     87     get_id=_thread_get_id,\r\n     88     pack_exception=pack_exception,\r\n     89     **kwargs,\r\n     90 )\r\n     92 # Cleanup pools associated to dead threads\r\n     93 with pools_lock:\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/local.py:506, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\r\n    504         _execute_task(task, data)  # Re-execute locally\r\n    505     else:\r\n--> 506         raise_exception(exc, tb)\r\n    507 res, worker_id = loads(res_info)\r\n    508 state[\"cache\"][key] = res\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/local.py:314, in reraise(exc, tb)\r\n    312 if exc.__traceback__ is not tb:\r\n    313     raise exc.with_traceback(tb)\r\n--> 314 raise exc\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/local.py:219, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\r\n    217 try:\r\n    218     task, data = loads(task_info)\r\n--> 219     result = _execute_task(task, data)\r\n    220     id = get_id()\r\n    221     result = dumps((result, id))\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)\r\n    115     func, args = arg[0], arg[1:]\r\n    116     # Note: Don't assign the subtask results to a variable. numpy detects\r\n    117     # temporaries by their reference count and can execute certain\r\n    118     # operations in-place.\r\n--> 119     return func(*(_execute_task(a, cache) for a in args))\r\n    120 elif not ishashable(arg):\r\n    121     return arg\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/dataframe/utils.py:397, in check_meta(x, meta, funcname, numeric_equal)\r\n    392         errmsg = \"Partition type: `{}`\\n{}\".format(\r\n    393             typename(type(meta)),\r\n    394             asciitable([\"Column\", \"Found\", \"Expected\"], bad_dtypes),\r\n    395         )\r\n    396     else:\r\n--> 397         check_matching_columns(meta, x)\r\n    398         return x\r\n    399 else:\r\n\r\nFile ~/mambaforge/envs/dask-use/lib/python3.9/site-packages/dask/dataframe/utils.py:422, in check_matching_columns(meta, actual)\r\n    420 else:\r\n    421     extra_info = \"Order of columns does not match\"\r\n--> 422 raise ValueError(\r\n    423     \"The columns in the computed data do not match\"\r\n    424     \" the columns in the provided metadata\\n\"\r\n    425     f\"{extra_info}\"\r\n    426 )\r\n\r\n```\r\n\r\n</details>\r\n\r\n```\r\nValueError: The columns in the computed data do not match the columns in the provided metadata\r\nOrder of columns does not match\r\n```\r\n\r\n\r\nComing from pandas I (likely wrongly) view `from_delayed` as an async analog to `pandas.concat`\r\n\r\nIn the first case of a truly empty dataframe, I could be convinced that users could be steered to write function that return an zero-row dataframe with the columns specified. (Counterpoint: it might be difficult to know exactly what the columns and their order will be). \r\n\r\nIn the second case, I feels more like something that we could handle, though I see how complexity will sneak its way in.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9034/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9034/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}