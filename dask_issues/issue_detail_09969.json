{
    "url": "https://api.github.com/repos/dask/dask/issues/9969",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/9969/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/9969/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/9969/events",
    "html_url": "https://github.com/dask/dask/issues/9969",
    "id": 1587620956,
    "node_id": "I_kwDOAbcwm85eoSxc",
    "number": 9969,
    "title": "Abandon encoded tuples as task definition in dsk graphs",
    "user": {
        "login": "fjetter",
        "id": 8629629,
        "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fjetter",
        "html_url": "https://github.com/fjetter",
        "followers_url": "https://api.github.com/users/fjetter/followers",
        "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
        "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
        "organizations_url": "https://api.github.com/users/fjetter/orgs",
        "repos_url": "https://api.github.com/users/fjetter/repos",
        "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
        "received_events_url": "https://api.github.com/users/fjetter/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 386719400,
            "node_id": "MDU6TGFiZWwzODY3MTk0MDA=",
            "url": "https://api.github.com/repos/dask/dask/labels/scheduler",
            "name": "scheduler",
            "color": "D10945",
            "default": false,
            "description": ""
        },
        {
            "id": 1342304743,
            "node_id": "MDU6TGFiZWwxMzQyMzA0NzQz",
            "url": "https://api.github.com/repos/dask/dask/labels/core",
            "name": "core",
            "color": "000000",
            "default": false,
            "description": ""
        },
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        },
        {
            "id": 2156573524,
            "node_id": "MDU6TGFiZWwyMTU2NTczNTI0",
            "url": "https://api.github.com/repos/dask/dask/labels/highlevelgraph",
            "name": "highlevelgraph",
            "color": "8c24d6",
            "default": false,
            "description": "Issues relating to HighLevelGraphs."
        },
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        },
        {
            "id": 3798602129,
            "node_id": "LA_kwDOAbcwm87iahGR",
            "url": "https://api.github.com/repos/dask/dask/labels/enhancement",
            "name": "enhancement",
            "color": "C2E0C6",
            "default": true,
            "description": "Improve existing functionality or make things work better"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 6,
    "created_at": "2023-02-16T12:49:44Z",
    "updated_at": "2024-03-11T01:44:55Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "# TLDR\r\n\r\nI would like to introduce a `Task` class (name TBD) instead of using tuples to define our graphs. I believe this can help us with many problem spaces like serialization, stringification, graph composition and analysis, etc.\r\n\r\n# Motivation\r\n\r\nCurrently, all task graphs are defined as dictionaries that are mapping a hashable to a tuple of _things_\r\n\r\nExample\r\n\r\n```python\r\ndsk = {\r\n    \"key-1\": (func, \"a\", \"b\"),\r\n    \"key-2\": (func, \"key-1\", \"b\"),\r\n    \"key-3\": (func, (func2, \"c\", \"key-1\"), \"key-2\")\r\n}\r\n```\r\n\r\nThis examples showcases three different possibilities of defining tasks\r\n\r\n- `key-1`: is the simplest example. This is a function that is supposed to be called with a couple of arguments. This will translate to an expression like `key_1 = func(\"a\", \"b\")`\r\n- `key-2`: This is a task that defines a dependency to `key-1` by defining the literal key value as an argument. [1, 2] It will evaluate to `key_2 = func(key_1, \"b\")`.\r\n- `key-3`: It is even possible to define tasks recursively with arbitrary nesting levels. It will evaluate to `key_3 = func(func2(\"c\", key_1), key_2)`.\r\n\r\nTo handle this structure, several utility functions are typically used through the code base to convert strings, generate tokens or walk the graph recursively. Below a couple of important methods\r\n\r\n- [`istask`](https://github.com/dask/dask/blob/3124376d68bd3a15d381ce803ca066c7eef4c24f/dask/core.py#L24-L38) inspects and object and guesses that if the structure is approximately as described above, the object is a task spec\r\n- [`get_deps`](https://github.com/dask/dask/blob/3124376d68bd3a15d381ce803ca066c7eef4c24f/dask/core.py#L263-L276), [`get_dependencies`](https://github.com/dask/dask/blob/3124376d68bd3a15d381ce803ca066c7eef4c24f/dask/core.py#L222-L260) and [`keys_in_tasks`](https://github.com/dask/dask/blob/3124376d68bd3a15d381ce803ca066c7eef4c24f/dask/core.py#L157-L191) which is used to walk the dask graph\r\n- [`_execute_task`](https://github.com/dask/dask/blob/3124376d68bd3a15d381ce803ca066c7eef4c24f/dask/core.py#L82-L125) which is used to execute a task and walk a recursive function like `key-3`. Note that a very similar functionality is provided by [`SubgraphCallable`](https://github.com/dask/dask/blob/3124376d68bd3a15d381ce803ca066c7eef4c24f/dask/optimization.py#L948-L996).\r\n- [`dumps_task`](https://github.com/dask/distributed/blob/405c0e62d008e40aa873611ec932f2e81f2731dd/distributed/worker.py#L2947-L2975) to serialize such a tuple and again recursing through it. The developer has to ensure that this only happens after [``](https://github.com/dask/distributed/blob/405c0e62d008e40aa873611ec932f2e81f2731dd/distributed/utils_comm.py#L248-L276)\r\n\r\nThe important piece to take away is that we are iterating and recursing over almost every iterable we can find in the graph and replace matching keys automatically. Multiple times. This is very error prone and not particularly performance sensitive [3].\r\n\r\nApart from unnecessarily walking the graph, a lot of the `HighLevelGraph` and `distributed.protocol` complexity stems from the attempt to not deserialize payload data or user functions on the scheduler. The reasons for this are plentiful but also being debated recently (e.g. scheduler environments differ from clients, performance by not serializing data unnecessarily). I consider this only a minor motivator for this change but I think it's a nice to have and comes for free.\r\n\r\n# Proposal\r\n\r\nMost, if not all, of the above described complexity could be avoided if we chose to go for a very explicit task representation that is not based on an encoded tuple which would avoid any semantic confusion and would reduce our need to walk and recurse significantly.\r\n\r\n```python\r\nfrom distributed.utils_comm import WrappedKey\r\nfrom typing import Callable, Any, NewType\r\n\r\nKey = NewType(\"Key\", str)\r\n\r\n_RemotePlaceholder = object()\r\n\r\n\r\nclass Task:\r\n    func: Callable\r\n    key: Key\r\n    dependencies: dict[int, Key | WrappedKey]\r\n    argspec: list[Any]\r\n\r\n    __slots__ = tuple(__annotations__)\r\n\r\n    def __init__(self, key: Key, func, args):\r\n        self.key = key\r\n        self.func = func\r\n        self.dependencies = {}\r\n        parsed_args: list[Any] = [None] * len(args)\r\n        for ix, arg in enumerate(args):\r\n            if isinstance(arg, (Key, WrappedKey)):\r\n                self.dependencies[ix] = arg\r\n                parsed_args[ix] = _RemotePlaceholder\r\n            else:\r\n                parsed_args[ix] = arg\r\n        self.argspec = parsed_args\r\n\r\n    def pack(self):\r\n        # Serialization could be expresse by just convering between two objects\r\n        # ... or this logic would just be implemented in __reduce__\r\n        return PackedTask(\r\n            key=self.key,\r\n            dependencies=self.dependencies,\r\n            # This could be just a pickle.dumps\r\n            runspec=dumps({\"func\": self.func, \"argspec\": self.argspec}),\r\n        )\r\n\r\n    def __call__(self, *args, **kwargs):\r\n        # resolve the input and match it to the dependencies\r\n        argspec = self.match_args_with_dependencies(*args, **kwargs)\r\n        return self.func(*argspec)\r\n\r\n# Note how graph related metadata stays untouched between packing/unpacking. In\r\n# the end, the dask scheduler only cares about the graph metadata and everything\r\n# else is a pass through.\r\n# More complicated versions of this could also curry arguments, e.g. user\r\n# functions or payload data, while keeping others still free for later\r\n# optimizations\r\n\r\nclass PackedTask:\r\n    key: Key\r\n    runspec: bytes\r\n    dependencies: dict[int, Key]\r\n    __slots__ = tuple(__annotations__)\r\n\r\n    def __init__(self, key: Key, runspec: bytes, dependencies: dict[int, Key]):\r\n        self.key = key\r\n        self.runspec = runspec\r\n        self.dependencies = dependencies\r\n\r\n    def unpack(self):\r\n        spec = loads(self.runspec)\r\n        return Task(\r\n            key=self.key,\r\n            # We could implement a fast path that\r\n            # is not computing dependencies again\r\n            #\r\n            # dependencies=self.dependencies,\r\n            func=spec[\"func\"],\r\n            args=spec[\"argspec\"],\r\n        )\r\n\r\n    @property\r\n    def nbytes(self) -> int:\r\n        # Some utility things could become object properties\r\n        ...\r\n```\r\n\r\nI ran a couple of micro benchmarks to estimate the performance impact of using a slotted class instead of tuples and could indeed see a slowdown during graph generation (maybe a factor of 2) but this appears to be easily amortized by just not walking the graph so often and not recursing into everything and matching strings, etc. since this class exposes everything we'd be interested to know.\r\n\r\nCherry on top, this would allow us to have an identical definition of a tasks runspec in vanilla dask, on the client, scheduler and worker. Specifically, we could get rid of all the `dumps_function`, `loads_function`, `dumps_task`, `execute_task`, etc. in `distributed/worker.py`.\r\n\r\nA migration path would be straight forward since we would convert at the outmost layer a legacy task graph to a new task graph such that all internal systems can use this representation instead. Nested tuples would be converted to `SubgraphCallable` (in fact, I believe in this world we should make `SubgraphCallable` a `Task` but that's a detail)\r\n\r\nI can genuinely only see benefits. This will be a bit of tedious work but I believe it would make working on internals so much easier, particularly around serialization topics.\r\n\r\nThoughts? Am I missing something?\r\n\r\n\r\n[1] The fact that we're using simple literals to identify dependencies can cause problems for developers if \"stringification\" is not properly applied since keys are not necessarily required to be strings until they reach the scheduler. However, if graph construction was not done properly, this can cause spurious errors, e.g. because the user function receives the non-stringified literal instead of the actual data.\r\n[2] Whenever users are providing a `key` themselves, e.g. in [scatter](https://github.com/dask/distributed/issues/3965) or [compute](https://github.com/dask/distributed/issues/7551) this can cause dask to resolve dependency structures falsely and confuses literal user argumnets with keys.\r\n[3] All of these functions are written efficiently and are operating on builtins so the performance overhead is OK but still unnecessary.\r\n\r\n\r\ncc @rjzamora, @madsbk, @jrbourbeau, @mrocklin, @crusaderky\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/9969/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/9969/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}