{
    "url": "https://api.github.com/repos/dask/dask/issues/11044",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/11044/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/11044/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/11044/events",
    "html_url": "https://github.com/dask/dask/issues/11044",
    "id": 2237078117,
    "node_id": "I_kwDOAbcwm86FVxpl",
    "number": 11044,
    "title": "FileExpired exception when reading parquet from a Minio bucket using Dask",
    "user": {
        "login": "PietroSpalluto",
        "id": 70751487,
        "node_id": "MDQ6VXNlcjcwNzUxNDg3",
        "avatar_url": "https://avatars.githubusercontent.com/u/70751487?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/PietroSpalluto",
        "html_url": "https://github.com/PietroSpalluto",
        "followers_url": "https://api.github.com/users/PietroSpalluto/followers",
        "following_url": "https://api.github.com/users/PietroSpalluto/following{/other_user}",
        "gists_url": "https://api.github.com/users/PietroSpalluto/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/PietroSpalluto/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/PietroSpalluto/subscriptions",
        "organizations_url": "https://api.github.com/users/PietroSpalluto/orgs",
        "repos_url": "https://api.github.com/users/PietroSpalluto/repos",
        "events_url": "https://api.github.com/users/PietroSpalluto/events{/privacy}",
        "received_events_url": "https://api.github.com/users/PietroSpalluto/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 6,
    "created_at": "2024-04-11T07:24:36Z",
    "updated_at": "2024-04-18T07:53:51Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\nI have a list of dataframes in a Minio bucket that are updated every 15 minutes. My script runs inside a Docker container in a loop and every 15 minutes a list of futures is created to read and preprocess every dataframe in the bucket. When computing the result it happens sometimes that the following exception is triggered:\r\n```\r\ns3fs.utils.FileExpired: [Errno 16] The remote file corresponding to filename filename.part.0.parquet and Etag \"76b9a0ed3044b29e8a326d6f4ade2036\" no longer exists\r\n```\r\n\r\nand triggers `TypeError: __init__() missing 1 required positional argument: 'e_tag'`. Even catching the exception does not solve the problem since during the next iteration it is triggered again. I checked on Minio and the Etag effectively does not correspond to that in the exception but I do not know how to solve this problem. The code to read data is this\r\n\r\n```\r\ndf = dd.read_parquet('s3://{}/{}/{}'.format(bucket, path, filename),\r\n                         storage_options={\r\n                             \"key\": key,\r\n                             \"secret\": secret,\r\n                             \"client_kwargs\": {'endpoint_url': 'http://{}'.format(minio_endpoint)}\r\n                         })\r\n```\r\n\r\n**Minimal Complete Verifiable Example**:\r\nProviding a verifiable example is difficult since it runs on Docker and it is the result of various interacting scripts. I tried to replicate it running a simple script outside of docker but the problem does not appear. This is the script I used that is similar to what the original script does.\r\n\r\n```\r\ndef load_user(bucket, path, file, minio_endpoint, key, secret):\r\n    print(f'Reading {file}...')\r\n    df = dd.read_parquet('s3://{}/{}/{}'.format(bucket, path, hash_filename(file)),\r\n                         storage_options={\r\n                             \"key\": key,\r\n                             \"secret\": secret,\r\n                             \"client_kwargs\": {'endpoint_url': 'http://{}'.format(minio_endpoint)}\r\n                         },\r\n                         ignore_metadata_file=True)\r\n\r\n    return df\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    bucket = bucket name\r\n    path = path\r\n    filenames = list of filenames\r\n\r\n    cluster = LocalCluster(silence_logs=logging.CRITICAL)\r\n    dask_client = Client(cluster)\r\n    minio_client = Minio(endpoint, key, secret, secure=False)\r\n\r\n    minio_endpoint = minio_client._base_url.host\r\n    key = minio_client._provider._credentials.access_key\r\n    secret = minio_client._provider._credentials.secret_key\r\n\r\n    while True:\r\n        futures = []\r\n        for file in filenames:\r\n            future = dask_client.submit(load_user,\r\n                                        bucket,\r\n                                        path,\r\n                                        file,\r\n                                        minio_endpoint,\r\n                                        key,\r\n                                        secret)\r\n            futures.append(future)\r\n\r\n        for user, future in zip(filenames, futures):\r\n            df = future.result()\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nI have tried to use the function `invalidate_cache()` of s3fs and using `ignore_metadata_file=True` when reading data but it didn't worked. Catching the exception works but the problem is not solved during the following iteration.\r\n\r\nHere is the complete traceback if you find it useful\r\n\r\n```\r\nTraceback (most recent call last):  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 113, in _error_wrapper    return await func(*args, **kwargs)  File \"/usr/local/lib/python3.9/site-packages/aiobotocore/client.py\", line 408, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.exceptions.ClientError: An error occurred (PreconditionFailed) when calling the GetObject operation: At least one of the pre-conditions you specified did not holdThe above exception was the direct cause of the following exception:Traceback (most recent call last):  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 2300, in _fetch_range    return _fetch_range(  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 2462, in _fetch_range    return sync(fs.loop, _inner_fetch, fs, bucket, key, version_id, start, end, req_kw)  File \"/usr/local/lib/python3.9/site-packages/fsspec/asyn.py\", line 103, in sync    raise return_result  File \"/usr/local/lib/python3.9/site-packages/fsspec/asyn.py\", line 56, in _runner    result[0] = await coro  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 2480, in _inner_fetch    return await _error_wrapper(_call_and_read, retries=fs.retries)  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 142, in _error_wrapper    raise err  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 113, in _error_wrapper    return await func(*args, **kwargs)  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 2467, in _call_and_read    resp = await fs._call_s3(  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 362, in _call_s3    return await _error_wrapper(  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 142, in _error_wrapper    raise errOSError: [Errno 22] At least one of the pre-conditions you specified did not holdThe above exception was the direct cause of the following exception:Traceback (most recent call last):  File \"/usr/local/lib/python3.9/site-packages/dask/backends.py\", line 141, in wrapper    return func(*args, **kwargs)  File \"/usr/local/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py\", line 529, in read_parquet    read_metadata_result = engine.read_metadata(  File \"/usr/local/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py\", line 546, in read_metadata    dataset_info = cls._collect_dataset_info(  File \"/usr/local/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py\", line 1061, in _collect_dataset_info    ds = pa_ds.dataset(  File \"/usr/local/lib/python3.9/site-packages/pyarrow/dataset.py\", line 785, in dataset    return _filesystem_dataset(source, **kwargs)  File \"/usr/local/lib/python3.9/site-packages/pyarrow/dataset.py\", line 475, in _filesystem_dataset    return factory.finish(schema)  File \"pyarrow/_dataset.pyx\", line 3025, in pyarrow._dataset.DatasetFactory.finish  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status  File \"pyarrow/error.pxi\", line 88, in pyarrow.lib.check_status  File \"/usr/local/lib/python3.9/site-packages/fsspec/spec.py\", line 1846, in read    out = self.cache._fetch(self.loc, self.loc + length)  File \"/usr/local/lib/python3.9/site-packages/fsspec/caching.py\", line 189, in _fetch    self.cache = self.fetcher(start, end)  # new block replaces old  File \"/usr/local/lib/python3.9/site-packages/s3fs/core.py\", line 2312, in _fetch_range    raise FileExpired(s3fs.utils.FileExpired: [Errno 16] The remote file corresponding to filename 16 and Etag The remote file corresponding to filename filename.part.0.parquet and Etag \"76b9a0ed3044b29e8a326d6f4ade2036\" no longer exists. no longer exists.During handling of the above exception, another exception occurred:Traceback (most recent call last):  File \"/python-docker/run.py\", line 19, in     service.start()  File \"/python-docker/chimera_ml_app/services.py\", line 130, in start    self.channel.start_consuming()  File \"/usr/local/lib/python3.9/site-packages/pika/adapters/blocking_connection.py\", line 1883, in start_consuming    self._process_data_events(time_limit=None)  File \"/usr/local/lib/python3.9/site-packages/pika/adapters/blocking_connection.py\", line 2044, in _process_data_events    self.connection.process_data_events(time_limit=time_limit)  File \"/usr/local/lib/python3.9/site-packages/pika/adapters/blocking_connection.py\", line 851, in process_data_events    self._dispatch_channel_events()  File \"/usr/local/lib/python3.9/site-packages/pika/adapters/blocking_connection.py\", line 567, in _dispatch_channel_events    impl_channel._get_cookie()._dispatch_events()  File \"/usr/local/lib/python3.9/site-packages/pika/adapters/blocking_connection.py\", line 1510, in _dispatch_events    consumer_info.on_message_callback(self, evt.method,  File \"/python-docker/chimera_ml_app/services.py\", line 157, in on_request    response = chimera.ml.predict(body=request)  File \"src/dependency_injector/_cwiring.pyx\", line 28, in dependency_injector._cwiring._get_sync_patched._patched  File \"/python-docker/chimera_ml_app/ml.py\", line 20, in predict    predictions = predictor.predict()  File \"/usr/local/lib/python3.9/site-packages/hijacking_detection/pipelines/PipelinePredictorKATANA.py\", line 125, in predict    preprocessing.preprocess(path, save_path)  File \"/usr/local/lib/python3.9/site-packages/hijacking_detection/preprocessing/PreprocessingTestResample.py\", line 55, in preprocess    dataframe = future.result()  File \"/usr/local/lib/python3.9/site-packages/distributed/client.py\", line 323, in result    return self.client.sync(self._result, callback_timeout=timeout)  File \"/usr/local/lib/python3.9/site-packages/hijacking_detection/preprocessing/PreprocessingTestResample.py\", line 91, in load_user    df = dd.read_parquet('s3://{}/{}'.format(bucket, path),  File \"/usr/local/lib/python3.9/site-packages/dask/backends.py\", line 143, in wrapper    raise type(e)(TypeError: __init__() missing 1 required positional argument: 'e_tag'\r\n```\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.2.1\r\n- Python version: 3.9.13\r\n- Operating System: Docker running [python:3.9.17-slim-buster\r\n](https://hub.docker.com/layers/library/python/3.9.17-slim-buster/images/sha256-d5cca64dca485c37ccf06721e36a93bf4331b0404bfd3ef2c7873867965359b7?context=explore)\r\n- Install method (conda, pip, source): pip\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/11044/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/11044/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}