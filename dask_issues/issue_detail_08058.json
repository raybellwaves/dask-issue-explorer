{
    "url": "https://api.github.com/repos/dask/dask/issues/8058",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/8058/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/8058/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/8058/events",
    "html_url": "https://github.com/dask/dask/issues/8058",
    "id": 973129186,
    "node_id": "MDU6SXNzdWU5NzMxMjkxODY=",
    "number": 8058,
    "title": "[Discussion] Improve Parquet-Metadata Processing in read_parquet",
    "user": {
        "login": "rjzamora",
        "id": 20461013,
        "node_id": "MDQ6VXNlcjIwNDYxMDEz",
        "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rjzamora",
        "html_url": "https://github.com/rjzamora",
        "followers_url": "https://api.github.com/users/rjzamora/followers",
        "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
        "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
        "organizations_url": "https://api.github.com/users/rjzamora/orgs",
        "repos_url": "https://api.github.com/users/rjzamora/repos",
        "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rjzamora/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 242862289,
            "node_id": "MDU6TGFiZWwyNDI4NjIyODk=",
            "url": "https://api.github.com/repos/dask/dask/labels/dataframe",
            "name": "dataframe",
            "color": "fbca04",
            "default": false,
            "description": null
        },
        {
            "id": 365513534,
            "node_id": "MDU6TGFiZWwzNjU1MTM1MzQ=",
            "url": "https://api.github.com/repos/dask/dask/labels/io",
            "name": "io",
            "color": "6f871c",
            "default": false,
            "description": ""
        },
        {
            "id": 1372867996,
            "node_id": "MDU6TGFiZWwxMzcyODY3OTk2",
            "url": "https://api.github.com/repos/dask/dask/labels/discussion",
            "name": "discussion",
            "color": "bebaf4",
            "default": false,
            "description": "Discussing a topic with no specific actions yet"
        },
        {
            "id": 2949099791,
            "node_id": "MDU6TGFiZWwyOTQ5MDk5Nzkx",
            "url": "https://api.github.com/repos/dask/dask/labels/parquet",
            "name": "parquet",
            "color": "77A66C",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 8,
    "created_at": "2021-08-17T23:09:04Z",
    "updated_at": "2021-10-12T07:18:17Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "\r\n## Current Metadata-Related Challenges in `read_parquet`\r\n\r\nThe current approach to parquet-metadata handling in Dask-Dataframe has been causing pain for many users recently. The problem is especially true for large-scale IO from cloud-based file systems.  The origin of this pain seems to be the historical decision to rely on a shared _metadata file.\r\n\r\n### Why we use _metadata\r\n\r\nThe historical reason for our adoption of the _metadata file is simple: The usual convention in Dask is to only use the client process to construct a graph when a public collection-related API is used. There are certainly exceptions to this (e.g. `set_index`), but it is typically true that non-compute/persist API calls will only execute on the client.  Therefore, in order to avoid the slow process of opening and processing footer metadata for every file in a parquet dataset on the client, we have encouraged the construction of a single/global _metadata file at write time.  This is why Dask's `to_parquet` implementation will construct and write this file by default (see the `write_metadata_file` argument).\r\n\r\n### Why _metadata can be a problem\r\n\r\nAlthough most parquet users will likely benefit from writing/reading a shared _metadata file, there are clearly cases where this approach breaks down. As illustrated in issues like #8031 and #8027, there are large-scale IO scenarios in which a single metadata file can be too large to write and/or read on a single process.  Given that the entire purpose of Dask-Dataframe is to enable/accelerate large-scale tabular-data processing, I feel that we should treat these cases seriously.\r\n\r\nAs I will present below, my short-term suggestion to the _metadata problem is two-fold:\r\n\r\n1. Make it possible to ignore the existence of a shared _metadata file when it is present (Note that this step is already implemented in #8034)\r\n2. Refactor the `Engine.read_metadata` implementation internals (especially for \"pyarrow-datsaet\" and \"fastparquet\") to allow `parts`/`statistics` to be collected in parallel, and to avoid the unnecessary/intermediate step of constructing a proxy global-metadata object when the _metadata file is missing and/or needs to be ignored.\r\n\r\n\r\n## The Current Organization of `read_parquet`\r\n\r\n\r\n### Core-Engine Interface\r\n\r\nTo understand the proposed short-term _metadata solution, it is useful to have a rough understanding of \"core\"-\"engine\" interface that is currently used within Dask-Dataframe's `read_parquet` API.  When the user calls `read_parquet`, they are either explicitly or implicitly specifying a backend engine. That engine is then expected to produce three critical pieces of information when `engine.read_metadata` is called:\r\n\r\n1. `meta`: The metadata (an empty `pandas.DataFrame` object) for the output DataFrame collection\r\n2. `parts`: The `list` of information needed to produce the output-DataFrame partitions at IO time.  After `parts` is finalized, each if its elements will be used for a distinct call to `engine.read_partition` to produce a single output-`pandas.DataFrame` partition.  For this reason, generating `parts` is effectively the same as materializing the task graph.\r\n3. `statistics`: A list of statistics for each element of `parts`. This list is required to calculate output divisions, aggregate files by `chunksize`, and/or apply filters (except for \"pyarrow-dataset\").\r\n\r\n**Outline of the existing read_parquet algorithm:**\r\n\r\n```python\r\ndef read_parquet(paths, filters, engine, ...):\r\n    ...\r\n    # ENGINE logic to calculate meta, parts & statistics.\r\n    #\r\n    # This is where the engine is expected to produce output\r\n    # `meta`, and the `parts` & `statistics` lists-\r\n    (\r\n        meta,\r\n        parts,\r\n        statistics,\r\n        ...\r\n    ) = engine.read_metadata(paths, filters, ...)\r\n\r\n    # CORE logic to apply filters and calculate divisions\r\n    (\r\n        parts,\r\n        divisions,\r\n        ...\r\n    ) = process_statistics(parts, statistics, filters, ...)\r\n\r\n    # CORE logic to define Layer/DataFrame\r\n    layer = DataFrameIOLayer(parts, ...)\r\n    ...\r\n    return new_dd_object(...)\r\n```\r\n\r\n\r\nAlthough the best long-term `read_parquet` API will likely need to break the single `read_metadata` API into 2-3 distinct functions, the short-term proposal will start by leaving the surface area of this function as is.  Instead of modifying the \"public\" Engine API, I suggest that we focus on a smaller internal refactor of `read_metadata`.\r\n\r\n\r\n### Engine-Specific Logic: Collecting Metadata and `parts`/`statistics`\r\n\r\nIn order to understand how we must modify the existing `Engine.read_metadata` implementations, it is useful to describe how these functions are currently designed.  The general approach comprises three steps:\r\n\r\n1. Use engine-specific logic to construct a global parquet-metadata object (or a global-metadata \"proxy\")\r\n2. Use collected metadata/schema information to define the output DataFrame `meta`\r\n3. Use a mix of shared and engine-specific logic to convert `metadata` to `parts` and `statistics`\r\n\r\n**Outline of the existing ArrowDatasetEngine.read_metadata implementation:**\r\n\r\n```python\r\nclass ArrowDatasetEngine:\r\n\r\n    @classmethod\r\n    def read_metadata(cls, paths, filters, ...):\r\n\t\r\n        # Collect a `metadata` structure. For \"pyarrow-legacy\",\r\n        # this is a `pyarrow.parquet.FileMetaData` object. For\r\n        # \"pyarrow-dataset\", this is a list of dataset fragments.\r\n        metadata, ... = cls._gather_metadata(paths, filters, ...)\r\n\r\n        # Use the pyarrow.dataset schema to construct the `meta`\r\n        # of the output DataFrame collection\r\n        meta, ... = cls._generate_dd_meta(schema, ...)\r\n\t\t\r\n        # Use the `metadata` object (list of fragments) to construct\r\n        # coupled `parts` and `statistics` lists\r\n        parts, statistics, ... = cls._construct_parts(metadata, ...)\r\n\t\t\r\n        return meta, statistics, parts\r\n\t\t\r\n    @classmethod\r\n    def _gather_metadata(cls, paths, ...):\r\n\t\r\n        # Create pyarrow.dataset object\r\n        ds = pa_dataset.dataset(paths, ...)\r\n\t\t\r\n        # Collect filtered list of dataset \"fragments\".\r\n        # Call this list of fragments the \"metadata\"\r\n        # (this is NOT a formal \"parquet-metadata\" object)\r\n        metadata = _collect_pyarrow_dataset_frags(ds, filters, ...)\r\n\r\n        return schema, metadata, ...\r\n\t\t\r\n    @classmethod\r\n    def _construct_parts(cls, metadata, ...):\r\n\t\r\n        # Here we use a combination of engine-specific and\r\n        # shared logic to construct coupled `parts` and `statistics`\r\n        parts, statistics = <messy-logic>(metadata, ...)\r\n\r\n        return parts, statistics, ...\r\n```\r\n\r\nThe exact details of these steps depend on the specific engine, but the general algorithm is pretty much the same for both \"pyarrow\" and \"fastparquet\". This general algorithm works well in many cases, but it has the following drawbacks:\r\n\r\n1. The metadata/metadata-proxy object construction does not yet allow the user to opt out of _metadata processing.\r\n2. The metadata/metadata-proxy object is only collected in parallel for the \"arrow-legacy\" API (not for \"pyarrow\" or \"fastparquet\")\r\n3. Even when the metadata object is collected in parallel for \"pyarrow-legacy\", it is still reduced into a single object and then processed in serial on the client.  This is clearly not the most efficient way to produce the final `parts`/`statistics` lists.\r\n4. (**more of a future problem**) The `meta` is not constructed until after a metadata or metadata-proxy object has been constructed.  This is not a problem yet, but is likely to become one when it is time to implement an abstract-expression API for read_parquet. \r\n\r\n\r\n## Short-Term Proposal\r\n\r\n### Make _metadata Processing Optional\r\n\r\nI propose that we allow the user to specify that a global _metadata file should be ignored by Dask. This first step is already implemented in #8034, where a new `ignore_metadata_file=` kwarg has been added to the public `read_parquet` API. Please feel free to provide specific feedback in that PR.\r\n\r\n### Refactor `*Engine.read_metadata`\r\n\r\n**Outline of the PROPOSED `ArrowDatasetEngine.read_metadat`a implementation**:\r\n\r\n```python\r\nclass ArrowDatasetEngine:\r\n    @classmethod\r\n    def read_metadata(cls, paths, filters, ignore_metadata_file, ...):\r\n\r\n        # Stage 1: Use a combination of engine specific logic and shared\r\n        # fsspec utilities to construct an engine-specific `datset_info` dictionary.\r\n        dataset_info = cls._collect_dataset_info(paths, ignore_metadata_file, ...)\r\n\r\n        # Stage 2: Use information in `dataset_info` (like schema) to define\r\n        # the `meta` for the output DataFrame collection.\r\n        meta, ... = cls._generate_dd_meta(dataset_info, ...)\r\n\t\t\r\n        # Stage 3: Use information in `dataset_info` to directly construct\r\n        # `parts` and `statistics`\r\n        parts, statistics = cls._make_partition_plan(dataset_info, meta, filters, ...)\r\n\t\t\r\n        return meta, statistics, parts\r\n```\r\n\r\n#### Stage-1 Details\r\n\r\nThe purpose of this stage is to do as little work as possible to populate a dictionary of high-level information about the dataset in question.  This \"high-level\" information will most likely include the schema, the paths, the file-system, and the discovered hive partitions.  Although our first pass at this should not try to do anything particularly clever here, we may eventually want to use this space to discover the paths/hive-partitions in parallel. We can also leverage #9051 (`categorical_partitions`) to avoid the need to discover all files up front (since we may not need full categorical dtypes for the schema).\r\n\r\nAssuming that we should keep stage 1 simple for now, the new `_collect_dataset_info` functions will effectively pull out the existing logic in `*engine.read_metadata` used to define a `pyarrow.dataset`/`ParquetDataset`/`ParquetFile` objects.  These objects can be stored, along with other important information, in the output `dataset_info` dictionary. Note that the only critical detail in this stage is that we must support the `ignore_metadata_file=True` option.\r\n\r\n\r\n#### Stage-2 Details\r\n\r\nThere is not much (if any) work to be done here.  The existing logic in the various `_generate_dd_meta` implementations can be reused.\r\n\r\n#### Stage-3 Details\r\n\r\nThis stage will correspond to the lion's share of the pain required for this work. While the short-term plan for Stage-1 and Stage-2 are to effectively move around existing code into simple functions with clear objectives, Stage-3 will require us to implement a new algorithm to (optionally) construct `parts`/`statistics` in parallel.  I expect the exact details here to be pretty engine-specific.  That is, \"pyarrow-datset\" will (optionally) parallelize over the processing of file fragments into `parts`/`statistics`, while \"fastparquet\" will need to parallelize over paths a bit differently.  However, I do expect all engine to follow a similar \"parallelize over file-path/object\" approach.  In the case that the _metadata file exists (and the user has not asked to ignore it), we should avoid constructing `parts`/`statistics` on the workers.\r\n\r\n\r\n**ROUGH illustration of the PROPOSED `ArrowDatasetEngine._make_partition_plan` implementation**:\r\n\r\n```python\r\n@classmethod\r\ndef _make_partition_plan(cls, dataset_info, meta, filters, split_row_groups, ...):\r\n\r\n    parts, statistics = [], []\r\n\t\r\n    # (OPTIONALLY) DASK-PARALLELIZE THIS LOOP:\r\n    for file_object in all_file_objects:\r\n        part, part_stats = cls._collect_file_parts(file_object, ...)\r\n\t\r\n    return parts, statistics\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/8058/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/8058/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}