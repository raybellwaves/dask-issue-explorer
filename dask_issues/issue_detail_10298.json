{
    "url": "https://api.github.com/repos/dask/dask/issues/10298",
    "repository_url": "https://api.github.com/repos/dask/dask",
    "labels_url": "https://api.github.com/repos/dask/dask/issues/10298/labels{/name}",
    "comments_url": "https://api.github.com/repos/dask/dask/issues/10298/comments",
    "events_url": "https://api.github.com/repos/dask/dask/issues/10298/events",
    "html_url": "https://github.com/dask/dask/issues/10298",
    "id": 1715398783,
    "node_id": "I_kwDOAbcwm85mPuh_",
    "number": 10298,
    "title": "Using `include_path_column` slows down reading multiple CSV and use excessive resources",
    "user": {
        "login": "guidocioni",
        "id": 12760310,
        "node_id": "MDQ6VXNlcjEyNzYwMzEw",
        "avatar_url": "https://avatars.githubusercontent.com/u/12760310?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/guidocioni",
        "html_url": "https://github.com/guidocioni",
        "followers_url": "https://api.github.com/users/guidocioni/followers",
        "following_url": "https://api.github.com/users/guidocioni/following{/other_user}",
        "gists_url": "https://api.github.com/users/guidocioni/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/guidocioni/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/guidocioni/subscriptions",
        "organizations_url": "https://api.github.com/users/guidocioni/orgs",
        "repos_url": "https://api.github.com/users/guidocioni/repos",
        "events_url": "https://api.github.com/users/guidocioni/events{/privacy}",
        "received_events_url": "https://api.github.com/users/guidocioni/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3468123446,
            "node_id": "LA_kwDOAbcwm87Ot102",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20attention",
            "name": "needs attention",
            "color": "6d626c",
            "default": false,
            "description": "It's been a while since this was pushed on. Needs attention from the owner or a maintainer."
        },
        {
            "id": 3880424463,
            "node_id": "LA_kwDOAbcwm87nSpQP",
            "url": "https://api.github.com/repos/dask/dask/labels/needs%20triage",
            "name": "needs triage",
            "color": "eeeeee",
            "default": false,
            "description": "Needs a response from a contributor"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2023-05-18T10:31:29Z",
    "updated_at": "2023-06-26T02:08:54Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I have a simple task.\r\nMerge 70k+ zip compressed CSV files (3KB compressed, 9KB unzipped) into a single parquet file, adding some metadata in the process.\r\nThe CSV files have only 4 columns inside, with 3 of them having float with a single decimal digit, so we're definitely not talking about \"BIG\" data here.\r\n\r\nThe serial version with Pandas works fine and has almost no impact on memory (and CPU).\r\n\r\n```python\r\nfiles = glob(\"*.zip\")\r\n\r\ndfs = []\r\nfor f in tqdm(files):\r\n    df = pd.read_csv(f,\r\n                     parse_dates=[0],\r\n                     compression='zip')\r\n    df['cell'] = re.findall(\"(?:/)(\\d{5})(?:_\\d{4}.zip)\", f)[0]\r\n    dfs.append(df)\r\n\r\nfinal = pd.concat(dfs)\r\nfinal.to_parquet('file.parquet', engine='fastparquet', compression='gzip')\r\n```\r\ntakes about 2 minutes.\r\nThe resulting file is about 220MB, which is what I would expect given that the folder with all zipped files is about 200MB in size.\r\n\r\nNow if I try to do the same thing with a Dask Local Cluster, which should clearly have the advantage in a similar task, \r\n\r\n```python\r\nclient = Client(n_workers=4)\r\ndf = dd.read_csv(\"*.zip\",\r\n                parse_dates=[0], compression='zip',\r\n                blocksize=None, include_path_column=True)\r\ndf['cell'] = df.path.str.findall(\"(?:/)(\\d{5})(?:_\\d{4}.zip)\")\r\ndf = df.drop(columns=['path'])\r\ndf.to_parquet.... # (or just df.compute)\r\n```\r\n\r\nThe CPU and memory usage quickly skyrockets and the resulting parquet file is 1GB in size just at the beginning. I didn't even manage to reach the end because my computer would almost crash. \r\n\r\nWhat am I missing here? Which is the part of the process that is not optimized or using a lot of resources. From what I gather the only computationally \"expensive\" parts are\r\n\r\n- parse the datetime column\r\n- decompress the files\r\n- execute the regex on the path column\r\n\r\nbut the serial version works just fine. \r\n\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/dask/dask/issues/10298/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/dask/dask/issues/10298/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}