[
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1191666070",
        "html_url": "https://github.com/dask/dask/issues/9292#issuecomment-1191666070",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9292",
        "id": 1191666070,
        "node_id": "IC_kwDOAbcwm85HB2GW",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-21T16:00:27Z",
        "updated_at": "2022-07-21T16:00:27Z",
        "author_association": "MEMBER",
        "body": "Thanks for raising this @MrPowers - I will certainly try to help with this effort.  **Minor Note**: You should specify `index=False` in your `dd.read_parquet` commands to avoid reading the unnecessary int64 \"null\" index into memory (you can also specify `write_index=False` when you **write** the dataset to avoid writing the \"null\" index).",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1191666070/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1196070807",
        "html_url": "https://github.com/dask/dask/issues/9292#issuecomment-1196070807",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9292",
        "id": 1196070807,
        "node_id": "IC_kwDOAbcwm85HSpeX",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-26T23:10:18Z",
        "updated_at": "2022-07-26T23:16:39Z",
        "author_association": "MEMBER",
        "body": "**Update**: I downloaded the billion-record csv file to my local system and played with this a bit on a LocalCluster with a memory allowance similar to the h2o instance. I found it a bit challenging to get the default `groupby(..).agg` operation to succeed with the existing ACA-based groupby algorithm. It **should** work with smaller `split_every` settings and/or larger `split_out` settings.  However, the operation was significantly more stable when I redirected the `aggregate` method to use the [`groupby_agg` function](https://github.com/rapidsai/cudf/blob/b5a2efb00bf6b32ca5bd082447ea3a63a7fd87f9/python/dask_cudf/dask_cudf/groupby.py#L460) I added to `dask_cudf` a couple years back.  For example, the following code ran in ~3.5mins:\r\n\r\n```python\r\nimport time\r\nimport dask.dataframe as dd\r\n\r\nfrom dask.distributed import LocalCluster, Client, wait\r\n\r\ncluster = LocalCluster(\r\n    local_directory=\"/.../dask-worker-space\",\r\n    n_workers=10,\r\n    memory_limit=\"24GB\",\r\n)\r\nclient = Client(cluster)\r\n\r\nddf = dd.read_csv(\r\n    \"/.../h2o/N_1e9_K_1e2_single.csv\",\r\n)\r\ncolumns = [\"id3\", \"v1\", \"v3\"]\r\nddf = ddf[columns]\r\nddf[\"id3\"] = ddf[\"id3\"].astype(\"string[pyarrow]\")\r\nagg = ddf.groupby(\"id3\", dropna=False).agg(\r\n    {\"v1\": \"sum\", \"v3\": \"mean\"},\r\n)\r\n\r\nt0 = time.time()\r\nresult = agg.persist()\r\nwait(result)\r\ntotal_time = time.time() - t0\r\n\r\nprint(total_time, len(result))\r\n```\r\n\r\nNote that `groupby_agg` lives in `dask_cudf`, but it only uses `cudf` if the input data is a `cudf.DataFrame`.  The logic is backend agnostic, because I had originally intended to upstream the code to `dask.dataframe`. The only reason I didn't follow through with this plan was because light preliminary benchmarking suggested that the \"optimized\" code path offered little-to-no benefit for  Pandas-backed Dask-DataFrame. This particular example clearly suggest that my preliminary benchmarking was probably **too** light, and that the real benefit comes at scale.",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1196070807/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 1,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/dask/dask/issues/comments/1239587709",
        "html_url": "https://github.com/dask/dask/issues/9292#issuecomment-1239587709",
        "issue_url": "https://api.github.com/repos/dask/dask/issues/9292",
        "id": 1239587709,
        "node_id": "IC_kwDOAbcwm85J4pt9",
        "user": {
            "login": "fjetter",
            "id": 8629629,
            "node_id": "MDQ6VXNlcjg2Mjk2Mjk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fjetter",
            "html_url": "https://github.com/fjetter",
            "followers_url": "https://api.github.com/users/fjetter/followers",
            "following_url": "https://api.github.com/users/fjetter/following{/other_user}",
            "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions",
            "organizations_url": "https://api.github.com/users/fjetter/orgs",
            "repos_url": "https://api.github.com/users/fjetter/repos",
            "events_url": "https://api.github.com/users/fjetter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fjetter/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-07T16:00:41Z",
        "updated_at": "2022-09-07T16:00:41Z",
        "author_association": "MEMBER",
        "body": "FYI I had an incredibly great experience with pre-shuffle compression for high column cardinality problems, see https://github.com/dask/dask/pull/6259 I never had the time to finish this but the code is straight forward enough if somebody wants to pick it up and give it a try\r\n\r\nThis implementation was developed for of ETL pipelines that heavily relied on shuffling for parquet dataset repartitioning, see https://github.com/data-engineering-collective/plateau/blob/main/plateau/io/dask/_shuffle.py",
        "reactions": {
            "url": "https://api.github.com/repos/dask/dask/issues/comments/1239587709/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]